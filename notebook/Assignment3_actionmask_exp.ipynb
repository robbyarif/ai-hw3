{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sbro3A_Qha1"
   },
   "source": [
    "# Taxi Env (Don't Modify it )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in ./.local/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/tljh/user/lib/python3.12/site-packages (from gymnasium) (2.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./.local/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.local/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a1HlV_-2QUNN"
   },
   "outputs": [],
   "source": [
    "from contextlib import closing\n",
    "from io import StringIO\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env, spaces, utils\n",
    "from gymnasium.envs.toy_text.utils import categorical_sample\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "MAP_0 = [\n",
    "    \"+---------+\",\n",
    "    \"|R: | : :G|\",\n",
    "    \"| : | : : |\",\n",
    "    \"| : : : : |\",\n",
    "    \"| | : | : |\",\n",
    "    \"|Y| : |B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "\n",
    "MAP_1 = [\n",
    "    \"+---------+\",\n",
    "    \"|R| : : :G|\",\n",
    "    \"| | : | : |\",\n",
    "    \"| : : | : |\",\n",
    "    \"| : | : : |\",\n",
    "    \"|Y: | :B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "\n",
    "MAP_2 = [\n",
    "    \"+---------+\",\n",
    "    \"|R: : : |G|\",\n",
    "    \"| : : : | |\",\n",
    "    \"| : | : : |\",\n",
    "    \"| : | | : |\",\n",
    "    \"|Y: : |B: |\",\n",
    "    \"+---------+\",\n",
    "]\n",
    "\n",
    "MAP = MAP_0\n",
    "MAPS = [MAP_0, MAP_1, MAP_2]\n",
    "WINDOW_SIZE = (550, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBkD72IKQxKT"
   },
   "outputs": [],
   "source": [
    "class TaxiEnv(Env):\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        render_mode: str | None = None,\n",
    "        is_rainy: bool = True,\n",
    "        fickle_passenger: bool = False,\n",
    "        use_multiple_maps: bool = True,\n",
    "        reward_step: float = -1,\n",
    "        reward_delivery: float = 20,\n",
    "        reward_illegal: float = -10,\n",
    "    ):\n",
    "        self.use_multiple_maps = use_multiple_maps\n",
    "\n",
    "        if use_multiple_maps:\n",
    "            self.desc_maps = [np.asarray(m, dtype=\"c\") for m in MAPS]\n",
    "            self.P_maps = []\n",
    "            self.current_map_id = 0\n",
    "            self.desc = self.desc_maps[0]\n",
    "        else:\n",
    "            self.desc = np.asarray(MAP, dtype=\"c\")\n",
    "\n",
    "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
    "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255)]\n",
    "\n",
    "        self.reward_step = reward_step\n",
    "        self.reward_delivery = reward_delivery\n",
    "        self.reward_illegal = reward_illegal\n",
    "\n",
    "        num_states = 500\n",
    "        num_rows = 5\n",
    "        num_columns = 5\n",
    "        self.max_row = num_rows - 1\n",
    "        self.max_col = num_columns - 1\n",
    "        self.initial_state_distrib = np.zeros(num_states)\n",
    "        num_actions = 6\n",
    "\n",
    "\n",
    "        if use_multiple_maps:\n",
    "\n",
    "            for map_idx in range(3):\n",
    "                self.desc = self.desc_maps[map_idx]\n",
    "                P = {\n",
    "                    state: {action: [] for action in range(num_actions)}\n",
    "                    for state in range(num_states)\n",
    "                }\n",
    "                self.P = P\n",
    "\n",
    "                # Iterate through all possible state combinations\n",
    "                for row in range(num_rows):\n",
    "                    for col in range(num_columns):\n",
    "                        for pass_idx in range(len(locs) + 1):\n",
    "                            for dest_idx in range(len(locs)):\n",
    "                                state = self.encode(row, col, pass_idx, dest_idx)\n",
    "                                if pass_idx < 4 and pass_idx != dest_idx:\n",
    "                                    self.initial_state_distrib[state] += 1\n",
    "                                for action in range(num_actions):\n",
    "                                    if is_rainy:\n",
    "                                        self._build_rainy_transitions(\n",
    "                                            row, col, pass_idx, dest_idx, action,\n",
    "                                        )\n",
    "                                    else:\n",
    "                                        self._build_dry_transitions(\n",
    "                                            row, col, pass_idx, dest_idx, action,\n",
    "                                        )\n",
    "\n",
    "                self.P_maps.append(P)\n",
    "\n",
    "\n",
    "            self.desc = self.desc_maps[0]\n",
    "            self.P = self.P_maps[0]\n",
    "        else:\n",
    "\n",
    "            self.P = {\n",
    "                state: {action: [] for action in range(num_actions)}\n",
    "                for state in range(num_states)\n",
    "            }\n",
    "\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_columns):\n",
    "                    for pass_idx in range(len(locs) + 1):\n",
    "                        for dest_idx in range(len(locs)):\n",
    "                            state = self.encode(row, col, pass_idx, dest_idx)\n",
    "                            if pass_idx < 4 and pass_idx != dest_idx:\n",
    "                                self.initial_state_distrib[state] += 1\n",
    "                            for action in range(num_actions):\n",
    "                                if is_rainy:\n",
    "                                    self._build_rainy_transitions(\n",
    "                                        row, col, pass_idx, dest_idx, action,\n",
    "                                    )\n",
    "                                else:\n",
    "                                    self._build_dry_transitions(\n",
    "                                        row, col, pass_idx, dest_idx, action,\n",
    "                                    )\n",
    "\n",
    "        self.initial_state_distrib /= self.initial_state_distrib.sum()\n",
    "        self.action_space = spaces.Discrete(num_actions)\n",
    "        self.observation_space = spaces.Discrete(num_states)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.fickle_passenger = fickle_passenger\n",
    "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.5\n",
    "\n",
    "        # pygame utils\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.cell_size = (\n",
    "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
    "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
    "        )\n",
    "        self.taxi_imgs = None\n",
    "        self.taxi_orientation = 0\n",
    "        self.passenger_img = None\n",
    "        self.destination_img = None\n",
    "        self.median_horiz = None\n",
    "        self.median_vert = None\n",
    "        self.background_img = None\n",
    "\n",
    "    def _pickup(self, taxi_loc, pass_idx, reward):\n",
    "        \"\"\"Computes the new location and reward for pickup action.\"\"\"\n",
    "        if pass_idx < 4 and taxi_loc == self.locs[pass_idx]:\n",
    "            new_pass_idx = 4\n",
    "            new_reward = reward\n",
    "        else:  # passenger not at location\n",
    "            new_pass_idx = pass_idx\n",
    "            new_reward = self.reward_illegal\n",
    "\n",
    "        return new_pass_idx, new_reward\n",
    "\n",
    "    def _dropoff(self, taxi_loc, pass_idx, dest_idx, default_reward):\n",
    "        \"\"\"Computes the new location and reward for return dropoff action.\"\"\"\n",
    "        if (taxi_loc == self.locs[dest_idx]) and pass_idx == 4:\n",
    "            new_pass_idx = dest_idx\n",
    "            new_terminated = True\n",
    "            new_reward = self.reward_delivery\n",
    "        elif (taxi_loc in self.locs) and pass_idx == 4:\n",
    "            new_pass_idx = self.locs.index(taxi_loc)\n",
    "            new_terminated = False\n",
    "            new_reward = default_reward\n",
    "        else:  # dropoff at wrong location\n",
    "            new_pass_idx = pass_idx\n",
    "            new_terminated = False\n",
    "            new_reward = self.reward_illegal\n",
    "\n",
    "        return new_pass_idx, new_reward, new_terminated\n",
    "\n",
    "    def _calc_new_position(self, row, col, movement, offset=0):\n",
    "        \"\"\"Calculates the new position for a row and col to the movement.\"\"\"\n",
    "        dr, dc = movement\n",
    "        new_row = max(0, min(row + dr, self.max_row))\n",
    "        new_col = max(0, min(col + dc, self.max_col))\n",
    "        if self.desc[1 + new_row, 2 * new_col + offset] == b\":\":\n",
    "            return new_row, new_col\n",
    "        else:  # Default to current position if not traversable\n",
    "            return row, col\n",
    "\n",
    "    def _build_rainy_transitions(self, row, col, pass_idx, dest_idx, action):\n",
    "        \"\"\"Computes the next action for a state (row, col, pass_idx, dest_idx) and action for `is_rainy`.\"\"\"\n",
    "        state = self.encode(row, col, pass_idx, dest_idx)\n",
    "\n",
    "        taxi_loc = left_pos = right_pos = (row, col)\n",
    "        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "        reward = self.reward_step\n",
    "        terminated = False\n",
    "\n",
    "        moves = {\n",
    "            0: ((1, 0), (0, -1), (0, 1)),  # Down\n",
    "            1: ((-1, 0), (0, -1), (0, 1)),  # Up\n",
    "            2: ((0, 1), (1, 0), (-1, 0)),  # Right\n",
    "            3: ((0, -1), (1, 0), (-1, 0)),  # Left\n",
    "        }\n",
    "\n",
    "        # Check if movement is allowed\n",
    "        if (\n",
    "            action in {0, 1}\n",
    "            or (action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\")\n",
    "            or (action == 3 and self.desc[1 + row, 2 * col] == b\":\")\n",
    "        ):\n",
    "            dr, dc = moves[action][0]\n",
    "            new_row = max(0, min(row + dr, self.max_row))\n",
    "            new_col = max(0, min(col + dc, self.max_col))\n",
    "\n",
    "            left_pos = self._calc_new_position(row, col, moves[action][1], offset=2)\n",
    "            right_pos = self._calc_new_position(row, col, moves[action][2])\n",
    "        elif action == 4:  # pickup\n",
    "            new_pass_idx, reward = self._pickup(taxi_loc, new_pass_idx, reward)\n",
    "        elif action == 5:  # dropoff\n",
    "            new_pass_idx, reward, terminated = self._dropoff(\n",
    "                taxi_loc, new_pass_idx, dest_idx, reward\n",
    "            )\n",
    "        intended_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
    "\n",
    "        if action <= 3:\n",
    "            left_state = self.encode(left_pos[0], left_pos[1], new_pass_idx, dest_idx)\n",
    "            right_state = self.encode(\n",
    "                right_pos[0], right_pos[1], new_pass_idx, dest_idx\n",
    "            )\n",
    "\n",
    "            self.P[state][action].append((0.8, intended_state, self.reward_step, terminated))\n",
    "            self.P[state][action].append((0.1, left_state, self.reward_step, terminated))\n",
    "            self.P[state][action].append((0.1, right_state, self.reward_step, terminated))\n",
    "        else:\n",
    "            self.P[state][action].append((1.0, intended_state, reward, terminated))\n",
    "\n",
    "\n",
    "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "        # (5) 5, 5, 4\n",
    "        i = taxi_row\n",
    "        i *= 5\n",
    "        i += taxi_col\n",
    "        i *= 5\n",
    "        i += pass_loc\n",
    "        i *= 4\n",
    "        i += dest_idx\n",
    "        return i\n",
    "\n",
    "    def decode(self, i):\n",
    "        out = []\n",
    "        out.append(i % 4)\n",
    "        i = i // 4\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i % 5)\n",
    "        i = i // 5\n",
    "        out.append(i)\n",
    "        assert 0 <= i < 5\n",
    "        return reversed(out)\n",
    "\n",
    "    def action_mask(self, state: int):\n",
    "        \"\"\"Computes an action mask for the action space using the state information.\"\"\"\n",
    "        mask = np.zeros(6, dtype=np.int8)\n",
    "        taxi_row, taxi_col, pass_loc, dest_idx = self.decode(state)\n",
    "        if taxi_row < 4:\n",
    "            mask[0] = 1\n",
    "        if taxi_row > 0:\n",
    "            mask[1] = 1\n",
    "        if taxi_col < 4 and self.desc[taxi_row + 1, 2 * taxi_col + 2] == b\":\":\n",
    "            mask[2] = 1\n",
    "        if taxi_col > 0 and self.desc[taxi_row + 1, 2 * taxi_col] == b\":\":\n",
    "            mask[3] = 1\n",
    "        if pass_loc < 4 and (taxi_row, taxi_col) == self.locs[pass_loc]:\n",
    "            mask[4] = 1\n",
    "        if pass_loc == 4 and (\n",
    "            (taxi_row, taxi_col) == self.locs[dest_idx]\n",
    "            or (taxi_row, taxi_col) in self.locs\n",
    "        ):\n",
    "            mask[5] = 1\n",
    "        return mask\n",
    "\n",
    "    def step(self, a):\n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s, r, t = transitions[i]\n",
    "        self.lastaction = a\n",
    "\n",
    "        shadow_row, shadow_col, shadow_pass_loc, shadow_dest_idx = self.decode(self.s)\n",
    "        taxi_row, taxi_col, pass_loc, _ = self.decode(s)\n",
    "\n",
    "        # If we are in the fickle step, the passenger has been in the vehicle for at least a step and this step the\n",
    "        # position changed\n",
    "        if (\n",
    "            self.fickle_passenger\n",
    "            and self.fickle_step\n",
    "            and shadow_pass_loc == 4\n",
    "            and (taxi_row != shadow_row or taxi_col != shadow_col)\n",
    "        ):\n",
    "            self.fickle_step = False\n",
    "            possible_destinations = [\n",
    "                i for i in range(len(self.locs)) if i != shadow_dest_idx\n",
    "            ]\n",
    "            dest_idx = self.np_random.choice(possible_destinations)\n",
    "            s = self.encode(taxi_row, taxi_col, pass_loc, dest_idx)\n",
    "\n",
    "        self.s = s\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return int(s), r, t, False, {\"prob\": p, \"action_mask\": self.action_mask(s)}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: int | None = None,\n",
    "        options: dict | None = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "\n",
    "        if self.use_multiple_maps and seed is not None:\n",
    "            map_id = seed % 3\n",
    "            self.current_map_id = map_id\n",
    "            self.desc = self.desc_maps[map_id]\n",
    "            self.P = self.P_maps[map_id]\n",
    "\n",
    "        self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
    "        self.lastaction = None\n",
    "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.3\n",
    "        self.taxi_orientation = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return int(self.s), {\"prob\": 1.0, \"action_mask\": self.action_mask(self.s)}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "        elif self.render_mode == \"ansi\":\n",
    "            return self._render_text()\n",
    "        else:  # self.render_mode in {\"human\", \"rgb_array\"}:\n",
    "            return self._render_gui(self.render_mode)\n",
    "\n",
    "    def _render_gui(self, mode):\n",
    "        try:\n",
    "            import pygame  # dependency to pygame only if rendering with human\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
    "            ) from e\n",
    "\n",
    "        if self.window is None:\n",
    "            pygame.init()\n",
    "            pygame.display.set_caption(\"Taxi\")\n",
    "            if mode == \"human\":\n",
    "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
    "            elif mode == \"rgb_array\":\n",
    "                self.window = pygame.Surface(WINDOW_SIZE)\n",
    "\n",
    "        assert (\n",
    "            self.window is not None\n",
    "        ), \"Something went wrong with pygame. This should never happen.\"\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.taxi_imgs is None:\n",
    "            file_names = [\n",
    "                path.join(path.dirname(__file__), \"img/cab_front.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/cab_rear.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/cab_right.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/cab_left.png\"),\n",
    "            ]\n",
    "            self.taxi_imgs = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.passenger_img is None:\n",
    "            file_name = path.join(path.dirname(__file__), \"img/passenger.png\")\n",
    "            self.passenger_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.destination_img is None:\n",
    "            file_name = path.join(path.dirname(__file__), \"img/hotel.png\")\n",
    "            self.destination_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "            self.destination_img.set_alpha(170)\n",
    "        if self.median_horiz is None:\n",
    "            file_names = [\n",
    "                path.join(path.dirname(__file__), \"img/gridworld_median_left.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/gridworld_median_horiz.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/gridworld_median_right.png\"),\n",
    "            ]\n",
    "            self.median_horiz = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.median_vert is None:\n",
    "            file_names = [\n",
    "                path.join(path.dirname(__file__), \"img/gridworld_median_top.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/gridworld_median_vert.png\"),\n",
    "                path.join(path.dirname(__file__), \"img/gridworld_median_bottom.png\"),\n",
    "            ]\n",
    "            self.median_vert = [\n",
    "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
    "                for file_name in file_names\n",
    "            ]\n",
    "        if self.background_img is None:\n",
    "            file_name = path.join(path.dirname(__file__), \"img/taxi_background.png\")\n",
    "            self.background_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        desc = self.desc\n",
    "\n",
    "        for y in range(0, desc.shape[0]):\n",
    "            for x in range(0, desc.shape[1]):\n",
    "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
    "                self.window.blit(self.background_img, cell)\n",
    "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
    "                    self.window.blit(self.median_vert[0], cell)\n",
    "                elif desc[y][x] == b\"|\" and (\n",
    "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_vert[2], cell)\n",
    "                elif desc[y][x] == b\"|\":\n",
    "                    self.window.blit(self.median_vert[1], cell)\n",
    "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
    "                    self.window.blit(self.median_horiz[0], cell)\n",
    "                elif desc[y][x] == b\"-\" and (\n",
    "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
    "                ):\n",
    "                    self.window.blit(self.median_horiz[2], cell)\n",
    "                elif desc[y][x] == b\"-\":\n",
    "                    self.window.blit(self.median_horiz[1], cell)\n",
    "\n",
    "        for cell, color in zip(self.locs, self.locs_colors):\n",
    "            color_cell = pygame.Surface(self.cell_size)\n",
    "            color_cell.set_alpha(128)\n",
    "            color_cell.fill(color)\n",
    "            loc = self.get_surf_loc(cell)\n",
    "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
    "\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "\n",
    "        if pass_idx < 4:\n",
    "            self.window.blit(self.passenger_img, self.get_surf_loc(self.locs[pass_idx]))\n",
    "\n",
    "        if self.lastaction in [0, 1, 2, 3]:\n",
    "            self.taxi_orientation = self.lastaction\n",
    "        dest_loc = self.get_surf_loc(self.locs[dest_idx])\n",
    "        taxi_location = self.get_surf_loc((taxi_row, taxi_col))\n",
    "\n",
    "        if dest_loc[1] <= taxi_location[1]:\n",
    "            self.window.blit(\n",
    "                self.destination_img,\n",
    "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
    "            )\n",
    "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
    "        else:  # change blit order for overlapping appearance\n",
    "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
    "            self.window.blit(\n",
    "                self.destination_img,\n",
    "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
    "            )\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        elif mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def get_surf_loc(self, map_loc):\n",
    "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
    "            map_loc[0] + 1\n",
    "        ) * self.cell_size[1]\n",
    "\n",
    "    def _render_text(self):\n",
    "        desc = self.desc.copy().tolist()\n",
    "        outfile = StringIO()\n",
    "\n",
    "        out = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
    "\n",
    "        def ul(x):\n",
    "            return \"_\" if x == \" \" else x\n",
    "\n",
    "        if pass_idx < 4:\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                out[1 + taxi_row][2 * taxi_col + 1], \"yellow\", highlight=True\n",
    "            )\n",
    "            pi, pj = self.locs[pass_idx]\n",
    "            out[1 + pi][2 * pj + 1] = utils.colorize(\n",
    "                out[1 + pi][2 * pj + 1], \"blue\", bold=True\n",
    "            )\n",
    "        else:  # passenger in taxi\n",
    "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
    "                ul(out[1 + taxi_row][2 * taxi_col + 1]), \"green\", highlight=True\n",
    "            )\n",
    "\n",
    "        di, dj = self.locs[dest_idx]\n",
    "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], \"magenta\")\n",
    "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\n",
    "                f\"  ({['South', 'North', 'East', 'West', 'Pickup', 'Dropoff'][self.lastaction]})\\n\"\n",
    "            )\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "        with closing(outfile):\n",
    "            return outfile.getvalue()\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe_oj--bRBoR"
   },
   "source": [
    "# Training Strategy(Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LJPgdF2ZRNB9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QMsqppIlRWWI"
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgentOptimized:\n",
    "\n",
    "    def __init__(self, n_states, n_actions,\n",
    "                 learning_rate=0.01,\n",
    "                 value_lr=0.1,\n",
    "                 lr_decay=0.9999,\n",
    "                 lr_min=0.0001,\n",
    "                 discount_factor=0.99,\n",
    "                 entropy_coef=0.01):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.lr_init = learning_rate\n",
    "        self.value_lr = value_lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_min = lr_min\n",
    "        self.gamma = discount_factor\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        # policy parameters\n",
    "        self.theta = np.zeros((n_states, n_actions))\n",
    "\n",
    "        # Value function\n",
    "        self.V = np.zeros(n_states)\n",
    "\n",
    "        # statistics information\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        \"\"\"calculate action probability distribution (softmax)\"\"\"\n",
    "        theta_state = self.theta[state] - np.max(self.theta[state])\n",
    "        exp_theta = np.exp(theta_state)\n",
    "        return exp_theta / np.sum(exp_theta)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"sample action according to policy\"\"\"\n",
    "        policy = self.get_policy(state)\n",
    "        return np.random.choice(self.n_actions, p=policy)\n",
    "\n",
    "    def update(self, episode_history):\n",
    "        \"\"\"\n",
    "        update policy using Advantage and Entropy\n",
    "        \"\"\"\n",
    "        if len(episode_history) == 0:\n",
    "            return\n",
    "\n",
    "        # 1. calculate return G_t\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode_history):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # 2. update Value function\n",
    "        for t, (state, action, reward) in enumerate(episode_history):\n",
    "            td_error = returns[t] - self.V[state]\n",
    "            self.V[state] += self.value_lr * td_error\n",
    "\n",
    "        # 3. calculate Advantage\n",
    "        advantages = []\n",
    "        for t, (state, action, reward) in enumerate(episode_history):\n",
    "            advantage = returns[t] - self.V[state]\n",
    "            advantages.append(advantage)\n",
    "\n",
    "        # 4. standardize Advantage\n",
    "        advantages = np.array(advantages)\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-9)\n",
    "\n",
    "        # 5. update policy parameters\n",
    "        for t, (state, action, reward) in enumerate(episode_history):\n",
    "            policy = self.get_policy(state)\n",
    "\n",
    "            # Policy gradient\n",
    "            grad = np.zeros(self.n_actions)\n",
    "            grad[action] = 1.0\n",
    "            grad -= policy\n",
    "\n",
    "            # Entropy gradient\n",
    "            entropy_grad = -np.log(policy + 1e-9) - 1\n",
    "\n",
    "            # combine update\n",
    "            total_grad = (advantages[t] * grad +\n",
    "                         self.entropy_coef * entropy_grad)\n",
    "\n",
    "            self.theta[state] += self.lr * total_grad\n",
    "\n",
    "        # 6. decay learning rate\n",
    "        self.decay_learning_rate()\n",
    "        self.episode_count += 1\n",
    "\n",
    "    def decay_learning_rate(self):\n",
    "        \"\"\"gradually decrease learning rate\"\"\"\n",
    "        self.lr = max(self.lr_min, self.lr * self.lr_decay)\n",
    "\n",
    "    @property\n",
    "    def Q(self):\n",
    "        \"\"\"compatible test function\"\"\"\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9xcmKP2yJpQ"
   },
   "source": [
    "# Reward Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "X7WUWULGyB2T"
   },
   "outputs": [],
   "source": [
    "class TaxiRewardWrapper(gym.Wrapper): # TODO\n",
    "    \"\"\"\n",
    "    Reward wrapper for Taxi environment\n",
    "    Allows customizing reward values without modifying the original environment\n",
    "    Also synchronizes the underlying env's reward attributes so code that\n",
    "    compares against `env.reward_delivery` etc. sees the customized values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
    "        super().__init__(env)\n",
    "        self.custom_reward_step = reward_step\n",
    "        self.custom_reward_delivery = reward_delivery\n",
    "        self.custom_reward_illegal = reward_illegal\n",
    "\n",
    "        # Mirror custom values onto the wrapped env so internal transitions\n",
    "        # that reference env.reward_* return values consistent with the wrapper.\n",
    "        try:\n",
    "            # Only set attributes if the wrapped env exposes them\n",
    "            setattr(self.env, 'reward_step', reward_step)\n",
    "            setattr(self.env, 'reward_delivery', reward_delivery)\n",
    "            setattr(self.env, 'reward_illegal', reward_illegal)\n",
    "        except Exception:\n",
    "            # Be conservative: if the wrapped env has different API, ignore.\n",
    "            pass\n",
    "\n",
    "        # Expose the custom delivery on the wrapper itself for convenience\n",
    "        self.reward_delivery = self.custom_reward_delivery\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Replace original rewards with custom rewards based on reward type\n",
    "        # (kept for backward compatibility in case the underlying env doesn't\n",
    "        # match the attributes we set above)\n",
    "        if reward == getattr(self.env, 'reward_delivery', None):\n",
    "            reward = self.custom_reward_delivery\n",
    "        elif reward == getattr(self.env, 'reward_illegal', None):\n",
    "            reward = self.custom_reward_illegal\n",
    "        elif reward == getattr(self.env, 'reward_step', None):\n",
    "            reward = self.custom_reward_step\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsWCn5CNR8uu"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BOs5fC-aSDra"
   },
   "outputs": [],
   "source": [
    "def train(n_episodes=50000, max_steps=200,\n",
    "          seed_start=0, seed_end=40000, verbose=True,\n",
    "          reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
    "    \"\"\"train optimized Policy Gradient Agent\"\"\"\n",
    "\n",
    "    # Wrap environment with custom rewards\n",
    "    env = TaxiRewardWrapper(\n",
    "        TaxiEnv(),\n",
    "        reward_step=reward_step,\n",
    "        reward_delivery=reward_delivery,\n",
    "        reward_illegal=reward_illegal\n",
    "    )\n",
    "\n",
    "    agent = PolicyGradientAgentOptimized(\n",
    "        n_states=env.observation_space.n,\n",
    "        n_actions=env.action_space.n,\n",
    "        learning_rate=0.01,      # TODO\n",
    "        value_lr=0.1,         # TODO\n",
    "        lr_decay=0.99999,       # TODO\n",
    "        discount_factor=0.99,     # TODO\n",
    "        entropy_coef=0.1        # TODO\n",
    "    )\n",
    "\n",
    "    episode_rewards = []\n",
    "    success_count = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"training episodes: {n_episodes}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
    "\n",
    "        episode_history = []\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_history.append((state, action, reward))\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if terminated and reward == reward_delivery:\n",
    "                    success_count += 1\n",
    "                break\n",
    "\n",
    "        agent.update(episode_history)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        if verbose and (episode + 1) % 1000 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            success_rate = success_count / min(1000, episode + 1)\n",
    "\n",
    "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"avg reward: {avg_reward:.2f} | \"\n",
    "                  f\"success rate: {success_rate:.1%} | \"\n",
    "                  f\"learning rate: {agent.lr:.6f}\")\n",
    "\n",
    "            if episode >= 99:\n",
    "                success_count = 0\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Training completed!\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg7uAA9uSH8n"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8SX1fiIOSOEb"
   },
   "outputs": [],
   "source": [
    "def test(model_filename, n_episodes=100, seed_start=42, verbose=True,\n",
    "         reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
    "    \"\"\"test agent\"\"\"\n",
    "\n",
    "    # Load model for testing\n",
    "    print(f\"\\nload model\")\n",
    "\n",
    "    # Wrap environment with custom rewards\n",
    "    env = TaxiRewardWrapper(\n",
    "        TaxiEnv(),\n",
    "        reward_step=reward_step,\n",
    "        reward_delivery=reward_delivery,\n",
    "        reward_illegal=reward_illegal\n",
    "    )\n",
    "\n",
    "    agent = PolicyGradientAgentOptimized(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n\n",
    "    )\n",
    "    agent.theta = np.load(model_filename)\n",
    "\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    successes = 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, info = env.reset(seed=seed_start + episode)\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        for step in range(200):\n",
    "            # use deterministic policy during testing\n",
    "            action = np.argmax(agent.theta[state])\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if reward == reward_delivery:\n",
    "                    successes += 1\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        steps_list.append(step_count)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    avg_steps = np.mean(steps_list)\n",
    "    success_rate = successes / n_episodes\n",
    "\n",
    "    # calculate evaluation score (success rate 20%, steps 80%)\n",
    "    normalized_steps = avg_steps / 200\n",
    "    step_score = 1 - normalized_steps\n",
    "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\ntest result (seed {seed_start}-{seed_start+n_episodes-1}):\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"   avg reward: {avg_reward:.2f}\")\n",
    "        print(f\"   avg steps: {avg_steps:.2f}\")\n",
    "        print(f\"   success rate: {success_rate:.1%}\")\n",
    "        print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return {\n",
    "        'avg_reward': avg_reward,\n",
    "        'avg_steps': avg_steps,\n",
    "        'success_rate': success_rate,\n",
    "        'evaluation_score': evaluation_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb6lgXalTOzA"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6KRCuQ4TNjK",
    "outputId": "82a214b7-a589-483c-d37a-008e7206a444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "training episodes: 50000\n",
      "======================================================================\n",
      "episode 1000/50000 | avg reward: -752.78 | success rate: 4.6% | learning rate: 0.009900\n",
      "episode 2000/50000 | avg reward: -730.74 | success rate: 7.5% | learning rate: 0.009802\n",
      "episode 3000/50000 | avg reward: -679.68 | success rate: 9.3% | learning rate: 0.009704\n",
      "episode 4000/50000 | avg reward: -690.49 | success rate: 9.9% | learning rate: 0.009608\n",
      "episode 5000/50000 | avg reward: -690.37 | success rate: 14.1% | learning rate: 0.009512\n",
      "episode 6000/50000 | avg reward: -643.50 | success rate: 18.4% | learning rate: 0.009418\n",
      "episode 7000/50000 | avg reward: -601.08 | success rate: 25.5% | learning rate: 0.009324\n",
      "episode 8000/50000 | avg reward: -617.31 | success rate: 26.6% | learning rate: 0.009231\n",
      "episode 9000/50000 | avg reward: -609.17 | success rate: 30.1% | learning rate: 0.009139\n",
      "episode 10000/50000 | avg reward: -606.84 | success rate: 33.8% | learning rate: 0.009048\n",
      "episode 11000/50000 | avg reward: -571.50 | success rate: 38.2% | learning rate: 0.008958\n",
      "episode 12000/50000 | avg reward: -560.82 | success rate: 41.4% | learning rate: 0.008869\n",
      "episode 13000/50000 | avg reward: -553.20 | success rate: 44.9% | learning rate: 0.008781\n",
      "episode 14000/50000 | avg reward: -556.96 | success rate: 42.2% | learning rate: 0.008694\n",
      "episode 15000/50000 | avg reward: -533.55 | success rate: 43.1% | learning rate: 0.008607\n",
      "episode 16000/50000 | avg reward: -497.58 | success rate: 46.0% | learning rate: 0.008521\n",
      "episode 17000/50000 | avg reward: -524.74 | success rate: 49.0% | learning rate: 0.008437\n",
      "episode 18000/50000 | avg reward: -541.24 | success rate: 46.4% | learning rate: 0.008353\n",
      "episode 19000/50000 | avg reward: -498.37 | success rate: 51.5% | learning rate: 0.008270\n",
      "episode 20000/50000 | avg reward: -508.11 | success rate: 52.0% | learning rate: 0.008187\n",
      "episode 21000/50000 | avg reward: -536.83 | success rate: 50.2% | learning rate: 0.008106\n",
      "episode 22000/50000 | avg reward: -539.98 | success rate: 53.5% | learning rate: 0.008025\n",
      "episode 23000/50000 | avg reward: -468.03 | success rate: 52.4% | learning rate: 0.007945\n",
      "episode 24000/50000 | avg reward: -509.24 | success rate: 52.3% | learning rate: 0.007866\n",
      "episode 25000/50000 | avg reward: -503.25 | success rate: 52.2% | learning rate: 0.007788\n",
      "episode 26000/50000 | avg reward: -489.80 | success rate: 51.0% | learning rate: 0.007711\n",
      "episode 27000/50000 | avg reward: -492.25 | success rate: 52.7% | learning rate: 0.007634\n",
      "episode 28000/50000 | avg reward: -510.52 | success rate: 53.3% | learning rate: 0.007558\n",
      "episode 29000/50000 | avg reward: -472.52 | success rate: 52.6% | learning rate: 0.007483\n",
      "episode 30000/50000 | avg reward: -528.95 | success rate: 51.1% | learning rate: 0.007408\n",
      "episode 31000/50000 | avg reward: -459.89 | success rate: 51.7% | learning rate: 0.007334\n",
      "episode 32000/50000 | avg reward: -505.63 | success rate: 52.3% | learning rate: 0.007261\n",
      "episode 33000/50000 | avg reward: -492.87 | success rate: 52.0% | learning rate: 0.007189\n",
      "episode 34000/50000 | avg reward: -496.98 | success rate: 51.2% | learning rate: 0.007118\n",
      "episode 35000/50000 | avg reward: -498.87 | success rate: 52.6% | learning rate: 0.007047\n",
      "episode 36000/50000 | avg reward: -525.78 | success rate: 52.1% | learning rate: 0.006977\n",
      "episode 37000/50000 | avg reward: -507.17 | success rate: 51.7% | learning rate: 0.006907\n",
      "episode 38000/50000 | avg reward: -493.47 | success rate: 51.5% | learning rate: 0.006839\n",
      "episode 39000/50000 | avg reward: -483.38 | success rate: 54.2% | learning rate: 0.006771\n",
      "episode 40000/50000 | avg reward: -530.20 | success rate: 50.3% | learning rate: 0.006703\n",
      "episode 41000/50000 | avg reward: -520.71 | success rate: 51.5% | learning rate: 0.006636\n",
      "episode 42000/50000 | avg reward: -525.91 | success rate: 52.8% | learning rate: 0.006570\n",
      "episode 43000/50000 | avg reward: -512.17 | success rate: 53.8% | learning rate: 0.006505\n",
      "episode 44000/50000 | avg reward: -489.65 | success rate: 55.3% | learning rate: 0.006440\n",
      "episode 45000/50000 | avg reward: -518.66 | success rate: 51.1% | learning rate: 0.006376\n",
      "episode 46000/50000 | avg reward: -483.48 | success rate: 54.1% | learning rate: 0.006313\n",
      "episode 47000/50000 | avg reward: -496.72 | success rate: 52.7% | learning rate: 0.006250\n",
      "episode 48000/50000 | avg reward: -523.82 | success rate: 53.4% | learning rate: 0.006188\n",
      "episode 49000/50000 | avg reward: -502.40 | success rate: 55.6% | learning rate: 0.006126\n",
      "episode 50000/50000 | avg reward: -502.36 | success rate: 56.5% | learning rate: 0.006065\n",
      "======================================================================\n",
      "Training completed!\n",
      "\n",
      "model saved to policy_gradient_optimized.npy\n",
      "\n",
      "load model\n",
      "\n",
      "test result (seed 420000-420999):\n",
      "======================================================================\n",
      "   avg reward: -135.55\n",
      "   avg steps: 142.23\n",
      "   success rate: 31.8%\n",
      "   evaluation score: 0.2947 (29.47%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Set reward parameters\n",
    "    REWARD_STEP = -1\n",
    "    REWARD_DELIVERY = 20\n",
    "    REWARD_ILLEGAL = -10\n",
    "\n",
    "    # train\n",
    "    agent, rewards = train(\n",
    "        n_episodes=50000,\n",
    "        verbose=True,\n",
    "        reward_step=REWARD_STEP,\n",
    "        reward_delivery=REWARD_DELIVERY,\n",
    "        reward_illegal=REWARD_ILLEGAL\n",
    "    )\n",
    "\n",
    "    # save model\n",
    "    model_filename = 'policy_gradient_optimized.npy'\n",
    "    np.save(model_filename, agent.theta)\n",
    "    print(f\"\\nmodel saved to {model_filename}\")\n",
    "\n",
    "    # test\n",
    "    test(\n",
    "        model_filename,\n",
    "        n_episodes=1000,\n",
    "        seed_start=420000,\n",
    "        verbose=True,\n",
    "        reward_step=REWARD_STEP,\n",
    "        reward_delivery=REWARD_DELIVERY,\n",
    "        reward_illegal=REWARD_ILLEGAL\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DNY5aZHbJ949"
   },
   "outputs": [],
   "source": [
    "reward_combinations = [\n",
    "    {'REWARD_STEP': -1, 'REWARD_DELIVERY': 20, 'REWARD_ILLEGAL': -10}, # Original combination\n",
    "    {'REWARD_STEP': -5, 'REWARD_DELIVERY': 20, 'REWARD_ILLEGAL': -1}, # Higher penalty for steps, lower penalty for illegal actions\n",
    "    {'REWARD_STEP': -1, 'REWARD_DELIVERY': 10, 'REWARD_ILLEGAL': -20}, # Lower reward for delivery, higher penalty for illegal actions\n",
    "    {'REWARD_STEP': -3, 'REWARD_DELIVERY': 25, 'REWARD_ILLEGAL': -5}, # Moderate penalties, higher reward for delivery\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training with REWARD_STEP=-1, REWARD_DELIVERY=20, REWARD_ILLEGAL=-10\n",
      "======================================================================\n",
      "training episodes: 50000\n",
      "======================================================================\n",
      "episode 1000/50000 | avg reward: -745.37 | success rate: 5.3% | learning rate: 0.009900\n",
      "episode 2000/50000 | avg reward: -729.94 | success rate: 8.2% | learning rate: 0.009802\n",
      "episode 3000/50000 | avg reward: -714.77 | success rate: 10.5% | learning rate: 0.009704\n",
      "episode 4000/50000 | avg reward: -640.69 | success rate: 15.4% | learning rate: 0.009608\n",
      "episode 5000/50000 | avg reward: -652.02 | success rate: 18.7% | learning rate: 0.009512\n",
      "episode 6000/50000 | avg reward: -619.32 | success rate: 21.6% | learning rate: 0.009418\n",
      "episode 7000/50000 | avg reward: -609.55 | success rate: 28.4% | learning rate: 0.009324\n",
      "episode 8000/50000 | avg reward: -621.53 | success rate: 28.6% | learning rate: 0.009231\n",
      "episode 9000/50000 | avg reward: -633.55 | success rate: 32.6% | learning rate: 0.009139\n",
      "episode 10000/50000 | avg reward: -553.06 | success rate: 37.0% | learning rate: 0.009048\n",
      "episode 11000/50000 | avg reward: -567.96 | success rate: 37.5% | learning rate: 0.008958\n",
      "episode 12000/50000 | avg reward: -554.08 | success rate: 43.0% | learning rate: 0.008869\n",
      "episode 13000/50000 | avg reward: -513.63 | success rate: 46.6% | learning rate: 0.008781\n",
      "episode 14000/50000 | avg reward: -559.23 | success rate: 43.1% | learning rate: 0.008694\n",
      "episode 15000/50000 | avg reward: -537.06 | success rate: 43.6% | learning rate: 0.008607\n",
      "episode 16000/50000 | avg reward: -547.11 | success rate: 48.2% | learning rate: 0.008521\n",
      "episode 17000/50000 | avg reward: -519.83 | success rate: 44.2% | learning rate: 0.008437\n",
      "episode 18000/50000 | avg reward: -531.64 | success rate: 48.3% | learning rate: 0.008353\n",
      "episode 19000/50000 | avg reward: -501.10 | success rate: 50.8% | learning rate: 0.008270\n",
      "episode 20000/50000 | avg reward: -490.90 | success rate: 49.9% | learning rate: 0.008187\n",
      "episode 21000/50000 | avg reward: -524.11 | success rate: 47.6% | learning rate: 0.008106\n",
      "episode 22000/50000 | avg reward: -503.40 | success rate: 53.5% | learning rate: 0.008025\n",
      "episode 23000/50000 | avg reward: -461.46 | success rate: 50.1% | learning rate: 0.007945\n",
      "episode 24000/50000 | avg reward: -520.02 | success rate: 51.5% | learning rate: 0.007866\n",
      "episode 25000/50000 | avg reward: -474.23 | success rate: 55.3% | learning rate: 0.007788\n",
      "episode 26000/50000 | avg reward: -506.29 | success rate: 56.8% | learning rate: 0.007711\n",
      "episode 27000/50000 | avg reward: -515.98 | success rate: 54.9% | learning rate: 0.007634\n",
      "episode 28000/50000 | avg reward: -472.41 | success rate: 52.8% | learning rate: 0.007558\n",
      "episode 29000/50000 | avg reward: -470.01 | success rate: 54.7% | learning rate: 0.007483\n",
      "episode 30000/50000 | avg reward: -510.17 | success rate: 52.7% | learning rate: 0.007408\n",
      "episode 31000/50000 | avg reward: -519.77 | success rate: 54.8% | learning rate: 0.007334\n",
      "episode 32000/50000 | avg reward: -477.00 | success rate: 53.4% | learning rate: 0.007261\n",
      "episode 33000/50000 | avg reward: -511.48 | success rate: 52.5% | learning rate: 0.007189\n",
      "episode 34000/50000 | avg reward: -463.38 | success rate: 52.5% | learning rate: 0.007118\n",
      "episode 35000/50000 | avg reward: -493.88 | success rate: 54.0% | learning rate: 0.007047\n",
      "episode 36000/50000 | avg reward: -521.36 | success rate: 56.1% | learning rate: 0.006977\n",
      "episode 37000/50000 | avg reward: -458.40 | success rate: 54.5% | learning rate: 0.006907\n",
      "episode 38000/50000 | avg reward: -492.29 | success rate: 53.5% | learning rate: 0.006839\n",
      "episode 39000/50000 | avg reward: -453.51 | success rate: 56.7% | learning rate: 0.006771\n",
      "episode 40000/50000 | avg reward: -491.56 | success rate: 57.2% | learning rate: 0.006703\n",
      "episode 41000/50000 | avg reward: -470.70 | success rate: 57.6% | learning rate: 0.006636\n",
      "episode 42000/50000 | avg reward: -525.28 | success rate: 57.8% | learning rate: 0.006570\n",
      "episode 43000/50000 | avg reward: -494.89 | success rate: 56.9% | learning rate: 0.006505\n",
      "episode 44000/50000 | avg reward: -487.90 | success rate: 55.2% | learning rate: 0.006440\n",
      "episode 45000/50000 | avg reward: -542.61 | success rate: 53.1% | learning rate: 0.006376\n",
      "episode 46000/50000 | avg reward: -499.40 | success rate: 54.5% | learning rate: 0.006313\n",
      "episode 47000/50000 | avg reward: -493.89 | success rate: 54.7% | learning rate: 0.006250\n",
      "episode 48000/50000 | avg reward: -472.12 | success rate: 53.8% | learning rate: 0.006188\n",
      "episode 49000/50000 | avg reward: -485.08 | success rate: 57.9% | learning rate: 0.006126\n",
      "episode 50000/50000 | avg reward: -520.38 | success rate: 54.5% | learning rate: 0.006065\n",
      "======================================================================\n",
      "Training completed!\n",
      "\n",
      "model saved to policy_gradient_step-1_delivery20_illegal-10.npy\n",
      "======================================================================\n",
      "Training with REWARD_STEP=-5, REWARD_DELIVERY=20, REWARD_ILLEGAL=-1\n",
      "======================================================================\n",
      "training episodes: 50000\n",
      "======================================================================\n",
      "episode 1000/50000 | avg reward: -720.21 | success rate: 6.0% | learning rate: 0.009900\n",
      "episode 2000/50000 | avg reward: -721.36 | success rate: 7.4% | learning rate: 0.009802\n",
      "episode 3000/50000 | avg reward: -690.27 | success rate: 10.6% | learning rate: 0.009704\n",
      "episode 4000/50000 | avg reward: -687.97 | success rate: 12.8% | learning rate: 0.009608\n",
      "episode 5000/50000 | avg reward: -684.46 | success rate: 14.8% | learning rate: 0.009512\n",
      "episode 6000/50000 | avg reward: -660.07 | success rate: 19.0% | learning rate: 0.009418\n",
      "episode 7000/50000 | avg reward: -643.67 | success rate: 20.8% | learning rate: 0.009324\n",
      "episode 8000/50000 | avg reward: -654.86 | success rate: 23.1% | learning rate: 0.009231\n",
      "episode 9000/50000 | avg reward: -639.07 | success rate: 27.1% | learning rate: 0.009139\n",
      "episode 10000/50000 | avg reward: -637.01 | success rate: 25.5% | learning rate: 0.009048\n",
      "episode 11000/50000 | avg reward: -604.75 | success rate: 30.1% | learning rate: 0.008958\n",
      "episode 12000/50000 | avg reward: -616.85 | success rate: 29.8% | learning rate: 0.008869\n",
      "episode 13000/50000 | avg reward: -602.22 | success rate: 34.4% | learning rate: 0.008781\n",
      "episode 14000/50000 | avg reward: -605.24 | success rate: 32.5% | learning rate: 0.008694\n",
      "episode 15000/50000 | avg reward: -604.23 | success rate: 34.8% | learning rate: 0.008607\n",
      "episode 16000/50000 | avg reward: -592.12 | success rate: 38.8% | learning rate: 0.008521\n",
      "episode 17000/50000 | avg reward: -612.47 | success rate: 38.4% | learning rate: 0.008437\n",
      "episode 18000/50000 | avg reward: -609.39 | success rate: 39.4% | learning rate: 0.008353\n",
      "episode 19000/50000 | avg reward: -578.35 | success rate: 37.6% | learning rate: 0.008270\n",
      "episode 20000/50000 | avg reward: -599.63 | success rate: 39.1% | learning rate: 0.008187\n",
      "episode 21000/50000 | avg reward: -601.64 | success rate: 40.7% | learning rate: 0.008106\n",
      "episode 22000/50000 | avg reward: -605.53 | success rate: 42.4% | learning rate: 0.008025\n",
      "episode 23000/50000 | avg reward: -591.19 | success rate: 43.3% | learning rate: 0.007945\n",
      "episode 24000/50000 | avg reward: -561.38 | success rate: 42.1% | learning rate: 0.007866\n",
      "episode 25000/50000 | avg reward: -566.66 | success rate: 43.9% | learning rate: 0.007788\n",
      "episode 26000/50000 | avg reward: -557.87 | success rate: 44.1% | learning rate: 0.007711\n",
      "episode 27000/50000 | avg reward: -581.06 | success rate: 41.3% | learning rate: 0.007634\n",
      "episode 28000/50000 | avg reward: -595.39 | success rate: 40.8% | learning rate: 0.007558\n",
      "episode 29000/50000 | avg reward: -570.04 | success rate: 41.4% | learning rate: 0.007483\n",
      "episode 30000/50000 | avg reward: -603.53 | success rate: 39.1% | learning rate: 0.007408\n",
      "episode 31000/50000 | avg reward: -565.59 | success rate: 43.2% | learning rate: 0.007334\n",
      "episode 32000/50000 | avg reward: -552.53 | success rate: 41.0% | learning rate: 0.007261\n",
      "episode 33000/50000 | avg reward: -601.73 | success rate: 41.6% | learning rate: 0.007189\n",
      "episode 34000/50000 | avg reward: -591.34 | success rate: 44.4% | learning rate: 0.007118\n",
      "episode 35000/50000 | avg reward: -581.19 | success rate: 42.0% | learning rate: 0.007047\n",
      "episode 36000/50000 | avg reward: -623.12 | success rate: 44.3% | learning rate: 0.006977\n",
      "episode 37000/50000 | avg reward: -552.84 | success rate: 40.4% | learning rate: 0.006907\n",
      "episode 38000/50000 | avg reward: -594.68 | success rate: 41.9% | learning rate: 0.006839\n",
      "episode 39000/50000 | avg reward: -580.37 | success rate: 40.7% | learning rate: 0.006771\n",
      "episode 40000/50000 | avg reward: -610.14 | success rate: 41.2% | learning rate: 0.006703\n",
      "episode 41000/50000 | avg reward: -557.76 | success rate: 44.1% | learning rate: 0.006636\n",
      "episode 42000/50000 | avg reward: -600.63 | success rate: 46.8% | learning rate: 0.006570\n",
      "episode 43000/50000 | avg reward: -563.51 | success rate: 47.6% | learning rate: 0.006505\n",
      "episode 44000/50000 | avg reward: -571.30 | success rate: 45.0% | learning rate: 0.006440\n",
      "episode 45000/50000 | avg reward: -593.43 | success rate: 45.5% | learning rate: 0.006376\n",
      "episode 46000/50000 | avg reward: -551.46 | success rate: 46.4% | learning rate: 0.006313\n",
      "episode 47000/50000 | avg reward: -570.94 | success rate: 45.7% | learning rate: 0.006250\n",
      "episode 48000/50000 | avg reward: -560.42 | success rate: 47.8% | learning rate: 0.006188\n",
      "episode 49000/50000 | avg reward: -607.61 | success rate: 45.8% | learning rate: 0.006126\n",
      "episode 50000/50000 | avg reward: -573.13 | success rate: 45.0% | learning rate: 0.006065\n",
      "======================================================================\n",
      "Training completed!\n",
      "\n",
      "model saved to policy_gradient_step-5_delivery20_illegal-1.npy\n",
      "======================================================================\n",
      "Training with REWARD_STEP=-1, REWARD_DELIVERY=10, REWARD_ILLEGAL=-20\n",
      "======================================================================\n",
      "training episodes: 50000\n",
      "======================================================================\n",
      "episode 1000/50000 | avg reward: -1340.66 | success rate: 4.4% | learning rate: 0.009900\n",
      "episode 2000/50000 | avg reward: -1298.14 | success rate: 9.3% | learning rate: 0.009802\n",
      "episode 3000/50000 | avg reward: -1199.18 | success rate: 10.3% | learning rate: 0.009704\n",
      "episode 4000/50000 | avg reward: -1265.97 | success rate: 14.2% | learning rate: 0.009608\n",
      "episode 5000/50000 | avg reward: -1209.36 | success rate: 17.2% | learning rate: 0.009512\n",
      "episode 6000/50000 | avg reward: -1183.68 | success rate: 21.0% | learning rate: 0.009418\n",
      "episode 7000/50000 | avg reward: -1146.77 | success rate: 25.7% | learning rate: 0.009324\n",
      "episode 8000/50000 | avg reward: -1094.45 | success rate: 28.5% | learning rate: 0.009231\n",
      "episode 9000/50000 | avg reward: -1072.51 | success rate: 34.4% | learning rate: 0.009139\n",
      "episode 10000/50000 | avg reward: -1031.31 | success rate: 34.3% | learning rate: 0.009048\n",
      "episode 11000/50000 | avg reward: -1012.53 | success rate: 37.0% | learning rate: 0.008958\n",
      "episode 12000/50000 | avg reward: -1001.58 | success rate: 41.0% | learning rate: 0.008869\n",
      "episode 13000/50000 | avg reward: -1011.99 | success rate: 43.8% | learning rate: 0.008781\n",
      "episode 14000/50000 | avg reward: -966.39 | success rate: 41.5% | learning rate: 0.008694\n",
      "episode 15000/50000 | avg reward: -903.29 | success rate: 43.3% | learning rate: 0.008607\n",
      "episode 16000/50000 | avg reward: -913.13 | success rate: 46.9% | learning rate: 0.008521\n",
      "episode 17000/50000 | avg reward: -967.67 | success rate: 46.8% | learning rate: 0.008437\n",
      "episode 18000/50000 | avg reward: -950.12 | success rate: 47.8% | learning rate: 0.008353\n",
      "episode 19000/50000 | avg reward: -950.51 | success rate: 44.8% | learning rate: 0.008270\n",
      "episode 20000/50000 | avg reward: -988.30 | success rate: 44.8% | learning rate: 0.008187\n",
      "episode 21000/50000 | avg reward: -940.97 | success rate: 47.6% | learning rate: 0.008106\n",
      "episode 22000/50000 | avg reward: -1016.97 | success rate: 49.4% | learning rate: 0.008025\n",
      "episode 23000/50000 | avg reward: -887.67 | success rate: 49.4% | learning rate: 0.007945\n",
      "episode 24000/50000 | avg reward: -903.95 | success rate: 51.9% | learning rate: 0.007866\n",
      "episode 25000/50000 | avg reward: -871.74 | success rate: 50.9% | learning rate: 0.007788\n",
      "episode 26000/50000 | avg reward: -878.43 | success rate: 53.2% | learning rate: 0.007711\n",
      "episode 27000/50000 | avg reward: -965.13 | success rate: 47.8% | learning rate: 0.007634\n",
      "episode 28000/50000 | avg reward: -900.09 | success rate: 48.9% | learning rate: 0.007558\n",
      "episode 29000/50000 | avg reward: -829.13 | success rate: 53.1% | learning rate: 0.007483\n",
      "episode 30000/50000 | avg reward: -909.58 | success rate: 51.1% | learning rate: 0.007408\n",
      "episode 31000/50000 | avg reward: -935.39 | success rate: 53.1% | learning rate: 0.007334\n",
      "episode 32000/50000 | avg reward: -867.94 | success rate: 51.3% | learning rate: 0.007261\n",
      "episode 33000/50000 | avg reward: -885.49 | success rate: 53.3% | learning rate: 0.007189\n",
      "episode 34000/50000 | avg reward: -882.96 | success rate: 51.7% | learning rate: 0.007118\n",
      "episode 35000/50000 | avg reward: -911.26 | success rate: 56.5% | learning rate: 0.007047\n",
      "episode 36000/50000 | avg reward: -958.58 | success rate: 52.1% | learning rate: 0.006977\n",
      "episode 37000/50000 | avg reward: -842.43 | success rate: 53.1% | learning rate: 0.006907\n",
      "episode 38000/50000 | avg reward: -919.12 | success rate: 48.6% | learning rate: 0.006839\n",
      "episode 39000/50000 | avg reward: -847.34 | success rate: 53.8% | learning rate: 0.006771\n",
      "episode 40000/50000 | avg reward: -848.64 | success rate: 51.8% | learning rate: 0.006703\n",
      "episode 41000/50000 | avg reward: -880.95 | success rate: 53.2% | learning rate: 0.006636\n",
      "episode 42000/50000 | avg reward: -889.06 | success rate: 54.2% | learning rate: 0.006570\n",
      "episode 43000/50000 | avg reward: -847.02 | success rate: 53.7% | learning rate: 0.006505\n",
      "episode 44000/50000 | avg reward: -927.28 | success rate: 53.6% | learning rate: 0.006440\n",
      "episode 45000/50000 | avg reward: -934.72 | success rate: 52.2% | learning rate: 0.006376\n",
      "episode 46000/50000 | avg reward: -918.39 | success rate: 54.8% | learning rate: 0.006313\n",
      "episode 47000/50000 | avg reward: -832.52 | success rate: 56.2% | learning rate: 0.006250\n",
      "episode 48000/50000 | avg reward: -875.95 | success rate: 50.8% | learning rate: 0.006188\n",
      "episode 49000/50000 | avg reward: -899.69 | success rate: 54.7% | learning rate: 0.006126\n",
      "episode 50000/50000 | avg reward: -896.50 | success rate: 51.8% | learning rate: 0.006065\n",
      "======================================================================\n",
      "Training completed!\n",
      "\n",
      "model saved to policy_gradient_step-1_delivery10_illegal-20.npy\n",
      "======================================================================\n",
      "Training with REWARD_STEP=-3, REWARD_DELIVERY=25, REWARD_ILLEGAL=-5\n",
      "======================================================================\n",
      "training episodes: 50000\n",
      "======================================================================\n",
      "episode 1000/50000 | avg reward: -715.09 | success rate: 4.6% | learning rate: 0.009900\n",
      "episode 2000/50000 | avg reward: -708.30 | success rate: 7.5% | learning rate: 0.009802\n",
      "episode 3000/50000 | avg reward: -690.51 | success rate: 10.4% | learning rate: 0.009704\n",
      "episode 4000/50000 | avg reward: -686.45 | success rate: 11.2% | learning rate: 0.009608\n",
      "episode 5000/50000 | avg reward: -695.40 | success rate: 13.3% | learning rate: 0.009512\n",
      "episode 6000/50000 | avg reward: -630.92 | success rate: 19.9% | learning rate: 0.009418\n",
      "episode 7000/50000 | avg reward: -627.31 | success rate: 23.0% | learning rate: 0.009324\n",
      "episode 8000/50000 | avg reward: -648.40 | success rate: 24.4% | learning rate: 0.009231\n",
      "episode 9000/50000 | avg reward: -639.66 | success rate: 30.9% | learning rate: 0.009139\n",
      "episode 10000/50000 | avg reward: -617.03 | success rate: 29.9% | learning rate: 0.009048\n",
      "episode 11000/50000 | avg reward: -607.47 | success rate: 32.5% | learning rate: 0.008958\n",
      "episode 12000/50000 | avg reward: -577.34 | success rate: 39.8% | learning rate: 0.008869\n",
      "episode 13000/50000 | avg reward: -589.76 | success rate: 40.2% | learning rate: 0.008781\n",
      "episode 14000/50000 | avg reward: -554.54 | success rate: 42.5% | learning rate: 0.008694\n",
      "episode 15000/50000 | avg reward: -585.69 | success rate: 42.4% | learning rate: 0.008607\n",
      "episode 16000/50000 | avg reward: -529.08 | success rate: 46.5% | learning rate: 0.008521\n",
      "episode 17000/50000 | avg reward: -551.24 | success rate: 44.7% | learning rate: 0.008437\n",
      "episode 18000/50000 | avg reward: -518.90 | success rate: 51.4% | learning rate: 0.008353\n",
      "episode 19000/50000 | avg reward: -527.63 | success rate: 49.4% | learning rate: 0.008270\n",
      "episode 20000/50000 | avg reward: -547.75 | success rate: 47.1% | learning rate: 0.008187\n",
      "episode 21000/50000 | avg reward: -535.97 | success rate: 45.5% | learning rate: 0.008106\n",
      "episode 22000/50000 | avg reward: -591.08 | success rate: 48.7% | learning rate: 0.008025\n",
      "episode 23000/50000 | avg reward: -531.82 | success rate: 45.8% | learning rate: 0.007945\n",
      "episode 24000/50000 | avg reward: -570.37 | success rate: 45.7% | learning rate: 0.007866\n",
      "episode 25000/50000 | avg reward: -505.42 | success rate: 48.3% | learning rate: 0.007788\n",
      "episode 26000/50000 | avg reward: -515.52 | success rate: 51.1% | learning rate: 0.007711\n",
      "episode 27000/50000 | avg reward: -537.23 | success rate: 50.4% | learning rate: 0.007634\n",
      "episode 28000/50000 | avg reward: -550.44 | success rate: 49.0% | learning rate: 0.007558\n",
      "episode 29000/50000 | avg reward: -488.08 | success rate: 52.8% | learning rate: 0.007483\n",
      "episode 30000/50000 | avg reward: -573.58 | success rate: 47.8% | learning rate: 0.007408\n",
      "episode 31000/50000 | avg reward: -539.25 | success rate: 52.1% | learning rate: 0.007334\n",
      "episode 32000/50000 | avg reward: -517.45 | success rate: 52.8% | learning rate: 0.007261\n",
      "episode 33000/50000 | avg reward: -509.31 | success rate: 52.3% | learning rate: 0.007189\n",
      "episode 34000/50000 | avg reward: -513.97 | success rate: 51.4% | learning rate: 0.007118\n",
      "episode 35000/50000 | avg reward: -529.41 | success rate: 54.8% | learning rate: 0.007047\n",
      "episode 36000/50000 | avg reward: -530.66 | success rate: 53.4% | learning rate: 0.006977\n",
      "episode 37000/50000 | avg reward: -529.89 | success rate: 48.8% | learning rate: 0.006907\n",
      "episode 38000/50000 | avg reward: -533.40 | success rate: 51.2% | learning rate: 0.006839\n",
      "episode 39000/50000 | avg reward: -512.93 | success rate: 51.7% | learning rate: 0.006771\n",
      "episode 40000/50000 | avg reward: -584.88 | success rate: 50.7% | learning rate: 0.006703\n",
      "episode 41000/50000 | avg reward: -536.54 | success rate: 51.4% | learning rate: 0.006636\n",
      "episode 42000/50000 | avg reward: -556.75 | success rate: 51.6% | learning rate: 0.006570\n",
      "episode 43000/50000 | avg reward: -510.09 | success rate: 53.1% | learning rate: 0.006505\n",
      "episode 44000/50000 | avg reward: -500.81 | success rate: 52.7% | learning rate: 0.006440\n",
      "episode 45000/50000 | avg reward: -575.98 | success rate: 48.2% | learning rate: 0.006376\n",
      "episode 46000/50000 | avg reward: -562.20 | success rate: 51.3% | learning rate: 0.006313\n",
      "episode 47000/50000 | avg reward: -553.77 | success rate: 48.2% | learning rate: 0.006250\n",
      "episode 48000/50000 | avg reward: -548.82 | success rate: 51.1% | learning rate: 0.006188\n",
      "episode 49000/50000 | avg reward: -512.60 | success rate: 50.7% | learning rate: 0.006126\n",
      "episode 50000/50000 | avg reward: -531.05 | success rate: 54.1% | learning rate: 0.006065\n",
      "======================================================================\n",
      "Training completed!\n",
      "\n",
      "model saved to policy_gradient_step-3_delivery25_illegal-5.npy\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = []\n",
    "\n",
    "for rewards in reward_combinations:\n",
    "    REWARD_STEP = rewards['REWARD_STEP']\n",
    "    REWARD_DELIVERY = rewards['REWARD_DELIVERY']\n",
    "    REWARD_ILLEGAL = rewards['REWARD_ILLEGAL']\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training with REWARD_STEP={REWARD_STEP}, REWARD_DELIVERY={REWARD_DELIVERY}, REWARD_ILLEGAL={REWARD_ILLEGAL}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    agent, episode_rewards = train(\n",
    "        n_episodes=50000,\n",
    "        verbose=True,\n",
    "        reward_step=REWARD_STEP,\n",
    "        reward_delivery=REWARD_DELIVERY,\n",
    "        reward_illegal=REWARD_ILLEGAL\n",
    "    )\n",
    "\n",
    "    model_filename = f'policy_gradient_step{REWARD_STEP}_delivery{REWARD_DELIVERY}_illegal{REWARD_ILLEGAL}.npy'\n",
    "    np.save(model_filename, agent.theta)\n",
    "    print(f\"\\nmodel saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./policy_gradient_step-1_delivery20_illegal-10.npy\n",
      "\n",
      "load model\n",
      "\n",
      "test result (seed 420000-420999):\n",
      "======================================================================\n",
      "   avg reward: -415.52\n",
      "   avg steps: 141.49\n",
      "   success rate: 31.9%\n",
      "   evaluation score: 0.2979 (29.79%)\n",
      "======================================================================\n",
      "./policy_gradient_step-3_delivery25_illegal-5.npy\n",
      "\n",
      "load model\n",
      "\n",
      "test result (seed 420000-420999):\n",
      "======================================================================\n",
      "   avg reward: -455.33\n",
      "   avg steps: 146.12\n",
      "   success rate: 29.0%\n",
      "   evaluation score: 0.2735 (27.35%)\n",
      "======================================================================\n",
      "./policy_gradient_step-5_delivery20_illegal-1.npy\n",
      "\n",
      "load model\n",
      "\n",
      "test result (seed 420000-420999):\n",
      "======================================================================\n",
      "   avg reward: -688.70\n",
      "   avg steps: 160.49\n",
      "   success rate: 21.1%\n",
      "   evaluation score: 0.2003 (20.03%)\n",
      "======================================================================\n",
      "./policy_gradient_step-1_delivery10_illegal-20.npy\n",
      "\n",
      "load model\n",
      "\n",
      "test result (seed 420000-420999):\n",
      "======================================================================\n",
      "   avg reward: -394.47\n",
      "   avg steps: 134.79\n",
      "   success rate: 35.4%\n",
      "   evaluation score: 0.3316 (33.16%)\n",
      "======================================================================\n",
      "./policy_gradient_optimized.npy\n",
      "\n",
      "load model\n",
      "\n",
      "test result (seed 420000-420999):\n",
      "======================================================================\n",
      "   avg reward: -421.62\n",
      "   avg steps: 143.40\n",
      "   success rate: 30.6%\n",
      "   evaluation score: 0.2876 (28.76%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# # test\n",
    "# test(\n",
    "#     model_filename,\n",
    "#     n_episodes=1000,\n",
    "#     seed_start=420000,\n",
    "#     verbose=True,\n",
    "#     reward_step=REWARD_STEP,\n",
    "#     reward_delivery=REWARD_DELIVERY,\n",
    "#     reward_illegal=REWARD_ILLEGAL\n",
    "# )\n",
    "\n",
    "\n",
    "import os\n",
    "for file in os.listdir(\"./\"):\n",
    "    if file.endswith(\".npy\"):\n",
    "        model_filename = os.path.join(\"\", file)\n",
    "        print(os.path.join(\"./\", model_filename))\n",
    "        test(\n",
    "            model_filename,\n",
    "            n_episodes=1000,\n",
    "            seed_start=420000,\n",
    "            verbose=True,\n",
    "            reward_step=REWARD_STEP,\n",
    "            reward_delivery=REWARD_DELIVERY,\n",
    "            reward_illegal=REWARD_ILLEGAL\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCloserWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Reward shaping wrapper that gives a small positive reward when the taxi moves\n",
    "    closer to the current passenger (if not picked up) or to the destination\n",
    "    (if passenger is onboard). This keeps `TaxiEnv` unchanged and is toggleable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, shaping_reward=0.5):\n",
    "        super().__init__(env)\n",
    "        self.shaping_reward = shaping_reward\n",
    "        self.prev_state = None\n",
    "\n",
    "    def _manhattan(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        try:\n",
    "            if self.prev_state is None:\n",
    "                self.prev_state = obs\n",
    "\n",
    "            prev_decoded = self.env.decode(self.prev_state)\n",
    "            curr_decoded = self.env.decode(obs)\n",
    "            prev_row, prev_col, prev_pass_idx, prev_dest = prev_decoded\n",
    "            curr_row, curr_col, curr_pass_idx, curr_dest = curr_decoded\n",
    "\n",
    "            if prev_pass_idx < 4:\n",
    "                target_prev = self.env.locs[prev_pass_idx]\n",
    "            else:\n",
    "                target_prev = self.env.locs[prev_dest]\n",
    "\n",
    "            if curr_pass_idx < 4:\n",
    "                target_curr = self.env.locs[curr_pass_idx]\n",
    "            else:\n",
    "                target_curr = self.env.locs[curr_dest]\n",
    "\n",
    "            prev_dist = self._manhattan((prev_row, prev_col), target_prev)\n",
    "            curr_dist = self._manhattan((curr_row, curr_col), target_curr)\n",
    "\n",
    "            if curr_dist < prev_dist and not terminated:\n",
    "                reward = reward + self.shaping_reward\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        self.prev_state = obs\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained 500/50000 | avg recent reward: -759.52\n",
      "trained 1000/50000 | avg recent reward: -737.78\n",
      "trained 1000/50000 | avg recent reward: -737.78\n",
      "trained 1500/50000 | avg recent reward: -717.49\n",
      "trained 1500/50000 | avg recent reward: -717.49\n",
      "trained 2000/50000 | avg recent reward: -725.37\n",
      "trained 2000/50000 | avg recent reward: -725.37\n",
      "trained 2500/50000 | avg recent reward: -681.19\n",
      "trained 2500/50000 | avg recent reward: -681.19\n",
      "trained 3000/50000 | avg recent reward: -670.33\n",
      "trained 3000/50000 | avg recent reward: -670.33\n",
      "trained 3500/50000 | avg recent reward: -677.06\n",
      "trained 3500/50000 | avg recent reward: -677.06\n",
      "trained 4000/50000 | avg recent reward: -691.62\n",
      "trained 4000/50000 | avg recent reward: -691.62\n",
      "trained 4500/50000 | avg recent reward: -660.64\n",
      "trained 4500/50000 | avg recent reward: -660.64\n",
      "trained 5000/50000 | avg recent reward: -654.75\n",
      "trained 5000/50000 | avg recent reward: -654.75\n",
      "trained 5500/50000 | avg recent reward: -629.75\n",
      "trained 5500/50000 | avg recent reward: -629.75\n",
      "trained 6000/50000 | avg recent reward: -617.59\n",
      "trained 6000/50000 | avg recent reward: -617.59\n",
      "trained 6500/50000 | avg recent reward: -619.41\n",
      "trained 6500/50000 | avg recent reward: -619.41\n",
      "trained 7000/50000 | avg recent reward: -634.23\n",
      "trained 7000/50000 | avg recent reward: -634.23\n",
      "trained 7500/50000 | avg recent reward: -586.47\n",
      "trained 7500/50000 | avg recent reward: -586.47\n",
      "trained 8000/50000 | avg recent reward: -623.13\n",
      "trained 8000/50000 | avg recent reward: -623.13\n",
      "trained 8500/50000 | avg recent reward: -570.63\n",
      "trained 8500/50000 | avg recent reward: -570.63\n",
      "trained 9000/50000 | avg recent reward: -562.43\n",
      "trained 9000/50000 | avg recent reward: -562.43\n",
      "trained 9500/50000 | avg recent reward: -598.34\n",
      "trained 9500/50000 | avg recent reward: -598.34\n",
      "trained 10000/50000 | avg recent reward: -573.53\n",
      "trained 10000/50000 | avg recent reward: -573.53\n",
      "trained 10500/50000 | avg recent reward: -593.34\n",
      "trained 10500/50000 | avg recent reward: -593.34\n",
      "trained 11000/50000 | avg recent reward: -550.36\n",
      "trained 11000/50000 | avg recent reward: -550.36\n",
      "trained 11500/50000 | avg recent reward: -591.11\n",
      "trained 11500/50000 | avg recent reward: -591.11\n",
      "trained 12000/50000 | avg recent reward: -575.83\n",
      "trained 12000/50000 | avg recent reward: -575.83\n",
      "trained 12500/50000 | avg recent reward: -541.93\n",
      "trained 12500/50000 | avg recent reward: -541.93\n",
      "trained 13000/50000 | avg recent reward: -556.12\n",
      "trained 13000/50000 | avg recent reward: -556.12\n",
      "trained 13500/50000 | avg recent reward: -548.65\n",
      "trained 13500/50000 | avg recent reward: -548.65\n",
      "trained 14000/50000 | avg recent reward: -546.39\n",
      "trained 14000/50000 | avg recent reward: -546.39\n",
      "trained 14500/50000 | avg recent reward: -546.71\n",
      "trained 14500/50000 | avg recent reward: -546.71\n",
      "trained 15000/50000 | avg recent reward: -515.55\n",
      "trained 15000/50000 | avg recent reward: -515.55\n",
      "trained 15500/50000 | avg recent reward: -555.56\n",
      "trained 15500/50000 | avg recent reward: -555.56\n",
      "trained 16000/50000 | avg recent reward: -540.18\n",
      "trained 16000/50000 | avg recent reward: -540.18\n",
      "trained 16500/50000 | avg recent reward: -518.66\n",
      "trained 16500/50000 | avg recent reward: -518.66\n",
      "trained 17000/50000 | avg recent reward: -555.57\n",
      "trained 17000/50000 | avg recent reward: -555.57\n",
      "trained 17500/50000 | avg recent reward: -537.22\n",
      "trained 17500/50000 | avg recent reward: -537.22\n",
      "trained 18000/50000 | avg recent reward: -488.12\n",
      "trained 18000/50000 | avg recent reward: -488.12\n",
      "trained 18500/50000 | avg recent reward: -558.60\n",
      "trained 18500/50000 | avg recent reward: -558.60\n",
      "trained 19000/50000 | avg recent reward: -509.05\n",
      "trained 19000/50000 | avg recent reward: -509.05\n",
      "trained 19500/50000 | avg recent reward: -497.76\n",
      "trained 19500/50000 | avg recent reward: -497.76\n",
      "trained 20000/50000 | avg recent reward: -527.54\n",
      "trained 20000/50000 | avg recent reward: -527.54\n",
      "trained 20500/50000 | avg recent reward: -518.40\n",
      "trained 20500/50000 | avg recent reward: -518.40\n",
      "trained 21000/50000 | avg recent reward: -512.78\n",
      "trained 21000/50000 | avg recent reward: -512.78\n",
      "trained 21500/50000 | avg recent reward: -487.37\n",
      "trained 21500/50000 | avg recent reward: -487.37\n",
      "trained 22000/50000 | avg recent reward: -536.64\n",
      "trained 22000/50000 | avg recent reward: -536.64\n",
      "trained 22500/50000 | avg recent reward: -572.84\n",
      "trained 22500/50000 | avg recent reward: -572.84\n",
      "trained 23000/50000 | avg recent reward: -498.24\n",
      "trained 23000/50000 | avg recent reward: -498.24\n",
      "trained 23500/50000 | avg recent reward: -522.22\n",
      "trained 23500/50000 | avg recent reward: -522.22\n",
      "trained 24000/50000 | avg recent reward: -530.82\n",
      "trained 24000/50000 | avg recent reward: -530.82\n",
      "trained 24500/50000 | avg recent reward: -462.86\n",
      "trained 24500/50000 | avg recent reward: -462.86\n",
      "trained 25000/50000 | avg recent reward: -476.56\n",
      "trained 25000/50000 | avg recent reward: -476.56\n",
      "trained 25500/50000 | avg recent reward: -495.64\n",
      "trained 25500/50000 | avg recent reward: -495.64\n",
      "trained 26000/50000 | avg recent reward: -485.66\n",
      "trained 26000/50000 | avg recent reward: -485.66\n",
      "trained 26500/50000 | avg recent reward: -509.26\n",
      "trained 26500/50000 | avg recent reward: -509.26\n",
      "trained 27000/50000 | avg recent reward: -477.02\n",
      "trained 27000/50000 | avg recent reward: -477.02\n",
      "trained 27500/50000 | avg recent reward: -509.95\n",
      "trained 27500/50000 | avg recent reward: -509.95\n",
      "trained 28000/50000 | avg recent reward: -515.89\n",
      "trained 28000/50000 | avg recent reward: -515.89\n",
      "trained 28500/50000 | avg recent reward: -483.54\n",
      "trained 28500/50000 | avg recent reward: -483.54\n",
      "trained 29000/50000 | avg recent reward: -475.21\n",
      "trained 29000/50000 | avg recent reward: -475.21\n",
      "trained 29500/50000 | avg recent reward: -515.77\n",
      "trained 29500/50000 | avg recent reward: -515.77\n",
      "trained 30000/50000 | avg recent reward: -513.13\n",
      "trained 30000/50000 | avg recent reward: -513.13\n",
      "trained 30500/50000 | avg recent reward: -500.53\n",
      "trained 30500/50000 | avg recent reward: -500.53\n",
      "trained 31000/50000 | avg recent reward: -499.83\n",
      "trained 31000/50000 | avg recent reward: -499.83\n",
      "trained 31500/50000 | avg recent reward: -481.55\n",
      "trained 31500/50000 | avg recent reward: -481.55\n",
      "trained 32000/50000 | avg recent reward: -513.50\n",
      "trained 32000/50000 | avg recent reward: -513.50\n",
      "trained 32500/50000 | avg recent reward: -534.89\n",
      "trained 32500/50000 | avg recent reward: -534.89\n",
      "trained 33000/50000 | avg recent reward: -504.02\n",
      "trained 33000/50000 | avg recent reward: -504.02\n",
      "trained 33500/50000 | avg recent reward: -505.77\n",
      "trained 33500/50000 | avg recent reward: -505.77\n",
      "trained 34000/50000 | avg recent reward: -499.77\n",
      "trained 34000/50000 | avg recent reward: -499.77\n",
      "trained 34500/50000 | avg recent reward: -478.07\n",
      "trained 34500/50000 | avg recent reward: -478.07\n",
      "trained 35000/50000 | avg recent reward: -500.40\n",
      "trained 35000/50000 | avg recent reward: -500.40\n",
      "trained 35500/50000 | avg recent reward: -455.93\n",
      "trained 35500/50000 | avg recent reward: -455.93\n",
      "trained 36000/50000 | avg recent reward: -510.91\n",
      "trained 36000/50000 | avg recent reward: -510.91\n",
      "trained 36500/50000 | avg recent reward: -477.96\n",
      "trained 36500/50000 | avg recent reward: -477.96\n",
      "trained 37000/50000 | avg recent reward: -447.00\n",
      "trained 37000/50000 | avg recent reward: -447.00\n",
      "trained 37500/50000 | avg recent reward: -502.66\n",
      "trained 37500/50000 | avg recent reward: -502.66\n",
      "trained 38000/50000 | avg recent reward: -511.53\n",
      "trained 38000/50000 | avg recent reward: -511.53\n",
      "trained 38500/50000 | avg recent reward: -508.37\n",
      "trained 38500/50000 | avg recent reward: -508.37\n",
      "trained 39000/50000 | avg recent reward: -481.92\n",
      "trained 39000/50000 | avg recent reward: -481.92\n",
      "trained 39500/50000 | avg recent reward: -500.27\n",
      "trained 39500/50000 | avg recent reward: -500.27\n",
      "trained 40000/50000 | avg recent reward: -529.13\n",
      "trained 40000/50000 | avg recent reward: -529.13\n",
      "trained 40500/50000 | avg recent reward: -497.47\n",
      "trained 40500/50000 | avg recent reward: -497.47\n",
      "trained 41000/50000 | avg recent reward: -519.95\n",
      "trained 41000/50000 | avg recent reward: -519.95\n",
      "trained 41500/50000 | avg recent reward: -503.83\n",
      "trained 41500/50000 | avg recent reward: -503.83\n",
      "trained 42000/50000 | avg recent reward: -508.01\n",
      "trained 42000/50000 | avg recent reward: -508.01\n",
      "trained 42500/50000 | avg recent reward: -527.99\n",
      "trained 42500/50000 | avg recent reward: -527.99\n",
      "trained 43000/50000 | avg recent reward: -511.90\n",
      "trained 43000/50000 | avg recent reward: -511.90\n",
      "trained 43500/50000 | avg recent reward: -515.46\n",
      "trained 43500/50000 | avg recent reward: -515.46\n",
      "trained 44000/50000 | avg recent reward: -508.84\n",
      "trained 44000/50000 | avg recent reward: -508.84\n",
      "trained 44500/50000 | avg recent reward: -485.20\n",
      "trained 44500/50000 | avg recent reward: -485.20\n",
      "trained 45000/50000 | avg recent reward: -503.55\n",
      "trained 45000/50000 | avg recent reward: -503.55\n",
      "trained 45500/50000 | avg recent reward: -502.03\n",
      "trained 45500/50000 | avg recent reward: -502.03\n",
      "trained 46000/50000 | avg recent reward: -501.29\n",
      "trained 46000/50000 | avg recent reward: -501.29\n",
      "trained 46500/50000 | avg recent reward: -462.08\n",
      "trained 46500/50000 | avg recent reward: -462.08\n",
      "trained 47000/50000 | avg recent reward: -459.33\n",
      "trained 47000/50000 | avg recent reward: -459.33\n",
      "trained 47500/50000 | avg recent reward: -473.50\n",
      "trained 47500/50000 | avg recent reward: -473.50\n",
      "trained 48000/50000 | avg recent reward: -479.21\n",
      "trained 48000/50000 | avg recent reward: -479.21\n",
      "trained 48500/50000 | avg recent reward: -488.82\n",
      "trained 48500/50000 | avg recent reward: -488.82\n",
      "trained 49000/50000 | avg recent reward: -482.32\n",
      "trained 49000/50000 | avg recent reward: -482.32\n",
      "trained 49500/50000 | avg recent reward: -472.51\n",
      "trained 49500/50000 | avg recent reward: -472.51\n",
      "trained 50000/50000 | avg recent reward: -467.94\n",
      "saved model to policy_gradient_shaped.npy\n",
      "trained 50000/50000 | avg recent reward: -467.94\n",
      "saved model to policy_gradient_shaped.npy\n",
      "\n",
      "shaped test result:\n",
      "============================================================\n",
      "   avg reward: -135.01\n",
      "   avg steps: 141.62\n",
      "   success rate: 31.5%\n",
      "   evaluation score: 0.2965 (29.65%)\n",
      "============================================================\n",
      "\n",
      "shaped test result:\n",
      "============================================================\n",
      "   avg reward: -135.01\n",
      "   avg steps: 141.62\n",
      "   success rate: 31.5%\n",
      "   evaluation score: 0.2965 (29.65%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick shaping experiment: short train + eval using the wrapper\n",
    "# NOTE: this cell runs a compact training loop (uses the PolicyGradientAgentOptimized defined earlier)\n",
    "\n",
    "n_episodes = 50000\n",
    "max_steps = 200\n",
    "shaping_reward = 0.5\n",
    "\n",
    "# Build wrapped environment for training\n",
    "env = RewardCloserWrapper(\n",
    "    TaxiRewardWrapper(TaxiEnv(), reward_step=-1, reward_delivery=20, reward_illegal=-10),\n",
    "    shaping_reward=shaping_reward,\n",
    ")\n",
    "\n",
    "agent = PolicyGradientAgentOptimized(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    learning_rate=0.01,\n",
    "    value_lr=0.1,\n",
    "    lr_decay=0.99999,\n",
    "    discount_factor=0.99,\n",
    "    entropy_coef=0.1,\n",
    ")\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, info = env.reset(seed=episode)\n",
    "    total_reward = 0\n",
    "    episode_history = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_history.append((state, action, reward))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.update(episode_history)\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        print(f\"trained {episode+1}/{n_episodes} | avg recent reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "# Save model\n",
    "model_filename = 'policy_gradient_shaped.npy'\n",
    "np.save(model_filename, agent.theta)\n",
    "print(f\"saved model to {model_filename}\")\n",
    "\n",
    "# Quick evaluation using the same wrapper\n",
    "n_test = 200\n",
    "rewards = []\n",
    "steps_list = []\n",
    "successes = 0\n",
    "\n",
    "for episode in range(n_test):\n",
    "    state, info = env.reset(seed=1000 + episode)\n",
    "    episode_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    for step in range(200):\n",
    "        action = np.argmax(agent.theta[state])\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "        if terminated or truncated:\n",
    "            if reward == env.env.custom_reward_delivery:\n",
    "                successes += 1\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    steps_list.append(step_count)\n",
    "\n",
    "avg_reward = np.mean(rewards)\n",
    "avg_steps = np.mean(steps_list)\n",
    "success_rate = successes / n_test\n",
    "normalized_steps = avg_steps / 200\n",
    "step_score = 1 - normalized_steps\n",
    "evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
    "\n",
    "print(\"\\nshaped test result:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   avg reward: {avg_reward:.2f}\")\n",
    "print(f\"   avg steps: {avg_steps:.2f}\")\n",
    "print(f\"   success rate: {success_rate:.1%}\")\n",
    "print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact, masked tabular Q-learning baseline (fast)\n",
    "# Defines masked helpers and Q-learning train/test functions.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def masked_random_choice(mask):\n",
    "    \"\"\"Return a random action index among allowed actions in mask (0/1 mask).\"\"\"\n",
    "    mask = np.asarray(mask, dtype=np.int8)\n",
    "    allowed = np.nonzero(mask)[0]\n",
    "    if allowed.size == 0:\n",
    "        return int(np.random.randint(0, len(mask)))\n",
    "    return int(np.random.choice(allowed))\n",
    "\n",
    "\n",
    "def masked_argmax(Q_row, mask):\n",
    "    \"\"\"Masked argmax: choose argmax among allowed actions; break ties by smallest index.\"\"\"\n",
    "    mask = np.asarray(mask, dtype=np.int8)\n",
    "    if mask.sum() == 0:\n",
    "        return int(np.argmax(Q_row))\n",
    "    scores = np.where(mask, Q_row, -np.inf)\n",
    "    return int(np.argmax(scores))\n",
    "\n",
    "\n",
    "def train_q_learning(env, n_episodes=2000, alpha=0.1, gamma=0.99, epsilon=0.1, max_steps=200, seed_start=0, verbose=True):\n",
    "    \"\"\"Train a tabular Q-learning agent using action masks from the env.\n",
    "    Returns Q-table (n_states x n_actions) and training stats.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
    "\n",
    "    episode_rewards = []\n",
    "    steps_per_episode = []\n",
    "    successes = 0\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset(seed=seed_start + ep)\n",
    "        total_reward = 0.0\n",
    "        for t in range(max_steps):\n",
    "            mask = info.get('action_mask', np.ones(n_actions, dtype=int))\n",
    "            if np.random.random() < epsilon:\n",
    "                action = masked_random_choice(mask)\n",
    "            else:\n",
    "                action = masked_argmax(Q[state], mask)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_mask = info.get('action_mask', np.ones(n_actions, dtype=int))\n",
    "            if next_mask.sum() == 0:\n",
    "                next_max = np.max(Q[next_state])\n",
    "            else:\n",
    "                next_max = np.max(np.where(next_mask, Q[next_state], -np.inf))\n",
    "                if np.isneginf(next_max):\n",
    "                    next_max = np.max(Q[next_state])\n",
    "\n",
    "            td_target = reward + gamma * next_max * (0 if done else 1)\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if terminated and reward == env.reward_delivery:\n",
    "                    successes += 1\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        steps_per_episode.append(t + 1)\n",
    "\n",
    "        if verbose and (ep + 1) % 500 == 0:\n",
    "            print(f\"Episode {ep+1}/{n_episodes} | avg reward (last 100): {np.mean(episode_rewards[-100:]):.2f} | avg steps (last 100): {np.mean(steps_per_episode[-100:]):.1f}\")\n",
    "\n",
    "    stats = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'steps_per_episode': steps_per_episode,\n",
    "        'successes': successes,\n",
    "    }\n",
    "    return Q, stats\n",
    "\n",
    "\n",
    "def test_q_learning(Q, env, n_episodes=200, seed_start=0, max_steps=200, verbose=True):\n",
    "    \"\"\"Evaluate a Q-table deterministically using masked argmax. Returns metrics dict.\"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    successes = 0\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset(seed=seed_start + ep)\n",
    "        ep_reward = 0.0\n",
    "        for t in range(max_steps):\n",
    "            mask = info.get('action_mask', np.ones(n_actions, dtype=int))\n",
    "            action = masked_argmax(Q[state], mask)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if terminated or truncated:\n",
    "                if terminated and reward == env.reward_delivery:\n",
    "                    successes += 1\n",
    "                break\n",
    "            state = next_state\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(t + 1)\n",
    "\n",
    "    avg_reward = np.mean(rewards)\n",
    "    avg_steps = np.mean(steps)\n",
    "    success_rate = successes / n_episodes\n",
    "    normalized_steps = avg_steps / max_steps\n",
    "    step_score = 1 - normalized_steps\n",
    "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
    "\n",
    "    metrics = {\n",
    "        'avg_reward': avg_reward,\n",
    "        'avg_steps': avg_steps,\n",
    "        'success_rate': success_rate,\n",
    "        'evaluation_score': evaluation_score,\n",
    "    }\n",
    "    if verbose:\n",
    "        print(\"\\nQ-learning test result:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  avg reward: {avg_reward:.2f}\")\n",
    "        print(f\"  avg steps: {avg_steps:.2f}\")\n",
    "        print(f\"  success rate: {success_rate:.1%}\")\n",
    "        print(f\"  evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
    "        print(\"=\"*60)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-learning...\n",
      "Episode 500/50000 | avg reward (last 100): -51.13 | avg steps (last 100): 71.3\n",
      "Episode 1000/50000 | avg reward (last 100): -22.86 | avg steps (last 100): 43.6\n",
      "Episode 1500/50000 | avg reward (last 100): -4.17 | avg steps (last 100): 25.2\n",
      "Episode 2000/50000 | avg reward (last 100): -2.21 | avg steps (last 100): 23.2\n",
      "Episode 2500/50000 | avg reward (last 100): -1.82 | avg steps (last 100): 22.8\n",
      "Episode 3000/50000 | avg reward (last 100): 0.42 | avg steps (last 100): 20.6\n",
      "Episode 3500/50000 | avg reward (last 100): 1.25 | avg steps (last 100): 19.8\n",
      "Episode 4000/50000 | avg reward (last 100): 0.26 | avg steps (last 100): 20.7\n",
      "Episode 4500/50000 | avg reward (last 100): 0.27 | avg steps (last 100): 20.7\n",
      "Episode 5000/50000 | avg reward (last 100): -0.32 | avg steps (last 100): 21.3\n",
      "Episode 5500/50000 | avg reward (last 100): 0.83 | avg steps (last 100): 20.2\n",
      "Episode 6000/50000 | avg reward (last 100): 0.69 | avg steps (last 100): 20.3\n",
      "Episode 6500/50000 | avg reward (last 100): 1.62 | avg steps (last 100): 19.4\n",
      "Episode 7000/50000 | avg reward (last 100): 0.61 | avg steps (last 100): 20.4\n",
      "Episode 7500/50000 | avg reward (last 100): 1.75 | avg steps (last 100): 19.2\n",
      "Episode 8000/50000 | avg reward (last 100): 0.16 | avg steps (last 100): 20.8\n",
      "Episode 8500/50000 | avg reward (last 100): 1.21 | avg steps (last 100): 19.8\n",
      "Episode 9000/50000 | avg reward (last 100): 0.55 | avg steps (last 100): 20.4\n",
      "Episode 9500/50000 | avg reward (last 100): 1.16 | avg steps (last 100): 19.8\n",
      "Episode 10000/50000 | avg reward (last 100): 1.21 | avg steps (last 100): 19.8\n",
      "Episode 10500/50000 | avg reward (last 100): 0.09 | avg steps (last 100): 20.9\n",
      "Episode 11000/50000 | avg reward (last 100): 1.07 | avg steps (last 100): 19.9\n",
      "Episode 11500/50000 | avg reward (last 100): 0.31 | avg steps (last 100): 20.7\n",
      "Episode 12000/50000 | avg reward (last 100): -0.27 | avg steps (last 100): 21.3\n",
      "Episode 12500/50000 | avg reward (last 100): 0.41 | avg steps (last 100): 20.6\n",
      "Episode 13000/50000 | avg reward (last 100): 1.07 | avg steps (last 100): 19.9\n",
      "Episode 13500/50000 | avg reward (last 100): 0.66 | avg steps (last 100): 20.3\n",
      "Episode 14000/50000 | avg reward (last 100): 0.75 | avg steps (last 100): 20.2\n",
      "Episode 14500/50000 | avg reward (last 100): 0.97 | avg steps (last 100): 20.0\n",
      "Episode 15000/50000 | avg reward (last 100): 0.79 | avg steps (last 100): 20.2\n",
      "Episode 15500/50000 | avg reward (last 100): 1.39 | avg steps (last 100): 19.6\n",
      "Episode 16000/50000 | avg reward (last 100): 0.36 | avg steps (last 100): 20.6\n",
      "Episode 16500/50000 | avg reward (last 100): 1.73 | avg steps (last 100): 19.3\n",
      "Episode 17000/50000 | avg reward (last 100): 0.13 | avg steps (last 100): 20.9\n",
      "Episode 17500/50000 | avg reward (last 100): 0.06 | avg steps (last 100): 20.9\n",
      "Episode 18000/50000 | avg reward (last 100): 0.67 | avg steps (last 100): 20.3\n",
      "Episode 18500/50000 | avg reward (last 100): 1.60 | avg steps (last 100): 19.4\n",
      "Episode 19000/50000 | avg reward (last 100): 0.03 | avg steps (last 100): 21.0\n",
      "Episode 19500/50000 | avg reward (last 100): 0.99 | avg steps (last 100): 20.0\n",
      "Episode 20000/50000 | avg reward (last 100): 1.26 | avg steps (last 100): 19.7\n",
      "Episode 20500/50000 | avg reward (last 100): -0.67 | avg steps (last 100): 21.7\n",
      "Episode 21000/50000 | avg reward (last 100): 0.76 | avg steps (last 100): 20.2\n",
      "Episode 21500/50000 | avg reward (last 100): 1.89 | avg steps (last 100): 19.1\n",
      "Episode 22000/50000 | avg reward (last 100): 0.76 | avg steps (last 100): 20.2\n",
      "Episode 22500/50000 | avg reward (last 100): 0.73 | avg steps (last 100): 20.3\n",
      "Episode 23000/50000 | avg reward (last 100): 0.69 | avg steps (last 100): 20.3\n",
      "Episode 23500/50000 | avg reward (last 100): 0.36 | avg steps (last 100): 20.6\n",
      "Episode 24000/50000 | avg reward (last 100): 1.57 | avg steps (last 100): 19.4\n",
      "Episode 24500/50000 | avg reward (last 100): 0.76 | avg steps (last 100): 20.2\n",
      "Episode 25000/50000 | avg reward (last 100): 1.03 | avg steps (last 100): 20.0\n",
      "Episode 25500/50000 | avg reward (last 100): 1.26 | avg steps (last 100): 19.7\n",
      "Episode 26000/50000 | avg reward (last 100): 1.89 | avg steps (last 100): 19.1\n",
      "Episode 26500/50000 | avg reward (last 100): 1.45 | avg steps (last 100): 19.6\n",
      "Episode 27000/50000 | avg reward (last 100): 1.62 | avg steps (last 100): 19.4\n",
      "Episode 27500/50000 | avg reward (last 100): 1.28 | avg steps (last 100): 19.7\n",
      "Episode 28000/50000 | avg reward (last 100): 1.40 | avg steps (last 100): 19.6\n",
      "Episode 28500/50000 | avg reward (last 100): -0.24 | avg steps (last 100): 21.2\n",
      "Episode 29000/50000 | avg reward (last 100): 0.96 | avg steps (last 100): 20.0\n",
      "Episode 29500/50000 | avg reward (last 100): 0.81 | avg steps (last 100): 20.2\n",
      "Episode 30000/50000 | avg reward (last 100): -0.59 | avg steps (last 100): 21.6\n",
      "Episode 30500/50000 | avg reward (last 100): 0.44 | avg steps (last 100): 20.6\n",
      "Episode 31000/50000 | avg reward (last 100): 0.87 | avg steps (last 100): 20.1\n",
      "Episode 31500/50000 | avg reward (last 100): 0.89 | avg steps (last 100): 20.1\n",
      "Episode 32000/50000 | avg reward (last 100): 1.58 | avg steps (last 100): 19.4\n",
      "Episode 32500/50000 | avg reward (last 100): 0.34 | avg steps (last 100): 20.7\n",
      "Episode 33000/50000 | avg reward (last 100): 1.29 | avg steps (last 100): 19.7\n",
      "Episode 33500/50000 | avg reward (last 100): 0.94 | avg steps (last 100): 20.1\n",
      "Episode 34000/50000 | avg reward (last 100): 0.40 | avg steps (last 100): 20.6\n",
      "Episode 34500/50000 | avg reward (last 100): -0.10 | avg steps (last 100): 21.1\n",
      "Episode 35000/50000 | avg reward (last 100): 1.53 | avg steps (last 100): 19.5\n",
      "Episode 35500/50000 | avg reward (last 100): 1.52 | avg steps (last 100): 19.5\n",
      "Episode 36000/50000 | avg reward (last 100): 0.33 | avg steps (last 100): 20.7\n",
      "Episode 36500/50000 | avg reward (last 100): 2.29 | avg steps (last 100): 18.7\n",
      "Episode 37000/50000 | avg reward (last 100): 1.00 | avg steps (last 100): 20.0\n",
      "Episode 37500/50000 | avg reward (last 100): 0.64 | avg steps (last 100): 20.4\n",
      "Episode 38000/50000 | avg reward (last 100): 0.47 | avg steps (last 100): 20.5\n",
      "Episode 38500/50000 | avg reward (last 100): 0.78 | avg steps (last 100): 20.2\n",
      "Episode 39000/50000 | avg reward (last 100): 0.97 | avg steps (last 100): 20.0\n",
      "Episode 39500/50000 | avg reward (last 100): 0.04 | avg steps (last 100): 21.0\n",
      "Episode 40000/50000 | avg reward (last 100): 0.42 | avg steps (last 100): 20.6\n",
      "Episode 40500/50000 | avg reward (last 100): 2.44 | avg steps (last 100): 18.6\n",
      "Episode 41000/50000 | avg reward (last 100): -0.25 | avg steps (last 100): 21.2\n",
      "Episode 41500/50000 | avg reward (last 100): 1.10 | avg steps (last 100): 19.9\n",
      "Episode 42000/50000 | avg reward (last 100): 1.26 | avg steps (last 100): 19.7\n",
      "Episode 42500/50000 | avg reward (last 100): 0.88 | avg steps (last 100): 20.1\n",
      "Episode 43000/50000 | avg reward (last 100): 0.90 | avg steps (last 100): 20.1\n",
      "Episode 43500/50000 | avg reward (last 100): 1.30 | avg steps (last 100): 19.7\n",
      "Episode 44000/50000 | avg reward (last 100): 0.42 | avg steps (last 100): 20.6\n",
      "Episode 44500/50000 | avg reward (last 100): 1.39 | avg steps (last 100): 19.6\n",
      "Episode 45000/50000 | avg reward (last 100): -1.10 | avg steps (last 100): 22.1\n",
      "Episode 45500/50000 | avg reward (last 100): 0.45 | avg steps (last 100): 20.6\n",
      "Episode 46000/50000 | avg reward (last 100): 0.65 | avg steps (last 100): 20.4\n",
      "Episode 46500/50000 | avg reward (last 100): 1.73 | avg steps (last 100): 19.3\n",
      "Episode 47000/50000 | avg reward (last 100): 1.43 | avg steps (last 100): 19.6\n",
      "Episode 47500/50000 | avg reward (last 100): 1.75 | avg steps (last 100): 19.2\n",
      "Episode 48000/50000 | avg reward (last 100): 1.33 | avg steps (last 100): 19.7\n",
      "Episode 48500/50000 | avg reward (last 100): 1.19 | avg steps (last 100): 19.8\n",
      "Episode 49000/50000 | avg reward (last 100): 1.26 | avg steps (last 100): 19.7\n",
      "Episode 49500/50000 | avg reward (last 100): 0.51 | avg steps (last 100): 20.5\n",
      "Episode 50000/50000 | avg reward (last 100): 0.49 | avg steps (last 100): 20.5\n",
      "Saved Q-table to q_learning_baseline.npy\n",
      "Evaluating learned Q-table...\n",
      "\n",
      "Q-learning test result:\n",
      "============================================================\n",
      "  avg reward: 0.63\n",
      "  avg steps: 20.33\n",
      "  success rate: 99.8%\n",
      "  evaluation score: 0.9183 (91.83%)\n",
      "============================================================\n",
      "\n",
      "Done. Metrics:\n",
      "{'avg_reward': np.float64(0.63), 'avg_steps': np.float64(20.328), 'success_rate': 0.998, 'evaluation_score': np.float64(0.9182880000000001)}\n"
     ]
    }
   ],
   "source": [
    "# Run a short Q-learning baseline using the Taxi env and masked actions\n",
    "# Configure these hyperparameters as needed\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "n_train = 50000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "max_steps = 200\n",
    "\n",
    "# Use the project's reward wrapper and environment\n",
    "env = TaxiRewardWrapper(TaxiEnv(), reward_step=-1, reward_delivery=20, reward_illegal=-10)\n",
    "\n",
    "print('Training Q-learning...')\n",
    "Q, stats = train_q_learning(env, n_episodes=n_train, alpha=alpha, gamma=gamma, epsilon=epsilon, max_steps=max_steps, verbose=True)\n",
    "\n",
    "model_path = Path('./q_learning_baseline.npy')\n",
    "np.save(model_path, Q)\n",
    "print(f'Saved Q-table to {model_path}')\n",
    "\n",
    "print('Evaluating learned Q-table...')\n",
    "metrics = test_q_learning(Q, env, n_episodes=500, seed_start=10000, max_steps=max_steps, verbose=True)\n",
    "\n",
    "print('\\nDone. Metrics:')\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating learned Q-table...\n",
      "\n",
      "Q-learning test result:\n",
      "============================================================\n",
      "  avg reward: 1.47\n",
      "  avg steps: 19.53\n",
      "  success rate: 100.0%\n",
      "  evaluation score: 0.9219 (92.19%)\n",
      "============================================================\n",
      "\n",
      "Done. Metrics:\n",
      "{'avg_reward': np.float64(1.4743333333333333), 'avg_steps': np.float64(19.525666666666666), 'success_rate': 1.0, 'evaluation_score': np.float64(0.9218973333333333)}\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating learned Q-table...')\n",
    "metrics = test_q_learning(Q, env, n_episodes=3000, seed_start=49, max_steps=max_steps, verbose=True)\n",
    "\n",
    "print('\\nDone. Metrics:')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TaxiRewardWrapper<TaxiEnv instance>>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-mask experiment: Policy Gradient vs Q-learning\n",
    "\n",
    "This cell runs 4 short experiments (quick smoke tests):\n",
    "\n",
    "- PolicyGradientAgent (unmasked)\n",
    "- PolicyGradientAgent (masked action selection using `info['action_mask']`)\n",
    "- Q-learning (unmasked)\n",
    "- Q-learning (masked  uses the masked helpers already defined)\n",
    "\n",
    "Results (metrics) are collected into a small pandas DataFrame and saved to\n",
    "`actionmask_experiment_summary.csv`. Models (.npy) are saved next to the notebook.\n",
    "\n",
    "Note: these are short runs (n_train small) intended for quick comparisons \n",
    "increase `n_train` for stronger baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments  this will take a short while...\n",
      "PG unmasked done\n",
      "PG masked done\n",
      "Q unmasked done\n",
      "Episode 500/2000 | avg reward (last 100): -61.99 | avg steps (last 100): 80.9\n",
      "Episode 1000/2000 | avg reward (last 100): -24.65 | avg steps (last 100): 45.6\n",
      "Episode 1500/2000 | avg reward (last 100): -6.41 | avg steps (last 100): 27.4\n",
      "Episode 2000/2000 | avg reward (last 100): -3.34 | avg steps (last 100): 24.3\n",
      "Q masked done\n",
      "Experiment finished in 58.6s  summary saved to actionmask_experiment_summary.csv\n",
      "   avg_reward  avg_steps  success_rate  evaluation_score           agent  \\\n",
      "0    -314.570    190.130         0.055           0.05048  PolicyGradient   \n",
      "1     -79.380     92.715         0.635           0.55614  PolicyGradient   \n",
      "2     -83.735     96.545         0.610           0.53582      Q-learning   \n",
      "3     -17.465     36.890         0.925           0.83744      Q-learning   \n",
      "\n",
      "       mode  n_train  \n",
      "0  unmasked     2000  \n",
      "1    masked     2000  \n",
      "2  unmasked     2000  \n",
      "3    masked     2000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Helper: sample action from a policy but respect action mask (0/1 array) if provided\n",
    "def sample_action_from_policy(agent, state, mask=None):\n",
    "    policy = agent.get_policy(state).astype(float)\n",
    "    if mask is None:\n",
    "        return np.random.choice(agent.n_actions, p=policy)\n",
    "    mask = np.asarray(mask, dtype=int)\n",
    "    masked = policy * mask\n",
    "    s = masked.sum()\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        # fallback: use original policy (renormalized) if mask disables all actions\n",
    "        policy = policy / policy.sum()\n",
    "        return np.random.choice(agent.n_actions, p=policy)\n",
    "    masked = masked / s\n",
    "    return np.random.choice(agent.n_actions, p=masked)\n",
    "\n",
    "# Policy Gradient trainer that optionally uses action masks during sampling\n",
    "def train_policy_gradient_custom(n_episodes=2000, max_steps=200, use_mask=False, reward_step=-1, reward_delivery=20, reward_illegal=-10):\n",
    "    env = TaxiRewardWrapper(TaxiEnv(), reward_step=reward_step, reward_delivery=reward_delivery, reward_illegal=reward_illegal)\n",
    "    agent = PolicyGradientAgentOptimized(n_states=env.observation_space.n, n_actions=env.action_space.n, learning_rate=0.01, value_lr=0.1, lr_decay=0.99999, discount_factor=0.99, entropy_coef=0.1)\n",
    "\n",
    "    episode_rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset(seed=ep)\n",
    "        total_reward = 0.0\n",
    "        episode_history = []\n",
    "        for t in range(max_steps):\n",
    "            if use_mask:\n",
    "                mask = info.get('action_mask', np.ones(agent.n_actions, dtype=int))\n",
    "                action = sample_action_from_policy(agent, state, mask)\n",
    "            else:\n",
    "                action = agent.choose_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_history.append((state, action, reward))\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.update(episode_history)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Q-learning trainer (unmasked)  same as train_q_learning but ignores action_mask during selection\n",
    "def train_q_learning_unmasked(env, n_episodes=2000, alpha=0.1, gamma=0.99, epsilon=0.1, max_steps=200, seed_start=0, verbose=True):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
    "    episode_rewards = []\n",
    "    steps_per_episode = []\n",
    "    successes = 0\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset(seed=seed_start + ep)\n",
    "        total_reward = 0.0\n",
    "        for t in range(max_steps):\n",
    "            # ignore mask: epsilon-greedy across all actions\n",
    "            if np.random.random() < epsilon:\n",
    "                action = int(np.random.randint(0, n_actions))\n",
    "            else:\n",
    "                action = int(np.argmax(Q[state]))\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_max = np.max(Q[next_state])\n",
    "            td_target = reward + gamma * next_max * (0 if done else 1)\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                if terminated and reward == env.reward_delivery:\n",
    "                    successes += 1\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        steps_per_episode.append(t + 1)\n",
    "\n",
    "    stats = {'episode_rewards': episode_rewards, 'steps_per_episode': steps_per_episode, 'successes': successes}\n",
    "    return Q, stats\n",
    "\n",
    "# Evaluation helpers\n",
    "def evaluate_policy_agent(agent, env, n_episodes=200, use_mask=False, max_steps=200):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "    successes = 0\n",
    "    for ep in range(n_episodes):\n",
    "        state, info = env.reset(seed=ep)\n",
    "        ep_reward = 0.0\n",
    "        for t in range(max_steps):\n",
    "            if use_mask:\n",
    "                mask = info.get('action_mask', np.ones(agent.n_actions, dtype=int))\n",
    "                action = masked_argmax(agent.theta[state], mask)\n",
    "            else:\n",
    "                action = int(np.argmax(agent.theta[state]))\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if terminated or truncated:\n",
    "                if terminated and reward == env.reward_delivery:\n",
    "                    successes += 1\n",
    "                break\n",
    "            state = next_state\n",
    "        rewards.append(ep_reward)\n",
    "        steps.append(t+1)\n",
    "    avg_reward = np.mean(rewards)\n",
    "    avg_steps = np.mean(steps)\n",
    "    success_rate = successes / n_episodes\n",
    "    normalized_steps = avg_steps / max_steps\n",
    "    step_score = 1 - normalized_steps\n",
    "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
    "    return {'avg_reward': avg_reward, 'avg_steps': avg_steps, 'success_rate': success_rate, 'evaluation_score': evaluation_score}\n",
    "\n",
    "# Run the 4 short experiments and collect results\n",
    "results = []\n",
    "n_pg = 2000\n",
    "n_q = 2000\n",
    "print('Starting experiments  this will take a short while...')\n",
    "start_all = time.time()\n",
    "# 1) PolicyGradient unmasked\n",
    "agent_pg_unmasked, rewards_pg_unmasked = train_policy_gradient_custom(n_episodes=n_pg, use_mask=False)\n",
    "np.save('pg_unmasked.npy', agent_pg_unmasked.theta)\n",
    "env_eval = TaxiRewardWrapper(TaxiEnv(), reward_step=-1, reward_delivery=20, reward_illegal=-10)\n",
    "metrics = evaluate_policy_agent(agent_pg_unmasked, env_eval, n_episodes=200, use_mask=False)\n",
    "metrics.update({'agent':'PolicyGradient','mode':'unmasked','n_train':n_pg})\n",
    "results.append(metrics)\n",
    "print('PG unmasked done')\n",
    "# 2) PolicyGradient masked\n",
    "agent_pg_masked, rewards_pg_masked = train_policy_gradient_custom(n_episodes=n_pg, use_mask=True)\n",
    "np.save('pg_masked.npy', agent_pg_masked.theta)\n",
    "env_eval = TaxiRewardWrapper(TaxiEnv(), reward_step=-1, reward_delivery=20, reward_illegal=-10)\n",
    "metrics = evaluate_policy_agent(agent_pg_masked, env_eval, n_episodes=200, use_mask=True)\n",
    "metrics.update({'agent':'PolicyGradient','mode':'masked','n_train':n_pg})\n",
    "results.append(metrics)\n",
    "print('PG masked done')\n",
    "# 3) Q-learning unmasked\n",
    "env_q = TaxiRewardWrapper(TaxiEnv(), reward_step=-1, reward_delivery=20, reward_illegal=-10)\n",
    "Q_unmasked, stats_unmasked = train_q_learning_unmasked(env_q, n_episodes=n_q, alpha=0.1, gamma=0.99, epsilon=0.1, max_steps=200, verbose=True)\n",
    "np.save('q_unmasked.npy', Q_unmasked)\n",
    "metrics = test_q_learning(Q_unmasked, env_q, n_episodes=200, seed_start=10000, max_steps=200, verbose=False)\n",
    "metrics.update({'agent':'Q-learning','mode':'unmasked','n_train':n_q})\n",
    "results.append(metrics)\n",
    "print('Q unmasked done')\n",
    "# 4) Q-learning masked (uses existing train_q_learning which is masked-aware)\n",
    "env_q2 = TaxiRewardWrapper(TaxiEnv(), reward_step=-1, reward_delivery=20, reward_illegal=-10)\n",
    "Q_masked, stats_masked = train_q_learning(env_q2, n_episodes=n_q, alpha=0.1, gamma=0.99, epsilon=0.1, max_steps=200, verbose=True)\n",
    "np.save('q_masked.npy', Q_masked)\n",
    "metrics = test_q_learning(Q_masked, env_q2, n_episodes=200, seed_start=10000, max_steps=200, verbose=False)\n",
    "metrics.update({'agent':'Q-learning','mode':'masked','n_train':n_q})\n",
    "results.append(metrics)\n",
    "print('Q masked done')\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "csv_out = 'actionmask_experiment_summary.csv'\n",
    "df.to_csv(csv_out, index=False)\n",
    "print(f'Experiment finished in {time.time()-start_all:.1f}s  summary saved to {csv_out}')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
