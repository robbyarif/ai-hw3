{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robbyarif/ai-hw3/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sbro3A_Qha1"
      },
      "source": [
        "# Taxi Env (Don't Modify it )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1HlV_-2QUNN"
      },
      "outputs": [],
      "source": [
        "from contextlib import closing\n",
        "from io import StringIO\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import Env, spaces, utils\n",
        "from gymnasium.envs.toy_text.utils import categorical_sample\n",
        "from gymnasium.error import DependencyNotInstalled\n",
        "\n",
        "MAP_0 = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP_1 = [\n",
        "    \"+---------+\",\n",
        "    \"|R| : : :G|\",\n",
        "    \"| | : | : |\",\n",
        "    \"| : : | : |\",\n",
        "    \"| : | : : |\",\n",
        "    \"|Y: | :B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP_2 = [\n",
        "    \"+---------+\",\n",
        "    \"|R: : : |G|\",\n",
        "    \"| : : : | |\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : | | : |\",\n",
        "    \"|Y: : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP = MAP_0\n",
        "MAPS = [MAP_0, MAP_1, MAP_2]\n",
        "WINDOW_SIZE = (550, 350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBkD72IKQxKT"
      },
      "outputs": [],
      "source": [
        "class TaxiEnv(Env):\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
        "        \"render_fps\": 4,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: str | None = None,\n",
        "        is_rainy: bool = True,\n",
        "        fickle_passenger: bool = False,\n",
        "        use_multiple_maps: bool = True,\n",
        "        reward_step: float = -1,\n",
        "        reward_delivery: float = 20,\n",
        "        reward_illegal: float = -10,\n",
        "    ):\n",
        "        self.use_multiple_maps = use_multiple_maps\n",
        "\n",
        "        if use_multiple_maps:\n",
        "            self.desc_maps = [np.asarray(m, dtype=\"c\") for m in MAPS]\n",
        "            self.P_maps = []\n",
        "            self.current_map_id = 0\n",
        "            self.desc = self.desc_maps[0]\n",
        "        else:\n",
        "            self.desc = np.asarray(MAP, dtype=\"c\")\n",
        "\n",
        "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
        "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255)]\n",
        "\n",
        "        self.reward_step = reward_step\n",
        "        self.reward_delivery = reward_delivery\n",
        "        self.reward_illegal = reward_illegal\n",
        "\n",
        "        num_states = 500\n",
        "        num_rows = 5\n",
        "        num_columns = 5\n",
        "        self.max_row = num_rows - 1\n",
        "        self.max_col = num_columns - 1\n",
        "        self.initial_state_distrib = np.zeros(num_states)\n",
        "        num_actions = 6\n",
        "\n",
        "\n",
        "        if use_multiple_maps:\n",
        "\n",
        "            for map_idx in range(3):\n",
        "                self.desc = self.desc_maps[map_idx]\n",
        "                P = {\n",
        "                    state: {action: [] for action in range(num_actions)}\n",
        "                    for state in range(num_states)\n",
        "                }\n",
        "                self.P = P\n",
        "\n",
        "                # Iterate through all possible state combinations\n",
        "                for row in range(num_rows):\n",
        "                    for col in range(num_columns):\n",
        "                        for pass_idx in range(len(locs) + 1):\n",
        "                            for dest_idx in range(len(locs)):\n",
        "                                state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                                if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                                    self.initial_state_distrib[state] += 1\n",
        "                                for action in range(num_actions):\n",
        "                                    if is_rainy:\n",
        "                                        self._build_rainy_transitions(\n",
        "                                            row, col, pass_idx, dest_idx, action,\n",
        "                                        )\n",
        "                                    else:\n",
        "                                        self._build_dry_transitions(\n",
        "                                            row, col, pass_idx, dest_idx, action,\n",
        "                                        )\n",
        "\n",
        "                self.P_maps.append(P)\n",
        "\n",
        "\n",
        "            self.desc = self.desc_maps[0]\n",
        "            self.P = self.P_maps[0]\n",
        "        else:\n",
        "\n",
        "            self.P = {\n",
        "                state: {action: [] for action in range(num_actions)}\n",
        "                for state in range(num_states)\n",
        "            }\n",
        "\n",
        "            for row in range(num_rows):\n",
        "                for col in range(num_columns):\n",
        "                    for pass_idx in range(len(locs) + 1):\n",
        "                        for dest_idx in range(len(locs)):\n",
        "                            state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                            if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                                self.initial_state_distrib[state] += 1\n",
        "                            for action in range(num_actions):\n",
        "                                if is_rainy:\n",
        "                                    self._build_rainy_transitions(\n",
        "                                        row, col, pass_idx, dest_idx, action,\n",
        "                                    )\n",
        "                                else:\n",
        "                                    self._build_dry_transitions(\n",
        "                                        row, col, pass_idx, dest_idx, action,\n",
        "                                    )\n",
        "\n",
        "        self.initial_state_distrib /= self.initial_state_distrib.sum()\n",
        "        self.action_space = spaces.Discrete(num_actions)\n",
        "        self.observation_space = spaces.Discrete(num_states)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.fickle_passenger = fickle_passenger\n",
        "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.5\n",
        "\n",
        "        # pygame utils\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.cell_size = (\n",
        "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
        "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
        "        )\n",
        "        self.taxi_imgs = None\n",
        "        self.taxi_orientation = 0\n",
        "        self.passenger_img = None\n",
        "        self.destination_img = None\n",
        "        self.median_horiz = None\n",
        "        self.median_vert = None\n",
        "        self.background_img = None\n",
        "\n",
        "    def _pickup(self, taxi_loc, pass_idx, reward):\n",
        "        \"\"\"Computes the new location and reward for pickup action.\"\"\"\n",
        "        if pass_idx < 4 and taxi_loc == self.locs[pass_idx]:\n",
        "            new_pass_idx = 4\n",
        "            new_reward = reward\n",
        "        else:  # passenger not at location\n",
        "            new_pass_idx = pass_idx\n",
        "            new_reward = self.reward_illegal\n",
        "\n",
        "        return new_pass_idx, new_reward\n",
        "\n",
        "    def _dropoff(self, taxi_loc, pass_idx, dest_idx, default_reward):\n",
        "        \"\"\"Computes the new location and reward for return dropoff action.\"\"\"\n",
        "        if (taxi_loc == self.locs[dest_idx]) and pass_idx == 4:\n",
        "            new_pass_idx = dest_idx\n",
        "            new_terminated = True\n",
        "            new_reward = self.reward_delivery\n",
        "        elif (taxi_loc in self.locs) and pass_idx == 4:\n",
        "            new_pass_idx = self.locs.index(taxi_loc)\n",
        "            new_terminated = False\n",
        "            new_reward = default_reward\n",
        "        else:  # dropoff at wrong location\n",
        "            new_pass_idx = pass_idx\n",
        "            new_terminated = False\n",
        "            new_reward = self.reward_illegal\n",
        "\n",
        "        return new_pass_idx, new_reward, new_terminated\n",
        "\n",
        "    def _calc_new_position(self, row, col, movement, offset=0):\n",
        "        \"\"\"Calculates the new position for a row and col to the movement.\"\"\"\n",
        "        dr, dc = movement\n",
        "        new_row = max(0, min(row + dr, self.max_row))\n",
        "        new_col = max(0, min(col + dc, self.max_col))\n",
        "        if self.desc[1 + new_row, 2 * new_col + offset] == b\":\":\n",
        "            return new_row, new_col\n",
        "        else:  # Default to current position if not traversable\n",
        "            return row, col\n",
        "\n",
        "    def _build_rainy_transitions(self, row, col, pass_idx, dest_idx, action):\n",
        "        \"\"\"Computes the next action for a state (row, col, pass_idx, dest_idx) and action for `is_rainy`.\"\"\"\n",
        "        state = self.encode(row, col, pass_idx, dest_idx)\n",
        "\n",
        "        taxi_loc = left_pos = right_pos = (row, col)\n",
        "        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
        "        reward = self.reward_step\n",
        "        terminated = False\n",
        "\n",
        "        moves = {\n",
        "            0: ((1, 0), (0, -1), (0, 1)),  # Down\n",
        "            1: ((-1, 0), (0, -1), (0, 1)),  # Up\n",
        "            2: ((0, 1), (1, 0), (-1, 0)),  # Right\n",
        "            3: ((0, -1), (1, 0), (-1, 0)),  # Left\n",
        "        }\n",
        "\n",
        "        # Check if movement is allowed\n",
        "        if (\n",
        "            action in {0, 1}\n",
        "            or (action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\")\n",
        "            or (action == 3 and self.desc[1 + row, 2 * col] == b\":\")\n",
        "        ):\n",
        "            dr, dc = moves[action][0]\n",
        "            new_row = max(0, min(row + dr, self.max_row))\n",
        "            new_col = max(0, min(col + dc, self.max_col))\n",
        "\n",
        "            left_pos = self._calc_new_position(row, col, moves[action][1], offset=2)\n",
        "            right_pos = self._calc_new_position(row, col, moves[action][2])\n",
        "        elif action == 4:  # pickup\n",
        "            new_pass_idx, reward = self._pickup(taxi_loc, new_pass_idx, reward)\n",
        "        elif action == 5:  # dropoff\n",
        "            new_pass_idx, reward, terminated = self._dropoff(\n",
        "                taxi_loc, new_pass_idx, dest_idx, reward\n",
        "            )\n",
        "        intended_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
        "\n",
        "        if action <= 3:\n",
        "            left_state = self.encode(left_pos[0], left_pos[1], new_pass_idx, dest_idx)\n",
        "            right_state = self.encode(\n",
        "                right_pos[0], right_pos[1], new_pass_idx, dest_idx\n",
        "            )\n",
        "\n",
        "            self.P[state][action].append((0.8, intended_state, self.reward_step, terminated))\n",
        "            self.P[state][action].append((0.1, left_state, self.reward_step, terminated))\n",
        "            self.P[state][action].append((0.1, right_state, self.reward_step, terminated))\n",
        "        else:\n",
        "            self.P[state][action].append((1.0, intended_state, reward, terminated))\n",
        "\n",
        "\n",
        "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
        "        # (5) 5, 5, 4\n",
        "        i = taxi_row\n",
        "        i *= 5\n",
        "        i += taxi_col\n",
        "        i *= 5\n",
        "        i += pass_loc\n",
        "        i *= 4\n",
        "        i += dest_idx\n",
        "        return i\n",
        "\n",
        "    def decode(self, i):\n",
        "        out = []\n",
        "        out.append(i % 4)\n",
        "        i = i // 4\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i)\n",
        "        assert 0 <= i < 5\n",
        "        return reversed(out)\n",
        "\n",
        "    def action_mask(self, state: int):\n",
        "        \"\"\"Computes an action mask for the action space using the state information.\"\"\"\n",
        "        mask = np.zeros(6, dtype=np.int8)\n",
        "        taxi_row, taxi_col, pass_loc, dest_idx = self.decode(state)\n",
        "        if taxi_row < 4:\n",
        "            mask[0] = 1\n",
        "        if taxi_row > 0:\n",
        "            mask[1] = 1\n",
        "        if taxi_col < 4 and self.desc[taxi_row + 1, 2 * taxi_col + 2] == b\":\":\n",
        "            mask[2] = 1\n",
        "        if taxi_col > 0 and self.desc[taxi_row + 1, 2 * taxi_col] == b\":\":\n",
        "            mask[3] = 1\n",
        "        if pass_loc < 4 and (taxi_row, taxi_col) == self.locs[pass_loc]:\n",
        "            mask[4] = 1\n",
        "        if pass_loc == 4 and (\n",
        "            (taxi_row, taxi_col) == self.locs[dest_idx]\n",
        "            or (taxi_row, taxi_col) in self.locs\n",
        "        ):\n",
        "            mask[5] = 1\n",
        "        return mask\n",
        "\n",
        "    def step(self, a):\n",
        "        transitions = self.P[self.s][a]\n",
        "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
        "        p, s, r, t = transitions[i]\n",
        "        self.lastaction = a\n",
        "\n",
        "        shadow_row, shadow_col, shadow_pass_loc, shadow_dest_idx = self.decode(self.s)\n",
        "        taxi_row, taxi_col, pass_loc, _ = self.decode(s)\n",
        "\n",
        "        # If we are in the fickle step, the passenger has been in the vehicle for at least a step and this step the\n",
        "        # position changed\n",
        "        if (\n",
        "            self.fickle_passenger\n",
        "            and self.fickle_step\n",
        "            and shadow_pass_loc == 4\n",
        "            and (taxi_row != shadow_row or taxi_col != shadow_col)\n",
        "        ):\n",
        "            self.fickle_step = False\n",
        "            possible_destinations = [\n",
        "                i for i in range(len(self.locs)) if i != shadow_dest_idx\n",
        "            ]\n",
        "            dest_idx = self.np_random.choice(possible_destinations)\n",
        "            s = self.encode(taxi_row, taxi_col, pass_loc, dest_idx)\n",
        "\n",
        "        self.s = s\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
        "        return int(s), r, t, False, {\"prob\": p, \"action_mask\": self.action_mask(s)}\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: int | None = None,\n",
        "        options: dict | None = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "\n",
        "        if self.use_multiple_maps and seed is not None:\n",
        "            map_id = seed % 3\n",
        "            self.current_map_id = map_id\n",
        "            self.desc = self.desc_maps[map_id]\n",
        "            self.P = self.P_maps[map_id]\n",
        "\n",
        "        self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
        "        self.lastaction = None\n",
        "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.3\n",
        "        self.taxi_orientation = 0\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return int(self.s), {\"prob\": 1.0, \"action_mask\": self.action_mask(self.s)}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            assert self.spec is not None\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "        elif self.render_mode == \"ansi\":\n",
        "            return self._render_text()\n",
        "        else:  # self.render_mode in {\"human\", \"rgb_array\"}:\n",
        "            return self._render_gui(self.render_mode)\n",
        "\n",
        "    def _render_gui(self, mode):\n",
        "        try:\n",
        "            import pygame  # dependency to pygame only if rendering with human\n",
        "        except ImportError as e:\n",
        "            raise DependencyNotInstalled(\n",
        "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
        "            ) from e\n",
        "\n",
        "        if self.window is None:\n",
        "            pygame.init()\n",
        "            pygame.display.set_caption(\"Taxi\")\n",
        "            if mode == \"human\":\n",
        "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
        "            elif mode == \"rgb_array\":\n",
        "                self.window = pygame.Surface(WINDOW_SIZE)\n",
        "\n",
        "        assert (\n",
        "            self.window is not None\n",
        "        ), \"Something went wrong with pygame. This should never happen.\"\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "        if self.taxi_imgs is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/cab_front.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_rear.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_right.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_left.png\"),\n",
        "            ]\n",
        "            self.taxi_imgs = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.passenger_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/passenger.png\")\n",
        "            self.passenger_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "        if self.destination_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/hotel.png\")\n",
        "            self.destination_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "            self.destination_img.set_alpha(170)\n",
        "        if self.median_horiz is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_left.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_horiz.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_right.png\"),\n",
        "            ]\n",
        "            self.median_horiz = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.median_vert is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_top.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_vert.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_bottom.png\"),\n",
        "            ]\n",
        "            self.median_vert = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.background_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/taxi_background.png\")\n",
        "            self.background_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "\n",
        "        desc = self.desc\n",
        "\n",
        "        for y in range(0, desc.shape[0]):\n",
        "            for x in range(0, desc.shape[1]):\n",
        "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
        "                self.window.blit(self.background_img, cell)\n",
        "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
        "                    self.window.blit(self.median_vert[0], cell)\n",
        "                elif desc[y][x] == b\"|\" and (\n",
        "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
        "                ):\n",
        "                    self.window.blit(self.median_vert[2], cell)\n",
        "                elif desc[y][x] == b\"|\":\n",
        "                    self.window.blit(self.median_vert[1], cell)\n",
        "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
        "                    self.window.blit(self.median_horiz[0], cell)\n",
        "                elif desc[y][x] == b\"-\" and (\n",
        "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
        "                ):\n",
        "                    self.window.blit(self.median_horiz[2], cell)\n",
        "                elif desc[y][x] == b\"-\":\n",
        "                    self.window.blit(self.median_horiz[1], cell)\n",
        "\n",
        "        for cell, color in zip(self.locs, self.locs_colors):\n",
        "            color_cell = pygame.Surface(self.cell_size)\n",
        "            color_cell.set_alpha(128)\n",
        "            color_cell.fill(color)\n",
        "            loc = self.get_surf_loc(cell)\n",
        "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
        "\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            self.window.blit(self.passenger_img, self.get_surf_loc(self.locs[pass_idx]))\n",
        "\n",
        "        if self.lastaction in [0, 1, 2, 3]:\n",
        "            self.taxi_orientation = self.lastaction\n",
        "        dest_loc = self.get_surf_loc(self.locs[dest_idx])\n",
        "        taxi_location = self.get_surf_loc((taxi_row, taxi_col))\n",
        "\n",
        "        if dest_loc[1] <= taxi_location[1]:\n",
        "            self.window.blit(\n",
        "                self.destination_img,\n",
        "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
        "            )\n",
        "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
        "        else:  # change blit order for overlapping appearance\n",
        "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
        "            self.window.blit(\n",
        "                self.destination_img,\n",
        "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
        "            )\n",
        "\n",
        "        if mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        elif mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def get_surf_loc(self, map_loc):\n",
        "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
        "            map_loc[0] + 1\n",
        "        ) * self.cell_size[1]\n",
        "\n",
        "    def _render_text(self):\n",
        "        desc = self.desc.copy().tolist()\n",
        "        outfile = StringIO()\n",
        "\n",
        "        out = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                out[1 + taxi_row][2 * taxi_col + 1], \"yellow\", highlight=True\n",
        "            )\n",
        "            pi, pj = self.locs[pass_idx]\n",
        "            out[1 + pi][2 * pj + 1] = utils.colorize(\n",
        "                out[1 + pi][2 * pj + 1], \"blue\", bold=True\n",
        "            )\n",
        "        else:  # passenger in taxi\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                ul(out[1 + taxi_row][2 * taxi_col + 1]), \"green\", highlight=True\n",
        "            )\n",
        "\n",
        "        di, dj = self.locs[dest_idx]\n",
        "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], \"magenta\")\n",
        "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
        "        if self.lastaction is not None:\n",
        "            outfile.write(\n",
        "                f\"  ({['South', 'North', 'East', 'West', 'Pickup', 'Dropoff'][self.lastaction]})\\n\"\n",
        "            )\n",
        "        else:\n",
        "            outfile.write(\"\\n\")\n",
        "\n",
        "        with closing(outfile):\n",
        "            return outfile.getvalue()\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe_oj--bRBoR"
      },
      "source": [
        "# Training Strategy(Policy Gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJPgdF2ZRNB9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMsqppIlRWWI"
      },
      "outputs": [],
      "source": [
        "class PolicyGradientAgentOptimized:\n",
        "\n",
        "    def __init__(self, n_states, n_actions,\n",
        "                 learning_rate=0.01,\n",
        "                 value_lr=0.1,\n",
        "                 lr_decay=0.9999,\n",
        "                 lr_min=0.0001,\n",
        "                 discount_factor=0.99,\n",
        "                 entropy_coef=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.lr_init = learning_rate\n",
        "        self.value_lr = value_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr_min = lr_min\n",
        "        self.gamma = discount_factor\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        # policy parameters\n",
        "        self.theta = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # Value function\n",
        "        self.V = np.zeros(n_states)\n",
        "\n",
        "        # statistics information\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        \"\"\"calculate action probability distribution (softmax)\"\"\"\n",
        "        theta_state = self.theta[state] - np.max(self.theta[state])\n",
        "        exp_theta = np.exp(theta_state)\n",
        "        return exp_theta / np.sum(exp_theta)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"sample action according to policy\"\"\"\n",
        "        policy = self.get_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update(self, episode_history):\n",
        "        \"\"\"\n",
        "        update policy using Advantage and Entropy\n",
        "        \"\"\"\n",
        "        if len(episode_history) == 0:\n",
        "            return\n",
        "\n",
        "        # 1. calculate return G_t\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for state, action, reward in reversed(episode_history):\n",
        "            G = reward + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # 2. update Value function\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            td_error = returns[t] - self.V[state]\n",
        "            self.V[state] += self.value_lr * td_error\n",
        "\n",
        "        # 3. calculate Advantage\n",
        "        advantages = []\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            advantage = returns[t] - self.V[state]\n",
        "            advantages.append(advantage)\n",
        "\n",
        "        # 4. standardize Advantage\n",
        "        advantages = np.array(advantages)\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-9)\n",
        "\n",
        "        # 5. update policy parameters\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            policy = self.get_policy(state)\n",
        "\n",
        "            # Policy gradient\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            grad[action] = 1.0\n",
        "            grad -= policy\n",
        "\n",
        "            # Entropy gradient\n",
        "            entropy_grad = -np.log(policy + 1e-9) - 1\n",
        "\n",
        "            # combine update\n",
        "            total_grad = (advantages[t] * grad +\n",
        "                         self.entropy_coef * entropy_grad)\n",
        "\n",
        "            self.theta[state] += self.lr * total_grad\n",
        "\n",
        "        # 6. decay learning rate\n",
        "        self.decay_learning_rate()\n",
        "        self.episode_count += 1\n",
        "\n",
        "    def decay_learning_rate(self):\n",
        "        \"\"\"gradually decrease learning rate\"\"\"\n",
        "        self.lr = max(self.lr_min, self.lr * self.lr_decay)\n",
        "\n",
        "    @property\n",
        "    def Q(self):\n",
        "        \"\"\"compatible test function\"\"\"\n",
        "        return self.theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward Wrapper"
      ],
      "metadata": {
        "id": "O9xcmKP2yJpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaxiRewardWrapper(gym.Wrapper): # TODO\n",
        "    \"\"\"\n",
        "    Reward wrapper for Taxi environment\n",
        "    Allows customizing reward values without modifying the original environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "        super().__init__(env)\n",
        "        self.custom_reward_step = reward_step\n",
        "        self.custom_reward_delivery = reward_delivery\n",
        "        self.custom_reward_illegal = reward_illegal\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Replace original rewards with custom rewards based on reward type\n",
        "        if reward == self.env.reward_delivery:\n",
        "\n",
        "            reward = self.custom_reward_delivery\n",
        "        elif reward == self.env.reward_illegal:\n",
        "\n",
        "            reward = self.custom_reward_illegal\n",
        "        elif reward == self.env.reward_step:\n",
        "\n",
        "            reward = self.custom_reward_step\n",
        "\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "X7WUWULGyB2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsWCn5CNR8uu"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOs5fC-aSDra"
      },
      "outputs": [],
      "source": [
        "def train(n_episodes=50000, max_steps=200,\n",
        "          seed_start=0, seed_end=40000, verbose=True,\n",
        "          reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"train optimized Policy Gradient Agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        reward_step=reward_step,\n",
        "        reward_delivery=reward_delivery,\n",
        "        reward_illegal=reward_illegal\n",
        "    )\n",
        "\n",
        "    agent = PolicyGradientAgentOptimized(\n",
        "        n_states=env.observation_space.n,\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.01,      # TODO\n",
        "        value_lr=0.1,         # TODO\n",
        "        lr_decay=0.99999,       # TODO\n",
        "        discount_factor=0.99,     # TODO\n",
        "        entropy_coef=0.1        # TODO\n",
        "    )\n",
        "\n",
        "    episode_rewards = []\n",
        "    success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"training episodes: {n_episodes}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
        "\n",
        "        episode_history = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_history.append((state, action, reward))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if terminated and reward == reward_delivery:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        agent.update(episode_history)\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            success_rate = success_count / min(1000, episode + 1)\n",
        "\n",
        "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
        "                  f\"avg reward: {avg_reward:.2f} | \"\n",
        "                  f\"success rate: {success_rate:.1%} | \"\n",
        "                  f\"learning rate: {agent.lr:.6f}\")\n",
        "\n",
        "            if episode >= 99:\n",
        "                success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7uAA9uSH8n"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SX1fiIOSOEb"
      },
      "outputs": [],
      "source": [
        "def test(model_filename, n_episodes=100, seed_start=42, verbose=True,\n",
        "         reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"test agent\"\"\"\n",
        "\n",
        "    # Load model for testing\n",
        "    print(f\"\\nload model\")\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        reward_step=reward_step,\n",
        "        reward_delivery=reward_delivery,\n",
        "        reward_illegal=reward_illegal\n",
        "    )\n",
        "\n",
        "    agent = PolicyGradientAgentOptimized(\n",
        "    n_states=env.observation_space.n,\n",
        "    n_actions=env.action_space.n\n",
        "    )\n",
        "    agent.theta = np.load(model_filename)\n",
        "\n",
        "    rewards = []\n",
        "    steps_list = []\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode)\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        for step in range(200):\n",
        "            # use deterministic policy during testing\n",
        "            action = np.argmax(agent.theta[state])\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if reward == reward_delivery:\n",
        "                    successes += 1\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps_list)\n",
        "    success_rate = successes / n_episodes\n",
        "\n",
        "    # calculate evaluation score (success rate 20%, steps 80%)\n",
        "    normalized_steps = avg_steps / 200\n",
        "    step_score = 1 - normalized_steps\n",
        "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\ntest result (seed {seed_start}-{seed_start+n_episodes-1}):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"   avg reward: {avg_reward:.2f}\")\n",
        "        print(f\"   avg steps: {avg_steps:.2f}\")\n",
        "        print(f\"   success rate: {success_rate:.1%}\")\n",
        "        print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_steps': avg_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'evaluation_score': evaluation_score,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb6lgXalTOzA"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6KRCuQ4TNjK",
        "outputId": "57679425-d98b-46db-9020-78a86b893e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "training episodes: 50000\n",
            "======================================================================\n",
            "episode 1000/50000 | avg reward: -720.81 | success rate: 4.7% | learning rate: 0.009900\n",
            "episode 2000/50000 | avg reward: -705.37 | success rate: 7.7% | learning rate: 0.009802\n",
            "episode 3000/50000 | avg reward: -711.15 | success rate: 9.5% | learning rate: 0.009704\n",
            "episode 4000/50000 | avg reward: -700.18 | success rate: 12.0% | learning rate: 0.009608\n",
            "episode 5000/50000 | avg reward: -701.70 | success rate: 13.1% | learning rate: 0.009512\n",
            "episode 6000/50000 | avg reward: -661.80 | success rate: 17.0% | learning rate: 0.009418\n",
            "episode 7000/50000 | avg reward: -663.53 | success rate: 17.8% | learning rate: 0.009324\n",
            "episode 8000/50000 | avg reward: -665.23 | success rate: 22.4% | learning rate: 0.009231\n",
            "episode 9000/50000 | avg reward: -666.26 | success rate: 28.8% | learning rate: 0.009139\n",
            "episode 10000/50000 | avg reward: -661.06 | success rate: 29.4% | learning rate: 0.009048\n",
            "episode 11000/50000 | avg reward: -624.29 | success rate: 32.5% | learning rate: 0.008958\n",
            "episode 12000/50000 | avg reward: -622.67 | success rate: 33.9% | learning rate: 0.008869\n",
            "episode 13000/50000 | avg reward: -624.34 | success rate: 33.7% | learning rate: 0.008781\n",
            "episode 14000/50000 | avg reward: -608.00 | success rate: 33.7% | learning rate: 0.008694\n",
            "episode 15000/50000 | avg reward: -600.52 | success rate: 34.2% | learning rate: 0.008607\n",
            "episode 16000/50000 | avg reward: -608.65 | success rate: 37.1% | learning rate: 0.008521\n",
            "episode 17000/50000 | avg reward: -613.58 | success rate: 38.0% | learning rate: 0.008437\n",
            "episode 18000/50000 | avg reward: -607.29 | success rate: 37.0% | learning rate: 0.008353\n",
            "episode 19000/50000 | avg reward: -610.27 | success rate: 41.5% | learning rate: 0.008270\n",
            "episode 20000/50000 | avg reward: -553.33 | success rate: 40.2% | learning rate: 0.008187\n",
            "episode 21000/50000 | avg reward: -609.31 | success rate: 37.8% | learning rate: 0.008106\n",
            "episode 22000/50000 | avg reward: -659.29 | success rate: 39.2% | learning rate: 0.008025\n",
            "episode 23000/50000 | avg reward: -550.95 | success rate: 40.5% | learning rate: 0.007945\n",
            "episode 24000/50000 | avg reward: -575.70 | success rate: 39.8% | learning rate: 0.007866\n",
            "episode 25000/50000 | avg reward: -589.52 | success rate: 43.6% | learning rate: 0.007788\n",
            "episode 26000/50000 | avg reward: -585.72 | success rate: 44.7% | learning rate: 0.007711\n",
            "episode 27000/50000 | avg reward: -598.89 | success rate: 42.2% | learning rate: 0.007634\n",
            "episode 28000/50000 | avg reward: -590.68 | success rate: 43.3% | learning rate: 0.007558\n",
            "episode 29000/50000 | avg reward: -580.79 | success rate: 42.6% | learning rate: 0.007483\n",
            "episode 30000/50000 | avg reward: -555.45 | success rate: 44.6% | learning rate: 0.007408\n",
            "episode 31000/50000 | avg reward: -588.20 | success rate: 42.2% | learning rate: 0.007334\n",
            "episode 32000/50000 | avg reward: -559.68 | success rate: 42.3% | learning rate: 0.007261\n",
            "episode 33000/50000 | avg reward: -589.49 | success rate: 42.8% | learning rate: 0.007189\n",
            "episode 34000/50000 | avg reward: -586.30 | success rate: 42.4% | learning rate: 0.007118\n",
            "episode 35000/50000 | avg reward: -566.30 | success rate: 46.6% | learning rate: 0.007047\n",
            "episode 36000/50000 | avg reward: -586.73 | success rate: 43.5% | learning rate: 0.006977\n",
            "episode 37000/50000 | avg reward: -603.32 | success rate: 43.3% | learning rate: 0.006907\n",
            "episode 38000/50000 | avg reward: -597.50 | success rate: 45.2% | learning rate: 0.006839\n",
            "episode 39000/50000 | avg reward: -545.71 | success rate: 44.5% | learning rate: 0.006771\n",
            "episode 40000/50000 | avg reward: -574.00 | success rate: 43.3% | learning rate: 0.006703\n",
            "episode 41000/50000 | avg reward: -579.38 | success rate: 42.3% | learning rate: 0.006636\n",
            "episode 42000/50000 | avg reward: -625.75 | success rate: 45.3% | learning rate: 0.006570\n",
            "episode 43000/50000 | avg reward: -539.56 | success rate: 43.1% | learning rate: 0.006505\n",
            "episode 44000/50000 | avg reward: -563.83 | success rate: 43.2% | learning rate: 0.006440\n",
            "episode 45000/50000 | avg reward: -625.67 | success rate: 37.9% | learning rate: 0.006376\n",
            "episode 46000/50000 | avg reward: -557.22 | success rate: 42.6% | learning rate: 0.006313\n",
            "episode 47000/50000 | avg reward: -580.05 | success rate: 43.8% | learning rate: 0.006250\n",
            "episode 48000/50000 | avg reward: -589.38 | success rate: 41.1% | learning rate: 0.006188\n",
            "episode 49000/50000 | avg reward: -591.74 | success rate: 44.0% | learning rate: 0.006126\n",
            "episode 50000/50000 | avg reward: -590.51 | success rate: 40.6% | learning rate: 0.006065\n",
            "======================================================================\n",
            "Training completed!\n",
            "\n",
            "model saved to policy_gradient_optimized.npy\n",
            "\n",
            "load model\n",
            "\n",
            "test result (seed 420000-420999):\n",
            "======================================================================\n",
            "   avg reward: -356.45\n",
            "   avg steps: 160.72\n",
            "   success rate: 21.0%\n",
            "   evaluation score: 0.1991 (19.91%)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set reward parameters\n",
        "    REWARD_STEP = -5\n",
        "    REWARD_DELIVERY = 20\n",
        "    REWARD_ILLEGAL = -1\n",
        "\n",
        "    # train\n",
        "    agent, rewards = train(\n",
        "        n_episodes=50000,\n",
        "        verbose=True,\n",
        "        reward_step=REWARD_STEP,\n",
        "        reward_delivery=REWARD_DELIVERY,\n",
        "        reward_illegal=REWARD_ILLEGAL\n",
        "    )\n",
        "\n",
        "    # save model\n",
        "    model_filename = 'policy_gradient_optimized.npy'\n",
        "    np.save(model_filename, agent.theta)\n",
        "    print(f\"\\nmodel saved to {model_filename}\")\n",
        "\n",
        "    # test\n",
        "    test(\n",
        "        model_filename,\n",
        "        n_episodes=1000,\n",
        "        seed_start=420000,\n",
        "        verbose=True,\n",
        "        reward_step=REWARD_STEP,\n",
        "        reward_delivery=REWARD_DELIVERY,\n",
        "        reward_illegal=REWARD_ILLEGAL\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}