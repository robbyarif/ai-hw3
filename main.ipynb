{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sbro3A_Qha1"
      },
      "source": [
        "# Taxi Env (Don't Modify it )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a1HlV_-2QUNN"
      },
      "outputs": [],
      "source": [
        "from contextlib import closing\n",
        "from io import StringIO\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import Env, spaces, utils\n",
        "from gymnasium.envs.toy_text.utils import categorical_sample\n",
        "from gymnasium.error import DependencyNotInstalled\n",
        "\n",
        "MAP_0 = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP_1 = [\n",
        "    \"+---------+\",\n",
        "    \"|R| : : :G|\",\n",
        "    \"| | : | : |\",\n",
        "    \"| : : | : |\",\n",
        "    \"| : | : : |\",\n",
        "    \"|Y: | :B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP_2 = [\n",
        "    \"+---------+\",\n",
        "    \"|R: : : |G|\",\n",
        "    \"| : : : | |\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : | | : |\",\n",
        "    \"|Y: : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP = MAP_0\n",
        "MAPS = [MAP_0, MAP_1, MAP_2]\n",
        "WINDOW_SIZE = (550, 350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tBkD72IKQxKT"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY TaxiEnv\n",
        "class TaxiEnv(Env):\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
        "        \"render_fps\": 4,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: str | None = None,\n",
        "        is_rainy: bool = True,\n",
        "        fickle_passenger: bool = False,\n",
        "        use_multiple_maps: bool = True,\n",
        "        reward_step: float = -1,\n",
        "        reward_delivery: float = 20,\n",
        "        reward_illegal: float = -10,\n",
        "    ):\n",
        "        self.use_multiple_maps = use_multiple_maps\n",
        "\n",
        "        if use_multiple_maps:\n",
        "            self.desc_maps = [np.asarray(m, dtype=\"c\") for m in MAPS]\n",
        "            self.P_maps = []\n",
        "            self.current_map_id = 0\n",
        "            self.desc = self.desc_maps[0]\n",
        "        else:\n",
        "            self.desc = np.asarray(MAP, dtype=\"c\")\n",
        "\n",
        "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
        "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255)]\n",
        "\n",
        "        self.reward_step = reward_step\n",
        "        self.reward_delivery = reward_delivery\n",
        "        self.reward_illegal = reward_illegal\n",
        "\n",
        "        num_states = 500\n",
        "        num_rows = 5\n",
        "        num_columns = 5\n",
        "        self.max_row = num_rows - 1\n",
        "        self.max_col = num_columns - 1\n",
        "        self.initial_state_distrib = np.zeros(num_states)\n",
        "        num_actions = 6\n",
        "\n",
        "\n",
        "        if use_multiple_maps:\n",
        "\n",
        "            for map_idx in range(3):\n",
        "                self.desc = self.desc_maps[map_idx]\n",
        "                P = {\n",
        "                    state: {action: [] for action in range(num_actions)}\n",
        "                    for state in range(num_states)\n",
        "                }\n",
        "                self.P = P\n",
        "\n",
        "                # Iterate through all possible state combinations\n",
        "                for row in range(num_rows):\n",
        "                    for col in range(num_columns):\n",
        "                        for pass_idx in range(len(locs) + 1):\n",
        "                            for dest_idx in range(len(locs)):\n",
        "                                state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                                if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                                    self.initial_state_distrib[state] += 1\n",
        "                                for action in range(num_actions):\n",
        "                                    if is_rainy:\n",
        "                                        self._build_rainy_transitions(\n",
        "                                            row, col, pass_idx, dest_idx, action,\n",
        "                                        )\n",
        "                                    else:\n",
        "                                        self._build_dry_transitions(\n",
        "                                            row, col, pass_idx, dest_idx, action,\n",
        "                                        )\n",
        "\n",
        "                self.P_maps.append(P)\n",
        "\n",
        "\n",
        "            self.desc = self.desc_maps[0]\n",
        "            self.P = self.P_maps[0]\n",
        "        else:\n",
        "\n",
        "            self.P = {\n",
        "                state: {action: [] for action in range(num_actions)}\n",
        "                for state in range(num_states)\n",
        "            }\n",
        "\n",
        "            for row in range(num_rows):\n",
        "                for col in range(num_columns):\n",
        "                    for pass_idx in range(len(locs) + 1):\n",
        "                        for dest_idx in range(len(locs)):\n",
        "                            state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                            if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                                self.initial_state_distrib[state] += 1\n",
        "                            for action in range(num_actions):\n",
        "                                if is_rainy:\n",
        "                                    self._build_rainy_transitions(\n",
        "                                        row, col, pass_idx, dest_idx, action,\n",
        "                                    )\n",
        "                                else:\n",
        "                                    self._build_dry_transitions(\n",
        "                                        row, col, pass_idx, dest_idx, action,\n",
        "                                    )\n",
        "\n",
        "        self.initial_state_distrib /= self.initial_state_distrib.sum()\n",
        "        self.action_space = spaces.Discrete(num_actions)\n",
        "        self.observation_space = spaces.Discrete(num_states)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.fickle_passenger = fickle_passenger\n",
        "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.5\n",
        "\n",
        "        # pygame utils\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.cell_size = (\n",
        "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
        "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
        "        )\n",
        "        self.taxi_imgs = None\n",
        "        self.taxi_orientation = 0\n",
        "        self.passenger_img = None\n",
        "        self.destination_img = None\n",
        "        self.median_horiz = None\n",
        "        self.median_vert = None\n",
        "        self.background_img = None\n",
        "\n",
        "    def _pickup(self, taxi_loc, pass_idx, reward):\n",
        "        \"\"\"Computes the new location and reward for pickup action.\"\"\"\n",
        "        if pass_idx < 4 and taxi_loc == self.locs[pass_idx]:\n",
        "            new_pass_idx = 4\n",
        "            new_reward = reward\n",
        "        else:  # passenger not at location\n",
        "            new_pass_idx = pass_idx\n",
        "            new_reward = self.reward_illegal\n",
        "\n",
        "        return new_pass_idx, new_reward\n",
        "\n",
        "    def _dropoff(self, taxi_loc, pass_idx, dest_idx, default_reward):\n",
        "        \"\"\"Computes the new location and reward for return dropoff action.\"\"\"\n",
        "        if (taxi_loc == self.locs[dest_idx]) and pass_idx == 4:\n",
        "            new_pass_idx = dest_idx\n",
        "            new_terminated = True\n",
        "            new_reward = self.reward_delivery\n",
        "        elif (taxi_loc in self.locs) and pass_idx == 4:\n",
        "            new_pass_idx = self.locs.index(taxi_loc)\n",
        "            new_terminated = False\n",
        "            new_reward = default_reward\n",
        "        else:  # dropoff at wrong location\n",
        "            new_pass_idx = pass_idx\n",
        "            new_terminated = False\n",
        "            new_reward = self.reward_illegal\n",
        "\n",
        "        return new_pass_idx, new_reward, new_terminated\n",
        "\n",
        "    def _calc_new_position(self, row, col, movement, offset=0):\n",
        "        \"\"\"Calculates the new position for a row and col to the movement.\"\"\"\n",
        "        dr, dc = movement\n",
        "        new_row = max(0, min(row + dr, self.max_row))\n",
        "        new_col = max(0, min(col + dc, self.max_col))\n",
        "        if self.desc[1 + new_row, 2 * new_col + offset] == b\":\":\n",
        "            return new_row, new_col\n",
        "        else:  # Default to current position if not traversable\n",
        "            return row, col\n",
        "\n",
        "    def _build_rainy_transitions(self, row, col, pass_idx, dest_idx, action):\n",
        "        \"\"\"Computes the next action for a state (row, col, pass_idx, dest_idx) and action for `is_rainy`.\"\"\"\n",
        "        state = self.encode(row, col, pass_idx, dest_idx)\n",
        "\n",
        "        taxi_loc = left_pos = right_pos = (row, col)\n",
        "        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
        "        reward = self.reward_step\n",
        "        terminated = False\n",
        "\n",
        "        moves = {\n",
        "            0: ((1, 0), (0, -1), (0, 1)),  # Down\n",
        "            1: ((-1, 0), (0, -1), (0, 1)),  # Up\n",
        "            2: ((0, 1), (1, 0), (-1, 0)),  # Right\n",
        "            3: ((0, -1), (1, 0), (-1, 0)),  # Left\n",
        "        }\n",
        "\n",
        "        # Check if movement is allowed\n",
        "        if (\n",
        "            action in {0, 1}\n",
        "            or (action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\")\n",
        "            or (action == 3 and self.desc[1 + row, 2 * col] == b\":\")\n",
        "        ):\n",
        "            dr, dc = moves[action][0]\n",
        "            new_row = max(0, min(row + dr, self.max_row))\n",
        "            new_col = max(0, min(col + dc, self.max_col))\n",
        "\n",
        "            left_pos = self._calc_new_position(row, col, moves[action][1], offset=2)\n",
        "            right_pos = self._calc_new_position(row, col, moves[action][2])\n",
        "        elif action == 4:  # pickup\n",
        "            new_pass_idx, reward = self._pickup(taxi_loc, new_pass_idx, reward)\n",
        "        elif action == 5:  # dropoff\n",
        "            new_pass_idx, reward, terminated = self._dropoff(\n",
        "                taxi_loc, new_pass_idx, dest_idx, reward\n",
        "            )\n",
        "        intended_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
        "\n",
        "        if action <= 3:\n",
        "            left_state = self.encode(left_pos[0], left_pos[1], new_pass_idx, dest_idx)\n",
        "            right_state = self.encode(\n",
        "                right_pos[0], right_pos[1], new_pass_idx, dest_idx\n",
        "            )\n",
        "\n",
        "            self.P[state][action].append((0.8, intended_state, self.reward_step, terminated))\n",
        "            self.P[state][action].append((0.1, left_state, self.reward_step, terminated))\n",
        "            self.P[state][action].append((0.1, right_state, self.reward_step, terminated))\n",
        "        else:\n",
        "            self.P[state][action].append((1.0, intended_state, reward, terminated))\n",
        "\n",
        "\n",
        "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
        "        # (5) 5, 5, 4\n",
        "        i = taxi_row\n",
        "        i *= 5\n",
        "        i += taxi_col\n",
        "        i *= 5\n",
        "        i += pass_loc\n",
        "        i *= 4\n",
        "        i += dest_idx\n",
        "        return i\n",
        "\n",
        "    def decode(self, i):\n",
        "        out = []\n",
        "        out.append(i % 4)\n",
        "        i = i // 4\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i)\n",
        "        assert 0 <= i < 5\n",
        "        return reversed(out)\n",
        "\n",
        "    def action_mask(self, state: int):\n",
        "        \"\"\"Computes an action mask for the action space using the state information.\"\"\"\n",
        "        mask = np.zeros(6, dtype=np.int8)\n",
        "        taxi_row, taxi_col, pass_loc, dest_idx = self.decode(state)\n",
        "        if taxi_row < 4:\n",
        "            mask[0] = 1\n",
        "        if taxi_row > 0:\n",
        "            mask[1] = 1\n",
        "        if taxi_col < 4 and self.desc[taxi_row + 1, 2 * taxi_col + 2] == b\":\":\n",
        "            mask[2] = 1\n",
        "        if taxi_col > 0 and self.desc[taxi_row + 1, 2 * taxi_col] == b\":\":\n",
        "            mask[3] = 1\n",
        "        if pass_loc < 4 and (taxi_row, taxi_col) == self.locs[pass_loc]:\n",
        "            mask[4] = 1\n",
        "        if pass_loc == 4 and (\n",
        "            (taxi_row, taxi_col) == self.locs[dest_idx]\n",
        "            or (taxi_row, taxi_col) in self.locs\n",
        "        ):\n",
        "            mask[5] = 1\n",
        "        return mask\n",
        "\n",
        "    def step(self, a):\n",
        "        transitions = self.P[self.s][a]\n",
        "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
        "        p, s, r, t = transitions[i]\n",
        "        self.lastaction = a\n",
        "\n",
        "        shadow_row, shadow_col, shadow_pass_loc, shadow_dest_idx = self.decode(self.s)\n",
        "        taxi_row, taxi_col, pass_loc, _ = self.decode(s)\n",
        "\n",
        "        # If we are in the fickle step, the passenger has been in the vehicle for at least a step and this step the\n",
        "        # position changed\n",
        "        if (\n",
        "            self.fickle_passenger\n",
        "            and self.fickle_step\n",
        "            and shadow_pass_loc == 4\n",
        "            and (taxi_row != shadow_row or taxi_col != shadow_col)\n",
        "        ):\n",
        "            self.fickle_step = False\n",
        "            possible_destinations = [\n",
        "                i for i in range(len(self.locs)) if i != shadow_dest_idx\n",
        "            ]\n",
        "            dest_idx = self.np_random.choice(possible_destinations)\n",
        "            s = self.encode(taxi_row, taxi_col, pass_loc, dest_idx)\n",
        "\n",
        "        self.s = s\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
        "        return int(s), r, t, False, {\"prob\": p, \"action_mask\": self.action_mask(s)}\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: int | None = None,\n",
        "        options: dict | None = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "\n",
        "        if self.use_multiple_maps and seed is not None:\n",
        "            map_id = seed % 3\n",
        "            self.current_map_id = map_id\n",
        "            self.desc = self.desc_maps[map_id]\n",
        "            self.P = self.P_maps[map_id]\n",
        "\n",
        "        self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
        "        self.lastaction = None\n",
        "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.3\n",
        "        self.taxi_orientation = 0\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return int(self.s), {\"prob\": 1.0, \"action_mask\": self.action_mask(self.s)}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            assert self.spec is not None\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "        elif self.render_mode == \"ansi\":\n",
        "            return self._render_text()\n",
        "        else:  # self.render_mode in {\"human\", \"rgb_array\"}:\n",
        "            return self._render_gui(self.render_mode)\n",
        "\n",
        "    def _render_gui(self, mode):\n",
        "        try:\n",
        "            import pygame  # dependency to pygame only if rendering with human\n",
        "        except ImportError as e:\n",
        "            raise DependencyNotInstalled(\n",
        "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
        "            ) from e\n",
        "\n",
        "        if self.window is None:\n",
        "            pygame.init()\n",
        "            pygame.display.set_caption(\"Taxi\")\n",
        "            if mode == \"human\":\n",
        "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
        "            elif mode == \"rgb_array\":\n",
        "                self.window = pygame.Surface(WINDOW_SIZE)\n",
        "\n",
        "        assert (\n",
        "            self.window is not None\n",
        "        ), \"Something went wrong with pygame. This should never happen.\"\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "        if self.taxi_imgs is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/cab_front.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_rear.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_right.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_left.png\"),\n",
        "            ]\n",
        "            self.taxi_imgs = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.passenger_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/passenger.png\")\n",
        "            self.passenger_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "        if self.destination_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/hotel.png\")\n",
        "            self.destination_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "            self.destination_img.set_alpha(170)\n",
        "        if self.median_horiz is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_left.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_horiz.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_right.png\"),\n",
        "            ]\n",
        "            self.median_horiz = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.median_vert is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_top.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_vert.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_bottom.png\"),\n",
        "            ]\n",
        "            self.median_vert = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.background_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/taxi_background.png\")\n",
        "            self.background_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "\n",
        "        desc = self.desc\n",
        "\n",
        "        for y in range(0, desc.shape[0]):\n",
        "            for x in range(0, desc.shape[1]):\n",
        "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
        "                self.window.blit(self.background_img, cell)\n",
        "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
        "                    self.window.blit(self.median_vert[0], cell)\n",
        "                elif desc[y][x] == b\"|\" and (\n",
        "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
        "                ):\n",
        "                    self.window.blit(self.median_vert[2], cell)\n",
        "                elif desc[y][x] == b\"|\":\n",
        "                    self.window.blit(self.median_vert[1], cell)\n",
        "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
        "                    self.window.blit(self.median_horiz[0], cell)\n",
        "                elif desc[y][x] == b\"-\" and (\n",
        "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
        "                ):\n",
        "                    self.window.blit(self.median_horiz[2], cell)\n",
        "                elif desc[y][x] == b\"-\":\n",
        "                    self.window.blit(self.median_horiz[1], cell)\n",
        "\n",
        "        for cell, color in zip(self.locs, self.locs_colors):\n",
        "            color_cell = pygame.Surface(self.cell_size)\n",
        "            color_cell.set_alpha(128)\n",
        "            color_cell.fill(color)\n",
        "            loc = self.get_surf_loc(cell)\n",
        "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
        "\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            self.window.blit(self.passenger_img, self.get_surf_loc(self.locs[pass_idx]))\n",
        "\n",
        "        if self.lastaction in [0, 1, 2, 3]:\n",
        "            self.taxi_orientation = self.lastaction\n",
        "        dest_loc = self.get_surf_loc(self.locs[dest_idx])\n",
        "        taxi_location = self.get_surf_loc((taxi_row, taxi_col))\n",
        "\n",
        "        if dest_loc[1] <= taxi_location[1]:\n",
        "            self.window.blit(\n",
        "                self.destination_img,\n",
        "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
        "            )\n",
        "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
        "        else:  # change blit order for overlapping appearance\n",
        "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
        "            self.window.blit(\n",
        "                self.destination_img,\n",
        "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
        "            )\n",
        "\n",
        "        if mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        elif mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def get_surf_loc(self, map_loc):\n",
        "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
        "            map_loc[0] + 1\n",
        "        ) * self.cell_size[1]\n",
        "\n",
        "    def _render_text(self):\n",
        "        desc = self.desc.copy().tolist()\n",
        "        outfile = StringIO()\n",
        "\n",
        "        out = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                out[1 + taxi_row][2 * taxi_col + 1], \"yellow\", highlight=True\n",
        "            )\n",
        "            pi, pj = self.locs[pass_idx]\n",
        "            out[1 + pi][2 * pj + 1] = utils.colorize(\n",
        "                out[1 + pi][2 * pj + 1], \"blue\", bold=True\n",
        "            )\n",
        "        else:  # passenger in taxi\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                ul(out[1 + taxi_row][2 * taxi_col + 1]), \"green\", highlight=True\n",
        "            )\n",
        "\n",
        "        di, dj = self.locs[dest_idx]\n",
        "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], \"magenta\")\n",
        "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
        "        if self.lastaction is not None:\n",
        "            outfile.write(\n",
        "                f\"  ({['South', 'North', 'East', 'West', 'Pickup', 'Dropoff'][self.lastaction]})\\n\"\n",
        "            )\n",
        "        else:\n",
        "            outfile.write(\"\\n\")\n",
        "\n",
        "        with closing(outfile):\n",
        "            return outfile.getvalue()\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe_oj--bRBoR"
      },
      "source": [
        "# Training Strategy(Policy Gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LJPgdF2ZRNB9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QMsqppIlRWWI"
      },
      "outputs": [],
      "source": [
        "class PolicyGradientAgentOptimized:\n",
        "\n",
        "    def __init__(self, n_states, n_actions,\n",
        "                 learning_rate=0.01,\n",
        "                 value_lr=0.1,\n",
        "                 lr_decay=0.9999,\n",
        "                 lr_min=0.0001,\n",
        "                 discount_factor=0.99,\n",
        "                 entropy_coef=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.lr_init = learning_rate\n",
        "        self.value_lr = value_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr_min = lr_min\n",
        "        self.gamma = discount_factor\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        # policy parameters\n",
        "        self.theta = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # Value function\n",
        "        self.V = np.zeros(n_states)\n",
        "\n",
        "        # statistics information\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        \"\"\"calculate action probability distribution (softmax)\"\"\"\n",
        "        theta_state = self.theta[state] - np.max(self.theta[state])\n",
        "        exp_theta = np.exp(theta_state)\n",
        "        return exp_theta / np.sum(exp_theta)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"sample action according to policy\"\"\"\n",
        "        policy = self.get_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update(self, episode_history):\n",
        "        \"\"\"\n",
        "        update policy using Advantage and Entropy\n",
        "        \"\"\"\n",
        "        if len(episode_history) == 0:\n",
        "            return\n",
        "\n",
        "        # 1. calculate return G_t\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for state, action, reward in reversed(episode_history):\n",
        "            G = reward + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # 2. update Value function\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            td_error = returns[t] - self.V[state]\n",
        "            self.V[state] += self.value_lr * td_error\n",
        "\n",
        "        # 3. calculate Advantage\n",
        "        advantages = []\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            advantage = returns[t] - self.V[state]\n",
        "            advantages.append(advantage)\n",
        "\n",
        "        # 4. standardize Advantage\n",
        "        advantages = np.array(advantages)\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-9)\n",
        "\n",
        "        # 5. update policy parameters\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            policy = self.get_policy(state)\n",
        "\n",
        "            # Policy gradient\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            grad[action] = 1.0\n",
        "            grad -= policy\n",
        "\n",
        "            # Entropy gradient\n",
        "            entropy_grad = -np.log(policy + 1e-9) - 1\n",
        "\n",
        "            # combine update\n",
        "            total_grad = (advantages[t] * grad +\n",
        "                         self.entropy_coef * entropy_grad)\n",
        "\n",
        "            self.theta[state] += self.lr * total_grad\n",
        "\n",
        "        # 6. decay learning rate\n",
        "        self.decay_learning_rate()\n",
        "        self.episode_count += 1\n",
        "\n",
        "    def decay_learning_rate(self):\n",
        "        \"\"\"gradually decrease learning rate\"\"\"\n",
        "        self.lr = max(self.lr_min, self.lr * self.lr_decay)\n",
        "\n",
        "    @property\n",
        "    def Q(self):\n",
        "        \"\"\"compatible test function\"\"\"\n",
        "        return self.theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward Wrapper"
      ],
      "metadata": {
        "id": "O9xcmKP2yJpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaxiRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Success-aware Reward Wrapper for Taxi-v3 with custom reward rules\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "        super().__init__(env)\n",
        "        # These are the base rewards from the environment, we'll modify them\n",
        "        # self.custom_reward_step = reward_step\n",
        "        # self.custom_reward_delivery = reward_delivery\n",
        "        # self.custom_reward_illegal = reward_illegal\n",
        "\n",
        "        # --- CUSTOM REWARD RULES ---\n",
        "        self._valid_move_penalty = -0.5\n",
        "        self._wall_penalty = -2\n",
        "        self._successful_pickup_bonus = 5\n",
        "        self._wrong_pickup_penalty = -5\n",
        "        self._successful_dropoff_reward = 20\n",
        "        self._wrong_dropoff_penalty = -5\n",
        "        self._delivery_bonus = 30 # This will be added on top of successful_dropoff_reward\n",
        "        self._timeout_penalty = -15\n",
        "        self._closer_reward = 1 # Reward for getting closer to the target\n",
        "        self._farther_penalty = -1.5 # Penalty for getting farther from the target\n",
        "\n",
        "\n",
        "        # trackers\n",
        "        self._prev_state = None\n",
        "        self._prev_distance = None\n",
        "        self._picked = False\n",
        "        self._last_action = None\n",
        "        self._last_pos = None\n",
        "        self._episode_timed_out = False # Added to track timeout\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self._prev_state = None\n",
        "        self._prev_distance = None\n",
        "        self._picked = False\n",
        "        self._last_action = None\n",
        "        self._last_pos = None\n",
        "        self._episode_timed_out = False # Reset on new episode\n",
        "        obs, info = super().reset(**kwargs)\n",
        "        self._prev_state = self.env.s\n",
        "        self._prev_distance, _, _ = self._distance_to_target(self._prev_state) # Initialize previous distance\n",
        "        return obs, info\n",
        "\n",
        "    def _decode_state(self, state):\n",
        "        return self.env.decode(state)\n",
        "\n",
        "    def _distance_to_target(self, state):\n",
        "        taxi_row, taxi_col, pass_loc, dest_idx = self._decode_state(state)\n",
        "        target_row, target_col = self.env.locs[dest_idx if pass_loc == 4 else pass_loc]\n",
        "        return abs(taxi_row - target_row) + abs(taxi_col - target_col), pass_loc, dest_idx\n",
        "\n",
        "    def action_mask(self, state):\n",
        "        \"\"\"Pass through the action_mask call to the wrapped environment.\"\"\"\n",
        "        return self.env.action_mask(state)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Get state before taking the step\n",
        "        prev_taxi_row, prev_taxi_col, prev_pass_loc, prev_dest_idx = self._decode_state(self.env.s)\n",
        "        prev_taxi_loc = (prev_taxi_row, prev_taxi_col)\n",
        "\n",
        "\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        next_state = obs\n",
        "\n",
        "        # Get state after taking the step\n",
        "        next_taxi_row, next_taxi_col, next_pass_loc, next_dest_idx = self._decode_state(next_state)\n",
        "        next_taxi_loc = (next_taxi_row, next_taxi_col)\n",
        "\n",
        "        custom_reward = 0\n",
        "\n",
        "        # --- Apply Custom Reward Rules ---\n",
        "\n",
        "        # 1. Successful drop-off and Delivery bonus\n",
        "        if terminated and (next_pass_loc == next_dest_idx) and (next_taxi_loc == self.env.locs[next_dest_idx]):\n",
        "            custom_reward += self._successful_dropoff_reward + self._delivery_bonus\n",
        "            info[\"success\"] = True\n",
        "        # 2. Episode too long (timeout)\n",
        "        elif truncated:\n",
        "            custom_reward += self._timeout_penalty\n",
        "            self._episode_timed_out = True\n",
        "            info[\"timeout\"] = True\n",
        "        # 3. Hitting a wall (invalid move) - Check if taxi location didn't change but action was a move\n",
        "        elif reward == self.env.reward_illegal and action in {0, 1, 2, 3}:\n",
        "             custom_reward += self._wall_penalty\n",
        "        # 4. Wrong pickup (no passenger) - Check if pickup action was taken but passenger is not in taxi and not at pickup location\n",
        "        elif action == 4 and next_pass_loc != 4 and prev_taxi_loc != self.env.locs[prev_pass_loc]:\n",
        "             custom_reward += self._wrong_pickup_penalty\n",
        "        # 5. Wrong drop-off - Check if dropoff action was taken but not at destination or passenger not in taxi\n",
        "        elif action == 5 and (next_pass_loc != next_dest_idx or next_taxi_loc != self.env.locs[next_dest_idx]):\n",
        "             custom_reward += self._wrong_dropoff_penalty\n",
        "        # 6. Successful pickup\n",
        "        elif action == 4 and next_pass_loc == 4 and prev_pass_loc != 4:\n",
        "            custom_reward += self._successful_pickup_bonus\n",
        "            self._picked = True\n",
        "        # 7. Distance-based reward/penalty for valid moves\n",
        "        elif reward == self.env.reward_step and action in {0, 1, 2, 3}: # Only apply to movement actions\n",
        "            current_distance, _, _ = self._distance_to_target(next_state)\n",
        "            if current_distance < self._prev_distance:\n",
        "                custom_reward += self._closer_reward\n",
        "            elif current_distance > self._prev_distance:\n",
        "                custom_reward += self._farther_penalty\n",
        "            else: # Stayed in the same spot but it was a valid move\n",
        "                custom_reward += self._valid_move_penalty # Apply a small penalty for not moving closer\n",
        "\n",
        "\n",
        "        # Update trackers\n",
        "        self._last_action = action\n",
        "        self._last_pos = (next_taxi_row, next_taxi_col)\n",
        "        self._prev_state = next_state\n",
        "        self._prev_distance, _, _ = self._distance_to_target(next_state) # Update previous distance\n",
        "\n",
        "        # Add info about picked status and distance for potential debugging/analysis\n",
        "        dist, _, _ = self._distance_to_target(next_state)\n",
        "        info.update({\n",
        "            \"picked\": self._picked,\n",
        "            \"distance_to_target\": dist,\n",
        "            \"episode_timed_out\": self._episode_timed_out # Include timeout info\n",
        "        })\n",
        "\n",
        "        return obs, custom_reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "OitVZ9Ezw9cX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsWCn5CNR8uu"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BOs5fC-aSDra"
      },
      "outputs": [],
      "source": [
        "def train(n_episodes=50000, max_steps=200,\n",
        "          seed_start=0, seed_end=40000, verbose=True,\n",
        "          reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"train optimized Policy Gradient Agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        # reward_step=reward_step,\n",
        "        # reward_delivery=reward_delivery,\n",
        "        # reward_illegal=reward_illegal,\n",
        "        # reward_closer_to_target=reward_closer_to_target,\n",
        "        # reward_pickup=reward_pickup\n",
        "    )\n",
        "\n",
        "    agent = PolicyGradientAgentOptimized(\n",
        "        n_states=env.observation_space.n,\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.01,      # TODO\n",
        "        value_lr=0.1,         # TODO\n",
        "        lr_decay=0.99999,       # TODO\n",
        "        discount_factor=0.99,     # TODO\n",
        "        entropy_coef=0.1        # TODO\n",
        "    )\n",
        "\n",
        "    episode_rewards = []\n",
        "    success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"training episodes: {n_episodes}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
        "\n",
        "        episode_history = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_history.append((state, action, reward))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if terminated and info.get(\"success\", False): # Check for success from the wrapper info\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "\n",
        "        agent.update(episode_history)\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            success_rate = success_count / min(1000, episode + 1)\n",
        "\n",
        "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
        "                  f\"avg reward: {avg_reward:.2f} | \"\n",
        "                  f\"success rate: {success_rate:.1%} | \"\n",
        "                  f\"learning rate: {agent.lr:.6f}\")\n",
        "\n",
        "            if episode >= 999: # Reset success count every 1000 episodes\n",
        "                success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7uAA9uSH8n"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8SX1fiIOSOEb"
      },
      "outputs": [],
      "source": [
        "def test(model_filename, n_episodes=100, seed_start=42, verbose=True,\n",
        "         reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"test agent\"\"\"\n",
        "\n",
        "    # Load model for testing\n",
        "    print(f\"\\nload model\")\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        # reward_step=reward_step,\n",
        "        # reward_delivery=reward_delivery,\n",
        "        # reward_illegal=reward_illegal,\n",
        "        # reward_closer_to_target=reward_closer_to_target,\n",
        "        # reward_pickup=reward_pickup\n",
        "    )\n",
        "\n",
        "    agent = PolicyGradientAgentOptimized(\n",
        "    n_states=env.observation_space.n,\n",
        "    n_actions=env.action_space.n\n",
        "    )\n",
        "    agent.theta = np.load(model_filename)\n",
        "\n",
        "    rewards = []\n",
        "    steps_list = []\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode)\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        for step in range(200):\n",
        "            # use deterministic policy during testing\n",
        "            action = np.argmax(agent.theta[state])\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if info.get(\"success\", False): # Check for success from the wrapper info\n",
        "                    successes += 1\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps_list)\n",
        "    success_rate = successes / n_episodes\n",
        "\n",
        "    # calculate evaluation score (success rate 20%, steps 80%)\n",
        "    normalized_steps = avg_steps / 200\n",
        "    step_score = 1 - normalized_steps\n",
        "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\ntest result (seed {seed_start}-{seed_start+n_episodes-1}):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"   avg reward: {avg_reward:.2f}\")\n",
        "        print(f\"   avg steps: {avg_steps:.2f}\")\n",
        "        print(f\"   success rate: {success_rate:.1%}\")\n",
        "        print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_steps': avg_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'evaluation_score': evaluation_score,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb6lgXalTOzA"
      },
      "source": [
        "# Baseline (Policy Gradient with Custom Reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6KRCuQ4TNjK",
        "outputId": "8f6a52dd-3662-49d8-f605-1ea0767c6f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "training episodes: 50000\n",
            "======================================================================\n",
            "episode 1000/50000 | avg reward: -314.23 | success rate: 6.1% | learning rate: 0.009900\n",
            "episode 2000/50000 | avg reward: -293.03 | success rate: 15.0% | learning rate: 0.009802\n",
            "episode 3000/50000 | avg reward: -246.99 | success rate: 22.4% | learning rate: 0.009704\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "\n",
        "# Initial reward parameters\n",
        "REWARD_STEP = -1\n",
        "REWARD_DELIVERY = 20\n",
        "REWARD_ILLEGAL = -5\n",
        "\n",
        "# train\n",
        "agent, rewards = train(\n",
        "    n_episodes=50000,\n",
        "    verbose=True,\n",
        "    reward_step=REWARD_STEP,\n",
        "    reward_delivery=REWARD_DELIVERY,\n",
        "    reward_illegal=REWARD_ILLEGAL,\n",
        ")\n",
        "\n",
        "# save model\n",
        "model_filename = 'policy_gradient_optimized.npy'\n",
        "np.save(model_filename, agent.theta)\n",
        "print(f\"\\nmodel saved to {model_filename}\")\n",
        "\n",
        "# test\n",
        "test(\n",
        "    model_filename,\n",
        "    n_episodes=1000,\n",
        "    seed_start=420000,\n",
        "    verbose=True,\n",
        "    reward_step=REWARD_STEP,\n",
        "    reward_delivery=REWARD_DELIVERY,\n",
        "    reward_illegal=REWARD_ILLEGAL,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning (Main Model)"
      ],
      "metadata": {
        "id": "GOhwtYbqBFrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay_rate=0.999, epsilon_min=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay_rate = epsilon_decay_rate\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.q_table = np.zeros((n_states, n_actions))\n",
        "\n",
        "    def choose_action(self, state, env):\n",
        "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Explore: choose a random action\n",
        "            return np.random.choice(self.n_actions)\n",
        "        else:\n",
        "            # Exploit: choose the action with the highest Q-value\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"Update Q-table using the Q-learning update rule\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
        "        td_error = td_target - self.q_table[state][action]\n",
        "        self.q_table[state][action] += self.lr * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay epsilon\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay_rate)"
      ],
      "metadata": {
        "id": "yw9oaU42Awti"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c331f1b4"
      },
      "source": [
        "def train_q_learning(n_episodes=50000, max_steps=200,\n",
        "                     seed_start=0, seed_end=40000, verbose=True):\n",
        "    \"\"\"train Q-learning Agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(TaxiEnv())\n",
        "\n",
        "# The best hyperparameters identified through the grid search were a learning_rate of 0.1, discount_factor of 0.99, epsilon_decay_rate of 0.999, and epsilon_min of 0.01.\n",
        "# This hyperparameter combination achieved the highest evaluation score of 0.7042.\n",
        "# The performance metrics for the best hyperparameters included an average reward of 15.18, an average of 66.95 steps per episode, and a success rate of 86.0%.\n",
        "\n",
        "    agent = QLearningAgent(\n",
        "        n_states=env.observation_space.n,\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.1,      #\n",
        "        discount_factor=0.99,     #\n",
        "        epsilon=1.0,          #\n",
        "        epsilon_decay_rate=0.999, # Updated from 0.9999 based on hyperparameter tuning\n",
        "        epsilon_min=0.01        #\n",
        "    )\n",
        "\n",
        "    episode_rewards = []\n",
        "    success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"training episodes: {n_episodes}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
        "\n",
        "        episode_history = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state, env)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.update(state, action, reward, next_state)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if terminated and info.get(\"success\", False): # Check for success from the wrapper info\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            success_rate = success_count / min(1000, episode + 1)\n",
        "\n",
        "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
        "                  f\"avg reward: {avg_reward:.2f} | \"\n",
        "                  f\"success rate: {success_rate:.1%} | \"\n",
        "                  f\"epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "            if episode >= 999: # Reset success count every 1000 episodes\n",
        "                success_count = 0\n",
        "\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aedd1572"
      },
      "source": [
        "def test_q_learning(agent, n_episodes=100, seed_start=42, verbose=True):\n",
        "    \"\"\"test Q-learning agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(TaxiEnv())\n",
        "\n",
        "    rewards = []\n",
        "    steps_list = []\n",
        "    successes = 0\n",
        "\n",
        "    # Set epsilon to 0 for deterministic policy during testing\n",
        "    agent.epsilon = 0\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode)\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        for step in range(200):\n",
        "            # use deterministic policy during testing\n",
        "            action = np.argmax(agent.q_table[state])\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if info.get(\"success\", False): # Check for success from the wrapper info\n",
        "                    successes += 1\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps_list)\n",
        "    success_rate = successes / n_episodes\n",
        "\n",
        "    # calculate evaluation score (success rate 20%, steps 80%)\n",
        "    normalized_steps = avg_steps / 200\n",
        "    step_score = 1 - normalized_steps\n",
        "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\ntest result (seed {seed_start}-{seed_start+n_episodes-1}):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"   avg reward: {avg_reward:.2f}\")\n",
        "        print(f\"   avg steps: {avg_steps:.2f}\")\n",
        "        print(f\"   success rate: {success_rate:.1%}\")\n",
        "        print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_steps': avg_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'evaluation_score': evaluation_score,\n",
        "    }"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 70)\n",
        "\n",
        "# train\n",
        "q_agent, q_rewards = train_q_learning(\n",
        "    n_episodes=50000,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# save model\n",
        "q_model_filename = 'Model.npy' # 'q_learning_agent.npy'\n",
        "np.save(q_model_filename, q_agent.q_table)\n",
        "print(f\"\\nmodel saved to {q_model_filename}\")\n",
        "\n",
        "# test\n",
        "test_q_learning(\n",
        "    q_agent,\n",
        "    n_episodes=1000,\n",
        "    seed_start=420000,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar1E03vQy8NV",
        "outputId": "9357ff1e-c183-4213-fc38-d3fb06734f8d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "training episodes: 50000\n",
            "======================================================================\n",
            "episode 1000/50000 | avg reward: -14.27 | success rate: 47.6% | epsilon: 0.3677\n",
            "episode 2000/50000 | avg reward: 26.32 | success rate: 87.5% | epsilon: 0.1352\n",
            "episode 3000/50000 | avg reward: 41.73 | success rate: 93.2% | epsilon: 0.0497\n",
            "episode 4000/50000 | avg reward: 42.19 | success rate: 93.8% | epsilon: 0.0183\n",
            "episode 5000/50000 | avg reward: 41.56 | success rate: 93.8% | epsilon: 0.0100\n",
            "episode 6000/50000 | avg reward: 46.03 | success rate: 94.3% | epsilon: 0.0100\n",
            "episode 7000/50000 | avg reward: 39.19 | success rate: 94.4% | epsilon: 0.0100\n",
            "episode 8000/50000 | avg reward: 45.81 | success rate: 95.8% | epsilon: 0.0100\n",
            "episode 9000/50000 | avg reward: 38.31 | success rate: 95.7% | epsilon: 0.0100\n",
            "episode 10000/50000 | avg reward: 47.47 | success rate: 94.9% | epsilon: 0.0100\n",
            "episode 11000/50000 | avg reward: 48.02 | success rate: 94.7% | epsilon: 0.0100\n",
            "episode 12000/50000 | avg reward: 48.26 | success rate: 96.6% | epsilon: 0.0100\n",
            "episode 13000/50000 | avg reward: 51.42 | success rate: 96.9% | epsilon: 0.0100\n",
            "episode 14000/50000 | avg reward: 47.13 | success rate: 96.7% | epsilon: 0.0100\n",
            "episode 15000/50000 | avg reward: 45.78 | success rate: 94.5% | epsilon: 0.0100\n",
            "episode 16000/50000 | avg reward: 48.86 | success rate: 96.5% | epsilon: 0.0100\n",
            "episode 17000/50000 | avg reward: 48.59 | success rate: 96.9% | epsilon: 0.0100\n",
            "episode 18000/50000 | avg reward: 46.95 | success rate: 96.0% | epsilon: 0.0100\n",
            "episode 19000/50000 | avg reward: 47.20 | success rate: 95.9% | epsilon: 0.0100\n",
            "episode 20000/50000 | avg reward: 47.81 | success rate: 97.5% | epsilon: 0.0100\n",
            "episode 21000/50000 | avg reward: 45.99 | success rate: 96.7% | epsilon: 0.0100\n",
            "episode 22000/50000 | avg reward: 42.21 | success rate: 96.6% | epsilon: 0.0100\n",
            "episode 23000/50000 | avg reward: 45.98 | success rate: 96.9% | epsilon: 0.0100\n",
            "episode 24000/50000 | avg reward: 48.53 | success rate: 97.0% | epsilon: 0.0100\n",
            "episode 25000/50000 | avg reward: 45.34 | success rate: 96.7% | epsilon: 0.0100\n",
            "episode 26000/50000 | avg reward: 49.53 | success rate: 97.3% | epsilon: 0.0100\n",
            "episode 27000/50000 | avg reward: 45.05 | success rate: 96.8% | epsilon: 0.0100\n",
            "episode 28000/50000 | avg reward: 49.53 | success rate: 96.5% | epsilon: 0.0100\n",
            "episode 29000/50000 | avg reward: 45.88 | success rate: 96.2% | epsilon: 0.0100\n",
            "episode 30000/50000 | avg reward: 44.34 | success rate: 96.3% | epsilon: 0.0100\n",
            "episode 31000/50000 | avg reward: 44.73 | success rate: 96.0% | epsilon: 0.0100\n",
            "episode 32000/50000 | avg reward: 45.78 | success rate: 96.0% | epsilon: 0.0100\n",
            "episode 33000/50000 | avg reward: 44.59 | success rate: 96.1% | epsilon: 0.0100\n",
            "episode 34000/50000 | avg reward: 42.37 | success rate: 96.1% | epsilon: 0.0100\n",
            "episode 35000/50000 | avg reward: 44.32 | success rate: 96.7% | epsilon: 0.0100\n",
            "episode 36000/50000 | avg reward: 46.51 | success rate: 96.5% | epsilon: 0.0100\n",
            "episode 37000/50000 | avg reward: 48.81 | success rate: 96.7% | epsilon: 0.0100\n",
            "episode 38000/50000 | avg reward: 44.33 | success rate: 96.6% | epsilon: 0.0100\n",
            "episode 39000/50000 | avg reward: 48.85 | success rate: 96.8% | epsilon: 0.0100\n",
            "episode 40000/50000 | avg reward: 50.26 | success rate: 96.2% | epsilon: 0.0100\n",
            "episode 41000/50000 | avg reward: 50.87 | success rate: 97.0% | epsilon: 0.0100\n",
            "episode 42000/50000 | avg reward: 47.20 | success rate: 97.2% | epsilon: 0.0100\n",
            "episode 43000/50000 | avg reward: 47.48 | success rate: 97.5% | epsilon: 0.0100\n",
            "episode 44000/50000 | avg reward: 44.31 | success rate: 97.3% | epsilon: 0.0100\n",
            "episode 45000/50000 | avg reward: 46.69 | success rate: 97.3% | epsilon: 0.0100\n",
            "episode 46000/50000 | avg reward: 52.99 | success rate: 97.8% | epsilon: 0.0100\n",
            "episode 47000/50000 | avg reward: 46.73 | success rate: 97.8% | epsilon: 0.0100\n",
            "episode 48000/50000 | avg reward: 50.45 | success rate: 98.5% | epsilon: 0.0100\n",
            "episode 49000/50000 | avg reward: 51.37 | success rate: 98.3% | epsilon: 0.0100\n",
            "episode 50000/50000 | avg reward: 49.70 | success rate: 97.4% | epsilon: 0.0100\n",
            "======================================================================\n",
            "Training completed!\n",
            "\n",
            "model saved to Model.npy\n",
            "\n",
            "test result (seed 420000-420999):\n",
            "======================================================================\n",
            "   avg reward: 41.18\n",
            "   avg steps: 63.75\n",
            "   success rate: 89.4%\n",
            "   evaluation score: 0.7238 (72.38%)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_reward': np.float64(41.1815),\n",
              " 'avg_steps': np.float64(63.747),\n",
              " 'success_rate': 0.894,\n",
              " 'evaluation_score': np.float64(0.7238120000000001)}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "test_q_learning(\n",
        "    q_agent,\n",
        "    n_episodes=1000,\n",
        "    seed_start=30000,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmAqJxNFCkbB",
        "outputId": "2cbf76ec-2acc-4176-d55e-8db594c561bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test result (seed 30000-30999):\n",
            "======================================================================\n",
            "   avg reward: 41.37\n",
            "   avg steps: 62.52\n",
            "   success rate: 89.2%\n",
            "   evaluation score: 0.7283 (72.83%)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_reward': np.float64(41.374),\n",
              " 'avg_steps': np.float64(62.523),\n",
              " 'success_rate': 0.892,\n",
              " 'evaluation_score': np.float64(0.728308)}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rz-Yrnu4BlOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c05650a5"
      },
      "source": [
        "# Task: Grid Search Hyperparameter Tuning (Optional)\n",
        "Create grid search to do hyperparameter tuning using q learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce40b15"
      },
      "source": [
        "## Define the hyperparameter grid\n",
        "\n",
        "### Subtask:\n",
        "Specify the ranges or lists of values for the hyperparameters you want to tune (e.g., learning rate, discount factor, epsilon decay rate, epsilon min).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54f31b1c"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a dictionary to store the hyperparameter grid with specified ranges for learning rate, discount factor, epsilon decay rate, and epsilon min.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4946dba1"
      },
      "source": [
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'discount_factor': [0.9, 0.95, 0.99],\n",
        "    'epsilon_decay_rate': [0.999, 0.9999, 0.99999],\n",
        "    'epsilon_min': [0.01, 0.05]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ef066c3"
      },
      "source": [
        "## Create a training and evaluation loop\n",
        "\n",
        "### Subtask:\n",
        "Write a loop that iterates through all possible combinations of hyperparameters defined in the grid.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aacdd16"
      },
      "source": [
        "**Reasoning**:\n",
        "Get all possible hyperparameter combinations and initialize a list to store results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bd082cc",
        "outputId": "22ae172d-85cd-44a3-95e8-3765be4e22fb"
      },
      "source": [
        "import itertools\n",
        "\n",
        "keys, values = zip(*param_grid.items())\n",
        "hyperparameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "results = []\n",
        "\n",
        "for params in hyperparameter_combinations:\n",
        "    print(f\"Training with params: {params}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f63a5837"
      },
      "source": [
        "## Train and evaluate the agent for each combination\n",
        "\n",
        "### Subtask:\n",
        "Inside the loop, train a new Q-learning agent with the current hyperparameter combination using the `train_q_learning` function and evaluate its performance using the `test_q_learning` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39fc5d2f"
      },
      "source": [
        "**Reasoning**:\n",
        "Train and test the Q-learning agent with the current hyperparameter combination and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73912ee8"
      },
      "source": [
        "**Reasoning**:\n",
        "The `train_q_learning` function does not accept the hyperparameters as keyword arguments. I need to modify the function to accept the hyperparameters and pass them to the `QLearningAgent` constructor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3BtxeADZjv",
        "outputId": "afbf81c0-dcf1-4b58-a6e6-a21d10077891"
      },
      "source": [
        "def train_q_learning(n_episodes=50000, max_steps=200,\n",
        "                     seed_start=0, seed_end=40000, verbose=True,\n",
        "                     learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay_rate=0.999, epsilon_min=0.01): # Added hyperparameter arguments\n",
        "    \"\"\"train Q-learning Agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(TaxiEnv())\n",
        "\n",
        "    agent = QLearningAgent(\n",
        "        n_states=env.observation_space.n,\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=learning_rate,      # Use passed argument\n",
        "        discount_factor=discount_factor,     # Use passed argument\n",
        "        epsilon=epsilon,          # Use passed argument\n",
        "        epsilon_decay_rate=epsilon_decay_rate, # Use passed argument\n",
        "        epsilon_min=epsilon_min        # Use passed argument\n",
        "    )\n",
        "\n",
        "    episode_rewards = []\n",
        "    success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"training episodes: {n_episodes}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
        "\n",
        "        episode_history = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state, env)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.update(state, action, reward, next_state)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if terminated and info.get(\"success\", False): # Check for success from the wrapper info\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            success_rate = success_count / min(1000, episode + 1)\n",
        "\n",
        "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
        "                  f\"avg reward: {avg_reward:.2f} | \"\n",
        "                  f\"success rate: {success_rate:.1%} | \"\n",
        "                  f\"epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "            if episode >= 999: # Reset success count every 1000 episodes\n",
        "                success_count = 0\n",
        "\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards\n",
        "\n",
        "import itertools\n",
        "\n",
        "keys, values = zip(*param_grid.items())\n",
        "hyperparameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "results = []\n",
        "\n",
        "for params in hyperparameter_combinations:\n",
        "    print(f\"Training with params: {params}\")\n",
        "    # Train the agent with current parameters\n",
        "    q_agent, q_rewards = train_q_learning(\n",
        "        n_episodes=10000,  # Reduced episodes for faster grid search\n",
        "        verbose=False, # Suppress verbose output during grid search\n",
        "        **params # Pass hyperparameters\n",
        "    )\n",
        "\n",
        "    # Test the trained agent\n",
        "    test_results = test_q_learning(\n",
        "        q_agent,\n",
        "        n_episodes=100, # Reduced episodes for faster grid search\n",
        "        seed_start=420000,\n",
        "        verbose=False # Suppress verbose output during grid search\n",
        "    )\n",
        "\n",
        "    # Store the results\n",
        "    results.append({\n",
        "        'params': params,\n",
        "        'test_results': test_results\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Training with params: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e2a453c"
      },
      "source": [
        "## Store and compare results\n",
        "\n",
        "### Subtask:\n",
        "Keep track of the performance metrics (e.g., average reward, success rate, evaluation score) for each hyperparameter combination.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec6a0fd6"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the results and print the hyperparameters and performance metrics for each combination.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4877601",
        "outputId": "1fc53fc9-368e-45ac-f57a-e0a9d71e703c"
      },
      "source": [
        "for result in results:\n",
        "    print(\"Hyperparameters:\", result['params'])\n",
        "    print(\"Test Results:\")\n",
        "    print(f\"  Avg Reward: {result['test_results']['avg_reward']:.2f}\")\n",
        "    print(f\"  Avg Steps: {result['test_results']['avg_steps']:.2f}\")\n",
        "    print(f\"  Success Rate: {result['test_results']['success_rate']:.1%}\")\n",
        "    print(f\"  Evaluation Score: {result['test_results']['evaluation_score']:.4f}\")\n",
        "    print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -37.46\n",
            "  Avg Steps: 190.65\n",
            "  Success Rate: 5.0%\n",
            "  Evaluation Score: 0.0474\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -33.70\n",
            "  Avg Steps: 168.17\n",
            "  Success Rate: 17.0%\n",
            "  Evaluation Score: 0.1613\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -61.54\n",
            "  Avg Steps: 154.16\n",
            "  Success Rate: 25.0%\n",
            "  Evaluation Score: 0.2334\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -62.80\n",
            "  Avg Steps: 155.34\n",
            "  Success Rate: 24.0%\n",
            "  Evaluation Score: 0.2266\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -57.21\n",
            "  Avg Steps: 177.39\n",
            "  Success Rate: 12.0%\n",
            "  Evaluation Score: 0.1144\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -55.30\n",
            "  Avg Steps: 173.64\n",
            "  Success Rate: 14.0%\n",
            "  Evaluation Score: 0.1334\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -42.73\n",
            "  Avg Steps: 188.74\n",
            "  Success Rate: 6.0%\n",
            "  Evaluation Score: 0.0570\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -34.34\n",
            "  Avg Steps: 170.11\n",
            "  Success Rate: 16.0%\n",
            "  Evaluation Score: 0.1516\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -48.54\n",
            "  Avg Steps: 137.47\n",
            "  Success Rate: 34.0%\n",
            "  Evaluation Score: 0.3181\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -54.45\n",
            "  Avg Steps: 147.10\n",
            "  Success Rate: 30.0%\n",
            "  Evaluation Score: 0.2716\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -56.11\n",
            "  Avg Steps: 169.93\n",
            "  Success Rate: 16.0%\n",
            "  Evaluation Score: 0.1523\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -57.83\n",
            "  Avg Steps: 167.91\n",
            "  Success Rate: 17.0%\n",
            "  Evaluation Score: 0.1624\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -31.59\n",
            "  Avg Steps: 177.53\n",
            "  Success Rate: 12.0%\n",
            "  Evaluation Score: 0.1139\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -28.99\n",
            "  Avg Steps: 168.29\n",
            "  Success Rate: 17.0%\n",
            "  Evaluation Score: 0.1608\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -53.99\n",
            "  Avg Steps: 144.33\n",
            "  Success Rate: 30.0%\n",
            "  Evaluation Score: 0.2827\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -42.95\n",
            "  Avg Steps: 149.94\n",
            "  Success Rate: 27.0%\n",
            "  Evaluation Score: 0.2542\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -65.90\n",
            "  Avg Steps: 173.54\n",
            "  Success Rate: 14.0%\n",
            "  Evaluation Score: 0.1338\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.01, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -59.60\n",
            "  Avg Steps: 164.18\n",
            "  Success Rate: 19.0%\n",
            "  Evaluation Score: 0.1813\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -1.05\n",
            "  Avg Steps: 131.26\n",
            "  Success Rate: 39.0%\n",
            "  Evaluation Score: 0.3530\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -4.39\n",
            "  Avg Steps: 122.07\n",
            "  Success Rate: 45.0%\n",
            "  Evaluation Score: 0.4017\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -47.23\n",
            "  Avg Steps: 136.20\n",
            "  Success Rate: 35.0%\n",
            "  Evaluation Score: 0.3252\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -44.05\n",
            "  Avg Steps: 139.27\n",
            "  Success Rate: 33.0%\n",
            "  Evaluation Score: 0.3089\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -65.41\n",
            "  Avg Steps: 162.62\n",
            "  Success Rate: 20.0%\n",
            "  Evaluation Score: 0.1895\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -58.27\n",
            "  Avg Steps: 149.64\n",
            "  Success Rate: 27.0%\n",
            "  Evaluation Score: 0.2554\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: 0.23\n",
            "  Avg Steps: 99.63\n",
            "  Success Rate: 61.0%\n",
            "  Evaluation Score: 0.5235\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -9.55\n",
            "  Avg Steps: 91.53\n",
            "  Success Rate: 63.0%\n",
            "  Evaluation Score: 0.5599\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -54.92\n",
            "  Avg Steps: 156.02\n",
            "  Success Rate: 24.0%\n",
            "  Evaluation Score: 0.2239\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -62.01\n",
            "  Avg Steps: 153.78\n",
            "  Success Rate: 25.0%\n",
            "  Evaluation Score: 0.2349\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -57.23\n",
            "  Avg Steps: 149.65\n",
            "  Success Rate: 27.0%\n",
            "  Evaluation Score: 0.2554\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -58.77\n",
            "  Avg Steps: 155.47\n",
            "  Success Rate: 24.0%\n",
            "  Evaluation Score: 0.2261\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: 15.18\n",
            "  Avg Steps: 66.95\n",
            "  Success Rate: 86.0%\n",
            "  Evaluation Score: 0.7042\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: 8.12\n",
            "  Avg Steps: 107.19\n",
            "  Success Rate: 55.0%\n",
            "  Evaluation Score: 0.4812\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -49.99\n",
            "  Avg Steps: 141.42\n",
            "  Success Rate: 32.0%\n",
            "  Evaluation Score: 0.2983\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -56.08\n",
            "  Avg Steps: 147.06\n",
            "  Success Rate: 29.0%\n",
            "  Evaluation Score: 0.2698\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -55.54\n",
            "  Avg Steps: 166.54\n",
            "  Success Rate: 18.0%\n",
            "  Evaluation Score: 0.1698\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -57.59\n",
            "  Avg Steps: 166.32\n",
            "  Success Rate: 18.0%\n",
            "  Evaluation Score: 0.1707\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -3.65\n",
            "  Avg Steps: 131.54\n",
            "  Success Rate: 39.0%\n",
            "  Evaluation Score: 0.3518\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -9.70\n",
            "  Avg Steps: 141.96\n",
            "  Success Rate: 33.0%\n",
            "  Evaluation Score: 0.2982\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -36.27\n",
            "  Avg Steps: 133.90\n",
            "  Success Rate: 36.0%\n",
            "  Evaluation Score: 0.3364\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -37.73\n",
            "  Avg Steps: 137.97\n",
            "  Success Rate: 34.0%\n",
            "  Evaluation Score: 0.3161\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -66.79\n",
            "  Avg Steps: 171.86\n",
            "  Success Rate: 15.0%\n",
            "  Evaluation Score: 0.1426\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.9, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -52.77\n",
            "  Avg Steps: 155.48\n",
            "  Success Rate: 24.0%\n",
            "  Evaluation Score: 0.2261\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -5.21\n",
            "  Avg Steps: 103.78\n",
            "  Success Rate: 58.0%\n",
            "  Evaluation Score: 0.5009\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -15.65\n",
            "  Avg Steps: 108.44\n",
            "  Success Rate: 54.0%\n",
            "  Evaluation Score: 0.4742\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -37.94\n",
            "  Avg Steps: 146.83\n",
            "  Success Rate: 29.0%\n",
            "  Evaluation Score: 0.2707\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -55.52\n",
            "  Avg Steps: 154.44\n",
            "  Success Rate: 27.0%\n",
            "  Evaluation Score: 0.2362\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -67.09\n",
            "  Avg Steps: 173.88\n",
            "  Success Rate: 14.0%\n",
            "  Evaluation Score: 0.1325\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.95, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -59.76\n",
            "  Avg Steps: 166.56\n",
            "  Success Rate: 18.0%\n",
            "  Evaluation Score: 0.1698\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: 8.09\n",
            "  Avg Steps: 75.30\n",
            "  Success Rate: 79.0%\n",
            "  Evaluation Score: 0.6568\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -15.71\n",
            "  Avg Steps: 113.02\n",
            "  Success Rate: 53.0%\n",
            "  Evaluation Score: 0.4539\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -54.99\n",
            "  Avg Steps: 153.97\n",
            "  Success Rate: 25.0%\n",
            "  Evaluation Score: 0.2341\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.9999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -52.49\n",
            "  Avg Steps: 145.98\n",
            "  Success Rate: 31.0%\n",
            "  Evaluation Score: 0.2781\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.01}\n",
            "Test Results:\n",
            "  Avg Reward: -52.98\n",
            "  Avg Steps: 169.95\n",
            "  Success Rate: 16.0%\n",
            "  Evaluation Score: 0.1522\n",
            "------------------------------\n",
            "Hyperparameters: {'learning_rate': 0.2, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.99999, 'epsilon_min': 0.05}\n",
            "Test Results:\n",
            "  Avg Reward: -66.25\n",
            "  Avg Steps: 166.34\n",
            "  Success Rate: 18.0%\n",
            "  Evaluation Score: 0.1706\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2oYpJiBLnTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fb10c1a"
      },
      "source": [
        "## Identify the best hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Identify the best hyperparameters by finding the combination that resulted in the highest evaluation score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac966a42"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the best hyperparameters by iterating through the results and comparing evaluation scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98de5389",
        "outputId": "81fd7bc7-301d-4443-fe75-d7ed94551253"
      },
      "source": [
        "best_evaluation_score = -1.0\n",
        "best_params = None\n",
        "\n",
        "for result in results:\n",
        "    current_evaluation_score = result['test_results']['evaluation_score']\n",
        "    if current_evaluation_score > best_evaluation_score:\n",
        "        best_evaluation_score = current_evaluation_score\n",
        "        best_params = result['params']\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found:\")\n",
        "print(best_params)\n",
        "print(f\"Corresponding Best Evaluation Score: {best_evaluation_score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters Found:\n",
            "{'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Corresponding Best Evaluation Score: 0.7042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03372cce"
      },
      "source": [
        "## Report the best hyperparameters and results\n",
        "\n",
        "### Subtask:\n",
        "Report the best hyperparameters found and the corresponding evaluation results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aa42c16"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the best hyperparameters and the corresponding evaluation results as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7448f96f",
        "outputId": "3f69cffd-bffd-4a89-eaf8-7c9793eba460"
      },
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"Best Hyperparameters and Results\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Best Evaluation Score: {best_evaluation_score:.4f}\")\n",
        "\n",
        "# Optional: Print the full test results for the best hyperparameters\n",
        "# Find the full result dictionary for the best parameters\n",
        "best_test_results = None\n",
        "for result in results:\n",
        "    if result['params'] == best_params:\n",
        "        best_test_results = result['test_results']\n",
        "        break\n",
        "\n",
        "if best_test_results:\n",
        "    print(\"\\nFull Test Results for Best Hyperparameters:\")\n",
        "    print(f\"  Avg Reward: {best_test_results['avg_reward']:.2f}\")\n",
        "    print(f\"  Avg Steps: {best_test_results['avg_steps']:.2f}\")\n",
        "    print(f\"  Success Rate: {best_test_results['success_rate']:.1%}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Best Hyperparameters and Results\n",
            "======================================================================\n",
            "Best Hyperparameters: {'learning_rate': 0.1, 'discount_factor': 0.99, 'epsilon_decay_rate': 0.999, 'epsilon_min': 0.01}\n",
            "Best Evaluation Score: 0.7042\n",
            "\n",
            "Full Test Results for Best Hyperparameters:\n",
            "  Avg Reward: 15.18\n",
            "  Avg Steps: 66.95\n",
            "  Success Rate: 86.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dDmFzV1MDB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "994d3e40"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The best hyperparameters identified through the grid search were a `learning_rate` of 0.1, `discount_factor` of 0.99, `epsilon_decay_rate` of 0.999, and `epsilon_min` of 0.01.\n",
        "*   This hyperparameter combination achieved the highest evaluation score of 0.7042.\n",
        "*   The performance metrics for the best hyperparameters included an average reward of 15.18, an average of 66.95 steps per episode, and a success rate of 86.0%.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The identified best hyperparameters provide a solid starting point for training a high-performing Q-learning agent for this specific task.\n",
        "*   Further tuning could be explored by refining the ranges around the best hyperparameters or by using more advanced tuning methods like random search or Bayesian optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxqLOnkezJU7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}