{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sbro3A_Qha1"
      },
      "source": [
        "# Taxi Env (Don't Modify it )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a1HlV_-2QUNN"
      },
      "outputs": [],
      "source": [
        "from contextlib import closing\n",
        "from io import StringIO\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import Env, spaces, utils\n",
        "from gymnasium.envs.toy_text.utils import categorical_sample\n",
        "from gymnasium.error import DependencyNotInstalled\n",
        "\n",
        "MAP_0 = [\n",
        "    \"+---------+\",\n",
        "    \"|R: | : :G|\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"| | : | : |\",\n",
        "    \"|Y| : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP_1 = [\n",
        "    \"+---------+\",\n",
        "    \"|R| : : :G|\",\n",
        "    \"| | : | : |\",\n",
        "    \"| : : | : |\",\n",
        "    \"| : | : : |\",\n",
        "    \"|Y: | :B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP_2 = [\n",
        "    \"+---------+\",\n",
        "    \"|R: : : |G|\",\n",
        "    \"| : : : | |\",\n",
        "    \"| : | : : |\",\n",
        "    \"| : | | : |\",\n",
        "    \"|Y: : |B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "MAP = MAP_0\n",
        "MAPS = [MAP_0, MAP_1, MAP_2]\n",
        "WINDOW_SIZE = (550, 350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tBkD72IKQxKT"
      },
      "outputs": [],
      "source": [
        "class TaxiEnv(Env):\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"ansi\", \"rgb_array\"],\n",
        "        \"render_fps\": 4,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: str | None = None,\n",
        "        is_rainy: bool = True,\n",
        "        fickle_passenger: bool = False,\n",
        "        use_multiple_maps: bool = True,\n",
        "        reward_step: float = -1,\n",
        "        reward_delivery: float = 20,\n",
        "        reward_illegal: float = -10,\n",
        "    ):\n",
        "        self.use_multiple_maps = use_multiple_maps\n",
        "\n",
        "        if use_multiple_maps:\n",
        "            self.desc_maps = [np.asarray(m, dtype=\"c\") for m in MAPS]\n",
        "            self.P_maps = []\n",
        "            self.current_map_id = 0\n",
        "            self.desc = self.desc_maps[0]\n",
        "        else:\n",
        "            self.desc = np.asarray(MAP, dtype=\"c\")\n",
        "\n",
        "        self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
        "        self.locs_colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255)]\n",
        "\n",
        "        self.reward_step = reward_step\n",
        "        self.reward_delivery = reward_delivery\n",
        "        self.reward_illegal = reward_illegal\n",
        "\n",
        "        num_states = 500\n",
        "        num_rows = 5\n",
        "        num_columns = 5\n",
        "        self.max_row = num_rows - 1\n",
        "        self.max_col = num_columns - 1\n",
        "        self.initial_state_distrib = np.zeros(num_states)\n",
        "        num_actions = 6\n",
        "\n",
        "\n",
        "        if use_multiple_maps:\n",
        "\n",
        "            for map_idx in range(3):\n",
        "                self.desc = self.desc_maps[map_idx]\n",
        "                P = {\n",
        "                    state: {action: [] for action in range(num_actions)}\n",
        "                    for state in range(num_states)\n",
        "                }\n",
        "                self.P = P\n",
        "\n",
        "                # Iterate through all possible state combinations\n",
        "                for row in range(num_rows):\n",
        "                    for col in range(num_columns):\n",
        "                        for pass_idx in range(len(locs) + 1):\n",
        "                            for dest_idx in range(len(locs)):\n",
        "                                state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                                if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                                    self.initial_state_distrib[state] += 1\n",
        "                                for action in range(num_actions):\n",
        "                                    if is_rainy:\n",
        "                                        self._build_rainy_transitions(\n",
        "                                            row, col, pass_idx, dest_idx, action,\n",
        "                                        )\n",
        "                                    else:\n",
        "                                        self._build_dry_transitions(\n",
        "                                            row, col, pass_idx, dest_idx, action,\n",
        "                                        )\n",
        "\n",
        "                self.P_maps.append(P)\n",
        "\n",
        "\n",
        "            self.desc = self.desc_maps[0]\n",
        "            self.P = self.P_maps[0]\n",
        "        else:\n",
        "\n",
        "            self.P = {\n",
        "                state: {action: [] for action in range(num_actions)}\n",
        "                for state in range(num_states)\n",
        "            }\n",
        "\n",
        "            for row in range(num_rows):\n",
        "                for col in range(num_columns):\n",
        "                    for pass_idx in range(len(locs) + 1):\n",
        "                        for dest_idx in range(len(locs)):\n",
        "                            state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                            if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                                self.initial_state_distrib[state] += 1\n",
        "                            for action in range(num_actions):\n",
        "                                if is_rainy:\n",
        "                                    self._build_rainy_transitions(\n",
        "                                        row, col, pass_idx, dest_idx, action,\n",
        "                                    )\n",
        "                                else:\n",
        "                                    self._build_dry_transitions(\n",
        "                                        row, col, pass_idx, dest_idx, action,\n",
        "                                    )\n",
        "\n",
        "        self.initial_state_distrib /= self.initial_state_distrib.sum()\n",
        "        self.action_space = spaces.Discrete(num_actions)\n",
        "        self.observation_space = spaces.Discrete(num_states)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.fickle_passenger = fickle_passenger\n",
        "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.5\n",
        "\n",
        "        # pygame utils\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.cell_size = (\n",
        "            WINDOW_SIZE[0] / self.desc.shape[1],\n",
        "            WINDOW_SIZE[1] / self.desc.shape[0],\n",
        "        )\n",
        "        self.taxi_imgs = None\n",
        "        self.taxi_orientation = 0\n",
        "        self.passenger_img = None\n",
        "        self.destination_img = None\n",
        "        self.median_horiz = None\n",
        "        self.median_vert = None\n",
        "        self.background_img = None\n",
        "\n",
        "    def _pickup(self, taxi_loc, pass_idx, reward):\n",
        "        \"\"\"Computes the new location and reward for pickup action.\"\"\"\n",
        "        if pass_idx < 4 and taxi_loc == self.locs[pass_idx]:\n",
        "            new_pass_idx = 4\n",
        "            new_reward = reward\n",
        "        else:  # passenger not at location\n",
        "            new_pass_idx = pass_idx\n",
        "            new_reward = self.reward_illegal\n",
        "\n",
        "        return new_pass_idx, new_reward\n",
        "\n",
        "    def _dropoff(self, taxi_loc, pass_idx, dest_idx, default_reward):\n",
        "        \"\"\"Computes the new location and reward for return dropoff action.\"\"\"\n",
        "        if (taxi_loc == self.locs[dest_idx]) and pass_idx == 4:\n",
        "            new_pass_idx = dest_idx\n",
        "            new_terminated = True\n",
        "            new_reward = self.reward_delivery\n",
        "        elif (taxi_loc in self.locs) and pass_idx == 4:\n",
        "            new_pass_idx = self.locs.index(taxi_loc)\n",
        "            new_terminated = False\n",
        "            new_reward = default_reward\n",
        "        else:  # dropoff at wrong location\n",
        "            new_pass_idx = pass_idx\n",
        "            new_terminated = False\n",
        "            new_reward = self.reward_illegal\n",
        "\n",
        "        return new_pass_idx, new_reward, new_terminated\n",
        "\n",
        "    def _calc_new_position(self, row, col, movement, offset=0):\n",
        "        \"\"\"Calculates the new position for a row and col to the movement.\"\"\"\n",
        "        dr, dc = movement\n",
        "        new_row = max(0, min(row + dr, self.max_row))\n",
        "        new_col = max(0, min(col + dc, self.max_col))\n",
        "        if self.desc[1 + new_row, 2 * new_col + offset] == b\":\":\n",
        "            return new_row, new_col\n",
        "        else:  # Default to current position if not traversable\n",
        "            return row, col\n",
        "\n",
        "    def _build_rainy_transitions(self, row, col, pass_idx, dest_idx, action):\n",
        "        \"\"\"Computes the next action for a state (row, col, pass_idx, dest_idx) and action for `is_rainy`.\"\"\"\n",
        "        state = self.encode(row, col, pass_idx, dest_idx)\n",
        "\n",
        "        taxi_loc = left_pos = right_pos = (row, col)\n",
        "        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
        "        reward = self.reward_step\n",
        "        terminated = False\n",
        "\n",
        "        moves = {\n",
        "            0: ((1, 0), (0, -1), (0, 1)),  # Down\n",
        "            1: ((-1, 0), (0, -1), (0, 1)),  # Up\n",
        "            2: ((0, 1), (1, 0), (-1, 0)),  # Right\n",
        "            3: ((0, -1), (1, 0), (-1, 0)),  # Left\n",
        "        }\n",
        "\n",
        "        # Check if movement is allowed\n",
        "        if (\n",
        "            action in {0, 1}\n",
        "            or (action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\")\n",
        "            or (action == 3 and self.desc[1 + row, 2 * col] == b\":\")\n",
        "        ):\n",
        "            dr, dc = moves[action][0]\n",
        "            new_row = max(0, min(row + dr, self.max_row))\n",
        "            new_col = max(0, min(col + dc, self.max_col))\n",
        "\n",
        "            left_pos = self._calc_new_position(row, col, moves[action][1], offset=2)\n",
        "            right_pos = self._calc_new_position(row, col, moves[action][2])\n",
        "        elif action == 4:  # pickup\n",
        "            new_pass_idx, reward = self._pickup(taxi_loc, new_pass_idx, reward)\n",
        "        elif action == 5:  # dropoff\n",
        "            new_pass_idx, reward, terminated = self._dropoff(\n",
        "                taxi_loc, new_pass_idx, dest_idx, reward\n",
        "            )\n",
        "        intended_state = self.encode(new_row, new_col, new_pass_idx, dest_idx)\n",
        "\n",
        "        if action <= 3:\n",
        "            left_state = self.encode(left_pos[0], left_pos[1], new_pass_idx, dest_idx)\n",
        "            right_state = self.encode(\n",
        "                right_pos[0], right_pos[1], new_pass_idx, dest_idx\n",
        "            )\n",
        "\n",
        "            self.P[state][action].append((0.8, intended_state, self.reward_step, terminated))\n",
        "            self.P[state][action].append((0.1, left_state, self.reward_step, terminated))\n",
        "            self.P[state][action].append((0.1, right_state, self.reward_step, terminated))\n",
        "        else:\n",
        "            self.P[state][action].append((1.0, intended_state, reward, terminated))\n",
        "\n",
        "\n",
        "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
        "        # (5) 5, 5, 4\n",
        "        i = taxi_row\n",
        "        i *= 5\n",
        "        i += taxi_col\n",
        "        i *= 5\n",
        "        i += pass_loc\n",
        "        i *= 4\n",
        "        i += dest_idx\n",
        "        return i\n",
        "\n",
        "    def decode(self, i):\n",
        "        out = []\n",
        "        out.append(i % 4)\n",
        "        i = i // 4\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i)\n",
        "        assert 0 <= i < 5\n",
        "        return reversed(out)\n",
        "\n",
        "    def action_mask(self, state: int):\n",
        "        \"\"\"Computes an action mask for the action space using the state information.\"\"\"\n",
        "        mask = np.zeros(6, dtype=np.int8)\n",
        "        taxi_row, taxi_col, pass_loc, dest_idx = self.decode(state)\n",
        "        if taxi_row < 4:\n",
        "            mask[0] = 1\n",
        "        if taxi_row > 0:\n",
        "            mask[1] = 1\n",
        "        if taxi_col < 4 and self.desc[taxi_row + 1, 2 * taxi_col + 2] == b\":\":\n",
        "            mask[2] = 1\n",
        "        if taxi_col > 0 and self.desc[taxi_row + 1, 2 * taxi_col] == b\":\":\n",
        "            mask[3] = 1\n",
        "        if pass_loc < 4 and (taxi_row, taxi_col) == self.locs[pass_loc]:\n",
        "            mask[4] = 1\n",
        "        if pass_loc == 4 and (\n",
        "            (taxi_row, taxi_col) == self.locs[dest_idx]\n",
        "            or (taxi_row, taxi_col) in self.locs\n",
        "        ):\n",
        "            mask[5] = 1\n",
        "        return mask\n",
        "\n",
        "    def step(self, a):\n",
        "        transitions = self.P[self.s][a]\n",
        "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
        "        p, s, r, t = transitions[i]\n",
        "        self.lastaction = a\n",
        "\n",
        "        shadow_row, shadow_col, shadow_pass_loc, shadow_dest_idx = self.decode(self.s)\n",
        "        taxi_row, taxi_col, pass_loc, _ = self.decode(s)\n",
        "\n",
        "        # If we are in the fickle step, the passenger has been in the vehicle for at least a step and this step the\n",
        "        # position changed\n",
        "        if (\n",
        "            self.fickle_passenger\n",
        "            and self.fickle_step\n",
        "            and shadow_pass_loc == 4\n",
        "            and (taxi_row != shadow_row or taxi_col != shadow_col)\n",
        "        ):\n",
        "            self.fickle_step = False\n",
        "            possible_destinations = [\n",
        "                i for i in range(len(self.locs)) if i != shadow_dest_idx\n",
        "            ]\n",
        "            dest_idx = self.np_random.choice(possible_destinations)\n",
        "            s = self.encode(taxi_row, taxi_col, pass_loc, dest_idx)\n",
        "\n",
        "        self.s = s\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
        "        return int(s), r, t, False, {\"prob\": p, \"action_mask\": self.action_mask(s)}\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: int | None = None,\n",
        "        options: dict | None = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # # TODO remove debug\n",
        "        # print(\"Use multiple maps: \", self.use_multiple_maps)\n",
        "\n",
        "        if self.use_multiple_maps and seed is not None:\n",
        "            map_id = seed % 3\n",
        "            self.current_map_id = map_id\n",
        "            self.desc = self.desc_maps[map_id]\n",
        "            self.P = self.P_maps[map_id]\n",
        "\n",
        "        # # TODO remove debug\n",
        "        # print(\"Using maps: \", self.current_map_id)\n",
        "\n",
        "        self.s = categorical_sample(self.initial_state_distrib, self.np_random)\n",
        "        self.lastaction = None\n",
        "        self.fickle_step = self.fickle_passenger and self.np_random.random() < 0.3\n",
        "        self.taxi_orientation = 0\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return int(self.s), {\"prob\": 1.0, \"action_mask\": self.action_mask(self.s)}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            assert self.spec is not None\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "        elif self.render_mode == \"ansi\":\n",
        "            return self._render_text()\n",
        "        else:  # self.render_mode in {\"human\", \"rgb_array\"}:\n",
        "            return self._render_gui(self.render_mode)\n",
        "\n",
        "    def _render_gui(self, mode):\n",
        "        try:\n",
        "            import pygame  # dependency to pygame only if rendering with human\n",
        "        except ImportError as e:\n",
        "            raise DependencyNotInstalled(\n",
        "                'pygame is not installed, run `pip install \"gymnasium[toy-text]\"`'\n",
        "            ) from e\n",
        "\n",
        "        if self.window is None:\n",
        "            pygame.init()\n",
        "            pygame.display.set_caption(\"Taxi\")\n",
        "            if mode == \"human\":\n",
        "                self.window = pygame.display.set_mode(WINDOW_SIZE)\n",
        "            elif mode == \"rgb_array\":\n",
        "                self.window = pygame.Surface(WINDOW_SIZE)\n",
        "\n",
        "        assert (\n",
        "            self.window is not None\n",
        "        ), \"Something went wrong with pygame. This should never happen.\"\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "        if self.taxi_imgs is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/cab_front.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_rear.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_right.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/cab_left.png\"),\n",
        "            ]\n",
        "            self.taxi_imgs = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.passenger_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/passenger.png\")\n",
        "            self.passenger_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "        if self.destination_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/hotel.png\")\n",
        "            self.destination_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "            self.destination_img.set_alpha(170)\n",
        "        if self.median_horiz is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_left.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_horiz.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_right.png\"),\n",
        "            ]\n",
        "            self.median_horiz = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.median_vert is None:\n",
        "            file_names = [\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_top.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_vert.png\"),\n",
        "                path.join(path.dirname(__file__), \"img/gridworld_median_bottom.png\"),\n",
        "            ]\n",
        "            self.median_vert = [\n",
        "                pygame.transform.scale(pygame.image.load(file_name), self.cell_size)\n",
        "                for file_name in file_names\n",
        "            ]\n",
        "        if self.background_img is None:\n",
        "            file_name = path.join(path.dirname(__file__), \"img/taxi_background.png\")\n",
        "            self.background_img = pygame.transform.scale(\n",
        "                pygame.image.load(file_name), self.cell_size\n",
        "            )\n",
        "\n",
        "        desc = self.desc\n",
        "\n",
        "        for y in range(0, desc.shape[0]):\n",
        "            for x in range(0, desc.shape[1]):\n",
        "                cell = (x * self.cell_size[0], y * self.cell_size[1])\n",
        "                self.window.blit(self.background_img, cell)\n",
        "                if desc[y][x] == b\"|\" and (y == 0 or desc[y - 1][x] != b\"|\"):\n",
        "                    self.window.blit(self.median_vert[0], cell)\n",
        "                elif desc[y][x] == b\"|\" and (\n",
        "                    y == desc.shape[0] - 1 or desc[y + 1][x] != b\"|\"\n",
        "                ):\n",
        "                    self.window.blit(self.median_vert[2], cell)\n",
        "                elif desc[y][x] == b\"|\":\n",
        "                    self.window.blit(self.median_vert[1], cell)\n",
        "                elif desc[y][x] == b\"-\" and (x == 0 or desc[y][x - 1] != b\"-\"):\n",
        "                    self.window.blit(self.median_horiz[0], cell)\n",
        "                elif desc[y][x] == b\"-\" and (\n",
        "                    x == desc.shape[1] - 1 or desc[y][x + 1] != b\"-\"\n",
        "                ):\n",
        "                    self.window.blit(self.median_horiz[2], cell)\n",
        "                elif desc[y][x] == b\"-\":\n",
        "                    self.window.blit(self.median_horiz[1], cell)\n",
        "\n",
        "        for cell, color in zip(self.locs, self.locs_colors):\n",
        "            color_cell = pygame.Surface(self.cell_size)\n",
        "            color_cell.set_alpha(128)\n",
        "            color_cell.fill(color)\n",
        "            loc = self.get_surf_loc(cell)\n",
        "            self.window.blit(color_cell, (loc[0], loc[1] + 10))\n",
        "\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            self.window.blit(self.passenger_img, self.get_surf_loc(self.locs[pass_idx]))\n",
        "\n",
        "        if self.lastaction in [0, 1, 2, 3]:\n",
        "            self.taxi_orientation = self.lastaction\n",
        "        dest_loc = self.get_surf_loc(self.locs[dest_idx])\n",
        "        taxi_location = self.get_surf_loc((taxi_row, taxi_col))\n",
        "\n",
        "        if dest_loc[1] <= taxi_location[1]:\n",
        "            self.window.blit(\n",
        "                self.destination_img,\n",
        "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
        "            )\n",
        "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
        "        else:  # change blit order for overlapping appearance\n",
        "            self.window.blit(self.taxi_imgs[self.taxi_orientation], taxi_location)\n",
        "            self.window.blit(\n",
        "                self.destination_img,\n",
        "                (dest_loc[0], dest_loc[1] - self.cell_size[1] // 2),\n",
        "            )\n",
        "\n",
        "        if mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        elif mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def get_surf_loc(self, map_loc):\n",
        "        return (map_loc[1] * 2 + 1) * self.cell_size[0], (\n",
        "            map_loc[0] + 1\n",
        "        ) * self.cell_size[1]\n",
        "\n",
        "    def _render_text(self):\n",
        "        desc = self.desc.copy().tolist()\n",
        "        outfile = StringIO()\n",
        "\n",
        "        out = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                out[1 + taxi_row][2 * taxi_col + 1], \"yellow\", highlight=True\n",
        "            )\n",
        "            pi, pj = self.locs[pass_idx]\n",
        "            out[1 + pi][2 * pj + 1] = utils.colorize(\n",
        "                out[1 + pi][2 * pj + 1], \"blue\", bold=True\n",
        "            )\n",
        "        else:  # passenger in taxi\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                ul(out[1 + taxi_row][2 * taxi_col + 1]), \"green\", highlight=True\n",
        "            )\n",
        "\n",
        "        di, dj = self.locs[dest_idx]\n",
        "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], \"magenta\")\n",
        "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
        "        if self.lastaction is not None:\n",
        "            outfile.write(\n",
        "                f\"  ({['South', 'North', 'East', 'West', 'Pickup', 'Dropoff'][self.lastaction]})\\n\"\n",
        "            )\n",
        "        else:\n",
        "            outfile.write(\"\\n\")\n",
        "\n",
        "        with closing(outfile):\n",
        "            return outfile.getvalue()\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe_oj--bRBoR"
      },
      "source": [
        "# Training Strategy(Policy Gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LJPgdF2ZRNB9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QMsqppIlRWWI"
      },
      "outputs": [],
      "source": [
        "class PolicyGradientAgentOptimized:\n",
        "\n",
        "    def __init__(self, n_states, n_actions,\n",
        "                 learning_rate=0.01,\n",
        "                 value_lr=0.1,\n",
        "                 lr_decay=0.9999,\n",
        "                 lr_min=0.0001,\n",
        "                 discount_factor=0.99,\n",
        "                 entropy_coef=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.lr_init = learning_rate\n",
        "        self.value_lr = value_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr_min = lr_min\n",
        "        self.gamma = discount_factor\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        # policy parameters\n",
        "        self.theta = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # Value function\n",
        "        self.V = np.zeros(n_states)\n",
        "\n",
        "        # statistics information\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        \"\"\"calculate action probability distribution (softmax)\"\"\"\n",
        "        theta_state = self.theta[state] - np.max(self.theta[state])\n",
        "        exp_theta = np.exp(theta_state)\n",
        "        return exp_theta / np.sum(exp_theta)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"sample action according to policy\"\"\"\n",
        "        policy = self.get_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update(self, episode_history):\n",
        "        \"\"\"\n",
        "        update policy using Advantage and Entropy\n",
        "        \"\"\"\n",
        "        if len(episode_history) == 0:\n",
        "            return\n",
        "\n",
        "        # 1. calculate return G_t\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for state, action, reward in reversed(episode_history):\n",
        "            G = reward + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # 2. update Value function\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            td_error = returns[t] - self.V[state]\n",
        "            self.V[state] += self.value_lr * td_error\n",
        "\n",
        "        # 3. calculate Advantage\n",
        "        advantages = []\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            advantage = returns[t] - self.V[state]\n",
        "            advantages.append(advantage)\n",
        "\n",
        "        # 4. standardize Advantage\n",
        "        advantages = np.array(advantages)\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-9)\n",
        "\n",
        "        # 5. update policy parameters\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            policy = self.get_policy(state)\n",
        "\n",
        "            # Policy gradient\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            grad[action] = 1.0\n",
        "            grad -= policy\n",
        "\n",
        "            # Entropy gradient\n",
        "            entropy_grad = -np.log(policy + 1e-9) - 1\n",
        "\n",
        "            # combine update\n",
        "            total_grad = (advantages[t] * grad +\n",
        "                         self.entropy_coef * entropy_grad)\n",
        "\n",
        "            self.theta[state] += self.lr * total_grad\n",
        "\n",
        "        # 6. decay learning rate\n",
        "        self.decay_learning_rate()\n",
        "        self.episode_count += 1\n",
        "\n",
        "    def decay_learning_rate(self):\n",
        "        \"\"\"gradually decrease learning rate\"\"\"\n",
        "        self.lr = max(self.lr_min, self.lr * self.lr_decay)\n",
        "\n",
        "    @property\n",
        "    def Q(self):\n",
        "        \"\"\"compatible test function\"\"\"\n",
        "        return self.theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward Wrapper"
      ],
      "metadata": {
        "id": "O9xcmKP2yJpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaxiRewardWrapper(gym.Wrapper): # TODO\n",
        "    \"\"\"\n",
        "    Reward wrapper for Taxi environment\n",
        "    Allows customizing reward values without modifying the original environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "        super().__init__(env)\n",
        "        self.custom_reward_step = reward_step\n",
        "        self.custom_reward_delivery = reward_delivery\n",
        "        self.custom_reward_illegal = reward_illegal\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # Replace original rewards with custom rewards based on reward type\n",
        "        if reward == self.env.reward_delivery:\n",
        "\n",
        "            reward = self.custom_reward_delivery\n",
        "        elif reward == self.env.reward_illegal:\n",
        "\n",
        "            reward = self.custom_reward_illegal\n",
        "        elif reward == self.env.reward_step:\n",
        "\n",
        "            reward = self.custom_reward_step\n",
        "\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "X7WUWULGyB2T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsWCn5CNR8uu"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BOs5fC-aSDra"
      },
      "outputs": [],
      "source": [
        "def train(n_episodes=50000, max_steps=200,\n",
        "          seed_start=0, seed_end=40000, verbose=True,\n",
        "          reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"train optimized Policy Gradient Agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        reward_step=reward_step,\n",
        "        reward_delivery=reward_delivery,\n",
        "        reward_illegal=reward_illegal\n",
        "    )\n",
        "\n",
        "    agent = PolicyGradientAgentOptimized(\n",
        "        n_states=env.observation_space.n,\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.01,      # TODO\n",
        "        value_lr=0.1,         # TODO\n",
        "        lr_decay=0.99999,       # TODO\n",
        "        discount_factor=0.99,     # TODO\n",
        "        entropy_coef=0.1        # TODO\n",
        "    )\n",
        "\n",
        "    episode_rewards = []\n",
        "    success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"training episodes: {n_episodes}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
        "\n",
        "        episode_history = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_history.append((state, action, reward))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if terminated and reward == reward_delivery:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        agent.update(episode_history)\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            success_rate = success_count / min(1000, episode + 1)\n",
        "\n",
        "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
        "                  f\"avg reward: {avg_reward:.2f} | \"\n",
        "                  f\"success rate: {success_rate:.1%} | \"\n",
        "                  f\"learning rate: {agent.lr:.6f}\")\n",
        "\n",
        "            if episode >= 99:\n",
        "                success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg7uAA9uSH8n"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8SX1fiIOSOEb"
      },
      "outputs": [],
      "source": [
        "def test(model_filename, n_episodes=100, seed_start=42, verbose=True,\n",
        "         reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"test agent\"\"\"\n",
        "\n",
        "    # Load model for testing\n",
        "    print(f\"\\nload model\")\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        reward_step=reward_step,\n",
        "        reward_delivery=reward_delivery,\n",
        "        reward_illegal=reward_illegal\n",
        "    )\n",
        "\n",
        "    agent = PolicyGradientAgentOptimized(\n",
        "    n_states=env.observation_space.n,\n",
        "    n_actions=env.action_space.n\n",
        "    )\n",
        "    agent.theta = np.load(model_filename)\n",
        "\n",
        "    rewards = []\n",
        "    steps_list = []\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode)\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        for step in range(200):\n",
        "            # use deterministic policy during testing\n",
        "            action = np.argmax(agent.theta[state])\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if reward == reward_delivery:\n",
        "                    successes += 1\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps_list)\n",
        "    success_rate = successes / n_episodes\n",
        "\n",
        "    # calculate evaluation score (success rate 20%, steps 80%)\n",
        "    normalized_steps = avg_steps / 200\n",
        "    step_score = 1 - normalized_steps\n",
        "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\ntest result (seed {seed_start}-{seed_start+n_episodes-1}):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"   avg reward: {avg_reward:.2f}\")\n",
        "        print(f\"   avg steps: {avg_steps:.2f}\")\n",
        "        print(f\"   success rate: {success_rate:.1%}\")\n",
        "        print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_steps': avg_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'evaluation_score': evaluation_score,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb6lgXalTOzA"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "t6KRCuQ4TNjK",
        "outputId": "ba042de1-6e75-4474-92c3-b32a7f621654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "training episodes: 50000\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-920118196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     agent, rewards = train(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4167590460.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_episodes, max_steps, seed_start, seed_end, verbose, reward_step, reward_delivery, reward_illegal)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3557714422.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, episode_history)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# 5. update policy parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# Policy gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3557714422.py\u001b[0m in \u001b[0;36mget_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtheta_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mexp_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexp_theta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set reward parameters\n",
        "    REWARD_STEP = -1\n",
        "    REWARD_DELIVERY = 20\n",
        "    REWARD_ILLEGAL = -10\n",
        "\n",
        "    # train\n",
        "    agent, rewards = train(\n",
        "        n_episodes=50000,\n",
        "        verbose=True,\n",
        "        reward_step=REWARD_STEP,\n",
        "        reward_delivery=REWARD_DELIVERY,\n",
        "        reward_illegal=REWARD_ILLEGAL\n",
        "    )\n",
        "\n",
        "    # save model\n",
        "    model_filename = 'policy_gradient_optimized.npy'\n",
        "    np.save(model_filename, agent.theta)\n",
        "    print(f\"\\nmodel saved to {model_filename}\")\n",
        "\n",
        "    # test\n",
        "    test(\n",
        "        model_filename,\n",
        "        n_episodes=1000,\n",
        "        seed_start=420000,\n",
        "        verbose=True,\n",
        "        reward_step=REWARD_STEP,\n",
        "        reward_delivery=REWARD_DELIVERY,\n",
        "        reward_illegal=REWARD_ILLEGAL\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af656016"
      },
      "source": [
        "# Task\n",
        "Modify the existing policy gradient agent to use an actor-critic architecture for training a reinforcement learning agent on the Taxi-v3 environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34685c14"
      },
      "source": [
        "## Modify the agent class\n",
        "\n",
        "### Subtask:\n",
        "Update the `PolicyGradientAgentOptimized` class to include both an actor (policy) and a critic (value function).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ad3bad4"
      },
      "source": [
        "**Reasoning**:\n",
        "Rename the class and ensure the actor and critic components (`self.theta` and `self.V`) are initialized in the `__init__` method as per the instructions. The other methods will remain unchanged in this step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02d53758"
      },
      "source": [
        "class ActorCriticAgent:\n",
        "\n",
        "    def __init__(self, n_states, n_actions,\n",
        "                 learning_rate=0.01,\n",
        "                 value_lr=0.1,\n",
        "                 lr_decay=0.9999,\n",
        "                 lr_min=0.0001,\n",
        "                 discount_factor=0.99,\n",
        "                 entropy_coef=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.lr_init = learning_rate\n",
        "        self.value_lr = value_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr_min = lr_min\n",
        "        self.gamma = discount_factor\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        # policy parameters (Actor)\n",
        "        self.theta = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # Value function (Critic)\n",
        "        self.V = np.zeros(n_states)\n",
        "\n",
        "        # statistics information\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        \"\"\"calculate action probability distribution (softmax)\"\"\"\n",
        "        theta_state = self.theta[state] - np.max(self.theta[state])\n",
        "        exp_theta = np.exp(theta_state)\n",
        "        return exp_theta / np.sum(exp_theta)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"sample action according to policy\"\"\"\n",
        "        policy = self.get_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update(self, episode_history):\n",
        "        \"\"\"\n",
        "        update policy using Advantage and Entropy\n",
        "        \"\"\"\n",
        "        if len(episode_history) == 0:\n",
        "            return\n",
        "\n",
        "        # 1. calculate return G_t\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for state, action, reward in reversed(episode_history):\n",
        "            G = reward + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = np.array(returns)\n",
        "\n",
        "        # 2. update Value function\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            td_error = returns[t] - self.V[state]\n",
        "            self.V[state] += self.value_lr * td_error\n",
        "\n",
        "        # 3. calculate Advantage\n",
        "        advantages = []\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            advantage = returns[t] - self.V[state]\n",
        "            advantages.append(advantage)\n",
        "\n",
        "        # 4. standardize Advantage\n",
        "        advantages = np.array(advantages)\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-9)\n",
        "\n",
        "        # 5. update policy parameters\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            policy = self.get_policy(state)\n",
        "\n",
        "            # Policy gradient\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            grad[action] = 1.0\n",
        "            grad -= policy\n",
        "\n",
        "            # Entropy gradient\n",
        "            entropy_grad = -np.log(policy + 1e-9) - 1\n",
        "\n",
        "            # combine update\n",
        "            total_grad = (advantages[t] * grad +\n",
        "                         self.entropy_coef * entropy_grad)\n",
        "\n",
        "            self.theta[state] += self.lr * total_grad\n",
        "\n",
        "        # 6. decay learning rate\n",
        "        self.decay_learning_rate()\n",
        "        self.episode_count += 1\n",
        "\n",
        "    def decay_learning_rate(self):\n",
        "        \"\"\"gradually decrease learning rate\"\"\"\n",
        "        self.lr = max(self.lr_min, self.lr * self.lr_decay)\n",
        "\n",
        "    @property\n",
        "    def Q(self):\n",
        "        \"\"\"compatible test function\"\"\"\n",
        "        return self.theta"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d3a68fb"
      },
      "source": [
        "## Update training logic\n",
        "\n",
        "### Subtask:\n",
        "Modify the `update` method to use the critic's value estimates to calculate the advantage for updating the actor's policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0d7585"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `update` method to implement the actor-critic update logic, including calculating the TD error, updating the value function, and using the TD error as the advantage for the policy update.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a0ef566"
      },
      "source": [
        "class ActorCriticAgent:\n",
        "\n",
        "    def __init__(self, n_states, n_actions,\n",
        "                 learning_rate=0.01,\n",
        "                 value_lr=0.1,\n",
        "                 lr_decay=0.9999,\n",
        "                 lr_min=0.0001,\n",
        "                 discount_factor=0.99,\n",
        "                 entropy_coef=0.01):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.lr_init = learning_rate\n",
        "        self.value_lr = value_lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.lr_min = lr_min\n",
        "        self.gamma = discount_factor\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        # policy parameters (Actor)\n",
        "        self.theta = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # Value function (Critic)\n",
        "        self.V = np.zeros(n_states)\n",
        "\n",
        "        # statistics information\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        \"\"\"calculate action probability distribution (softmax)\"\"\"\n",
        "        theta_state = self.theta[state] - np.max(self.theta[state])\n",
        "        exp_theta = np.exp(theta_state)\n",
        "        return exp_theta / np.sum(exp_theta)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"sample action according to policy\"\"\"\n",
        "        policy = self.get_policy(state)\n",
        "        return np.random.choice(self.n_actions, p=policy)\n",
        "\n",
        "    def update(self, episode_history):\n",
        "        \"\"\"\n",
        "        update policy using Advantage and Entropy\n",
        "        \"\"\"\n",
        "        if len(episode_history) == 0:\n",
        "            return\n",
        "\n",
        "        # Actor-Critic Update\n",
        "        advantages = []\n",
        "        for t in range(len(episode_history)):\n",
        "            state, action, reward = episode_history[t]\n",
        "            next_state = episode_history[t+1][0] if t + 1 < len(episode_history) else state # Assume next state is current state if episode ends\n",
        "            terminated = t + 1 == len(episode_history) # Check if current step is the last step\n",
        "\n",
        "            # Calculate TD Target\n",
        "            td_target = reward + self.gamma * self.V[next_state] * (1 - terminated)\n",
        "\n",
        "            # Calculate TD Error\n",
        "            td_error = td_target - self.V[state]\n",
        "\n",
        "            # Update Value function (Critic)\n",
        "            self.V[state] += self.value_lr * td_error\n",
        "\n",
        "            # Calculate Advantage (TD Error)\n",
        "            advantages.append(td_error)\n",
        "\n",
        "        # Standardize Advantage\n",
        "        advantages = np.array(advantages)\n",
        "        if len(advantages) > 1:\n",
        "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-9)\n",
        "\n",
        "\n",
        "        # Update policy parameters (Actor)\n",
        "        for t, (state, action, reward) in enumerate(episode_history):\n",
        "            policy = self.get_policy(state)\n",
        "\n",
        "            # Policy gradient\n",
        "            grad = np.zeros(self.n_actions)\n",
        "            grad[action] = 1.0\n",
        "            grad -= policy\n",
        "\n",
        "            # Entropy gradient\n",
        "            entropy_grad = -np.log(policy + 1e-9) - 1\n",
        "\n",
        "            # combine update\n",
        "            total_grad = (advantages[t] * grad +\n",
        "                         self.entropy_coef * entropy_grad)\n",
        "\n",
        "            self.theta[state] += self.lr * total_grad\n",
        "\n",
        "        # Decay learning rate\n",
        "        self.decay_learning_rate()\n",
        "        self.episode_count += 1\n",
        "\n",
        "    def decay_learning_rate(self):\n",
        "        \"\"\"gradually decrease learning rate\"\"\"\n",
        "        self.lr = max(self.lr_min, self.lr * self.lr_decay)\n",
        "\n",
        "    @property\n",
        "    def Q(self):\n",
        "        \"\"\"compatible test function\"\"\"\n",
        "        return self.theta"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ac7c6e9"
      },
      "source": [
        "## Adjust reward wrapper (if needed)\n",
        "\n",
        "### Subtask:\n",
        "Review the `TaxiRewardWrapper` to ensure it's compatible with the actor-critic approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4268b601"
      },
      "source": [
        "## Modify training function\n",
        "\n",
        "### Subtask:\n",
        "Update the `train` function to accommodate the changes in the agent and the training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf5b502b"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the train function to use the ActorCriticAgent class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9142f7c"
      },
      "source": [
        "def train(n_episodes=50000, max_steps=200,\n",
        "          seed_start=0, seed_end=40000, verbose=True,\n",
        "          reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"train optimized Policy Gradient Agent\"\"\"\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        reward_step=reward_step,\n",
        "        reward_delivery=reward_delivery,\n",
        "        reward_illegal=reward_illegal\n",
        "    )\n",
        "\n",
        "    agent = ActorCriticAgent(\n",
        "        n_states=env.observation_space.n,\n",
        "        n_actions=env.action_space.n,\n",
        "        learning_rate=0.01,      # TODO\n",
        "        value_lr=0.1,         # TODO\n",
        "        lr_decay=0.99999,       # TODO\n",
        "        discount_factor=0.99,     # TODO\n",
        "        entropy_coef=0.1        # TODO\n",
        "    )\n",
        "\n",
        "    episode_rewards = []\n",
        "    success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"training episodes: {n_episodes}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode % seed_end)\n",
        "\n",
        "        episode_history = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_history.append((state, action, reward))\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if terminated and reward == reward_delivery:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        agent.update(episode_history)\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            success_rate = success_count / min(1000, episode + 1)\n",
        "\n",
        "            print(f\"episode {episode + 1}/{n_episodes} | \"\n",
        "                  f\"avg reward: {avg_reward:.2f} | \"\n",
        "                  f\"success rate: {success_rate:.1%} | \"\n",
        "                  f\"learning rate: {agent.lr:.6f}\")\n",
        "\n",
        "            if episode >= 99:\n",
        "                success_count = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    env.close()\n",
        "    return agent, episode_rewards"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dccef1cd"
      },
      "source": [
        "## Modify testing function\n",
        "\n",
        "### Subtask:\n",
        "Update the `test` function to work with the new agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c3da43d"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the `test` function to use the `ActorCriticAgent` instead of the `PolicyGradientAgentOptimized`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c9c3e7c"
      },
      "source": [
        "def test(model_filename, n_episodes=100, seed_start=42, verbose=True,\n",
        "         reward_step=-5, reward_delivery=20, reward_illegal=-1):\n",
        "    \"\"\"test agent\"\"\"\n",
        "\n",
        "    # Load model for testing\n",
        "    print(f\"\\nload model\")\n",
        "\n",
        "    # Wrap environment with custom rewards\n",
        "    env = TaxiRewardWrapper(\n",
        "        TaxiEnv(),\n",
        "        reward_step=reward_step,\n",
        "        reward_delivery=reward_delivery,\n",
        "        reward_illegal=reward_illegal\n",
        "    )\n",
        "\n",
        "    agent = ActorCriticAgent(\n",
        "    n_states=env.observation_space.n,\n",
        "    n_actions=env.action_space.n\n",
        "    )\n",
        "    agent.theta = np.load(model_filename)\n",
        "\n",
        "    rewards = []\n",
        "    steps_list = []\n",
        "    successes = 0\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, info = env.reset(seed=seed_start + episode)\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        for step in range(200):\n",
        "            # use deterministic policy during testing\n",
        "            action = np.argmax(agent.theta[state])\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if reward == reward_delivery:\n",
        "                    successes += 1\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "        steps_list.append(step_count)\n",
        "\n",
        "    avg_reward = np.mean(rewards)\n",
        "    avg_steps = np.mean(steps_list)\n",
        "    success_rate = successes / n_episodes\n",
        "\n",
        "    # calculate evaluation score (success rate 20%, steps 80%)\n",
        "    normalized_steps = avg_steps / 200\n",
        "    step_score = 1 - normalized_steps\n",
        "    evaluation_score = success_rate * 0.2 + step_score * 0.8\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\ntest result (seed {seed_start}-{seed_start+n_episodes-1}):\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"   avg reward: {avg_reward:.2f}\")\n",
        "        print(f\"   avg steps: {avg_steps:.2f}\")\n",
        "        print(f\"   success rate: {success_rate:.1%}\")\n",
        "        print(f\"   evaluation score: {evaluation_score:.4f} ({evaluation_score*100:.2f}%)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return {\n",
        "        'avg_reward': avg_reward,\n",
        "        'avg_steps': avg_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'evaluation_score': evaluation_score,\n",
        "    }"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5495c55"
      },
      "source": [
        "## Modify main execution\n",
        "\n",
        "### Subtask:\n",
        "Update the `if __name__ == \"__main__\":` block to use the modified train and test functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed014dd"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the main block to use the ActorCriticAgent and verify the training and testing processes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f25bd0c9",
        "outputId": "74b1fb76-587b-4de4-ced5-fcc942429744"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Set reward parameters\n",
        "    REWARD_STEP = -1\n",
        "    REWARD_DELIVERY = 20\n",
        "    REWARD_ILLEGAL = -10\n",
        "\n",
        "    # train\n",
        "    agent, rewards = train(\n",
        "        n_episodes=50000,\n",
        "        verbose=True,\n",
        "        reward_step=REWARD_STEP,\n",
        "        reward_delivery=REWARD_DELIVERY,\n",
        "        reward_illegal=REWARD_ILLEGAL\n",
        "    )\n",
        "\n",
        "    # save model\n",
        "    model_filename = 'actor_critic_agent.npy' # Changed filename\n",
        "    np.save(model_filename, agent.theta)\n",
        "    print(f\"\\nmodel saved to {model_filename}\")\n",
        "\n",
        "    # test\n",
        "    test(\n",
        "        model_filename,\n",
        "        n_episodes=1000,\n",
        "        seed_start=420000,\n",
        "        verbose=True,\n",
        "        reward_step=REWARD_STEP,\n",
        "        reward_delivery=REWARD_DELIVERY,\n",
        "        reward_illegal=REWARD_ILLEGAL\n",
        "    )\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "training episodes: 50000\n",
            "======================================================================\n",
            "episode 1000/50000 | avg reward: -150.03 | success rate: 51.6% | learning rate: 0.009900\n",
            "episode 2000/50000 | avg reward: -43.43 | success rate: 96.2% | learning rate: 0.009802\n",
            "episode 3000/50000 | avg reward: -44.80 | success rate: 98.2% | learning rate: 0.009704\n",
            "episode 4000/50000 | avg reward: -21.76 | success rate: 99.0% | learning rate: 0.009608\n",
            "episode 5000/50000 | avg reward: -12.45 | success rate: 99.7% | learning rate: 0.009512\n",
            "episode 6000/50000 | avg reward: -16.02 | success rate: 100.0% | learning rate: 0.009418\n",
            "episode 7000/50000 | avg reward: -18.02 | success rate: 99.6% | learning rate: 0.009324\n",
            "episode 8000/50000 | avg reward: -14.19 | success rate: 99.9% | learning rate: 0.009231\n",
            "episode 9000/50000 | avg reward: -12.01 | success rate: 100.0% | learning rate: 0.009139\n",
            "episode 10000/50000 | avg reward: -13.02 | success rate: 99.8% | learning rate: 0.009048\n",
            "episode 11000/50000 | avg reward: -11.43 | success rate: 99.8% | learning rate: 0.008958\n",
            "episode 12000/50000 | avg reward: -13.01 | success rate: 99.7% | learning rate: 0.008869\n",
            "episode 13000/50000 | avg reward: -14.76 | success rate: 99.7% | learning rate: 0.008781\n",
            "episode 14000/50000 | avg reward: -10.56 | success rate: 99.9% | learning rate: 0.008694\n",
            "episode 15000/50000 | avg reward: -9.26 | success rate: 100.0% | learning rate: 0.008607\n",
            "episode 16000/50000 | avg reward: -12.45 | success rate: 99.9% | learning rate: 0.008521\n",
            "episode 17000/50000 | avg reward: -9.31 | success rate: 99.8% | learning rate: 0.008437\n",
            "episode 18000/50000 | avg reward: -7.30 | success rate: 99.9% | learning rate: 0.008353\n",
            "episode 19000/50000 | avg reward: -14.43 | success rate: 99.8% | learning rate: 0.008270\n",
            "episode 20000/50000 | avg reward: -9.59 | success rate: 99.9% | learning rate: 0.008187\n",
            "episode 21000/50000 | avg reward: -9.72 | success rate: 99.8% | learning rate: 0.008106\n",
            "episode 22000/50000 | avg reward: -5.97 | success rate: 100.0% | learning rate: 0.008025\n",
            "episode 23000/50000 | avg reward: -8.06 | success rate: 99.9% | learning rate: 0.007945\n",
            "episode 24000/50000 | avg reward: -11.12 | success rate: 100.0% | learning rate: 0.007866\n",
            "episode 25000/50000 | avg reward: -7.92 | success rate: 100.0% | learning rate: 0.007788\n",
            "episode 26000/50000 | avg reward: -15.37 | success rate: 99.7% | learning rate: 0.007711\n",
            "episode 27000/50000 | avg reward: -14.75 | success rate: 99.8% | learning rate: 0.007634\n",
            "episode 28000/50000 | avg reward: -10.68 | success rate: 100.0% | learning rate: 0.007558\n",
            "episode 29000/50000 | avg reward: -9.85 | success rate: 100.0% | learning rate: 0.007483\n",
            "episode 30000/50000 | avg reward: -8.46 | success rate: 100.0% | learning rate: 0.007408\n",
            "episode 31000/50000 | avg reward: -8.67 | success rate: 100.0% | learning rate: 0.007334\n",
            "episode 32000/50000 | avg reward: -12.49 | success rate: 100.0% | learning rate: 0.007261\n",
            "episode 33000/50000 | avg reward: -7.01 | success rate: 100.0% | learning rate: 0.007189\n",
            "episode 34000/50000 | avg reward: -9.97 | success rate: 99.8% | learning rate: 0.007118\n",
            "episode 35000/50000 | avg reward: -10.35 | success rate: 99.9% | learning rate: 0.007047\n",
            "episode 36000/50000 | avg reward: -8.47 | success rate: 99.8% | learning rate: 0.006977\n",
            "episode 37000/50000 | avg reward: -8.65 | success rate: 100.0% | learning rate: 0.006907\n",
            "episode 38000/50000 | avg reward: -9.25 | success rate: 100.0% | learning rate: 0.006839\n",
            "episode 39000/50000 | avg reward: -8.18 | success rate: 99.9% | learning rate: 0.006771\n",
            "episode 40000/50000 | avg reward: -8.59 | success rate: 99.6% | learning rate: 0.006703\n",
            "episode 41000/50000 | avg reward: -9.35 | success rate: 100.0% | learning rate: 0.006636\n",
            "episode 42000/50000 | avg reward: -8.53 | success rate: 100.0% | learning rate: 0.006570\n",
            "episode 43000/50000 | avg reward: -9.49 | success rate: 100.0% | learning rate: 0.006505\n",
            "episode 44000/50000 | avg reward: -9.09 | success rate: 100.0% | learning rate: 0.006440\n",
            "episode 45000/50000 | avg reward: -7.88 | success rate: 100.0% | learning rate: 0.006376\n",
            "episode 46000/50000 | avg reward: -9.14 | success rate: 100.0% | learning rate: 0.006313\n",
            "episode 47000/50000 | avg reward: -8.44 | success rate: 100.0% | learning rate: 0.006250\n",
            "episode 48000/50000 | avg reward: -12.76 | success rate: 99.8% | learning rate: 0.006188\n",
            "episode 49000/50000 | avg reward: -9.12 | success rate: 100.0% | learning rate: 0.006126\n",
            "episode 50000/50000 | avg reward: -6.58 | success rate: 99.9% | learning rate: 0.006065\n",
            "======================================================================\n",
            "Training completed!\n",
            "\n",
            "model saved to actor_critic_agent.npy\n",
            "\n",
            "load model\n",
            "\n",
            "test result (seed 420000-420999):\n",
            "======================================================================\n",
            "   avg reward: -180.88\n",
            "   avg steps: 182.83\n",
            "   success rate: 9.3%\n",
            "   evaluation score: 0.0873 (8.73%)\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REWARD_STEP = -1\n",
        "REWARD_DELIVERY = 20\n",
        "REWARD_ILLEGAL = -10\n",
        "\n",
        "# train\n",
        "agent, rewards = train(\n",
        "    n_episodes=100,\n",
        "    verbose=True,\n",
        "    reward_step=REWARD_STEP,\n",
        "    reward_delivery=REWARD_DELIVERY,\n",
        "    reward_illegal=REWARD_ILLEGAL\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWSUyEUrzrPi",
        "outputId": "0f8cc33d-5f65-45ff-f883-12fc8f4e6df2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training episodes: 100\n",
            "======================================================================\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "Use multiple maps:  True\n",
            "Using maps:  1\n",
            "Use multiple maps:  True\n",
            "Using maps:  2\n",
            "Use multiple maps:  True\n",
            "Using maps:  0\n",
            "======================================================================\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "test(\n",
        "    model_filename,\n",
        "    n_episodes=1000,\n",
        "    seed_start=420000,\n",
        "    verbose=True,\n",
        "    reward_step=REWARD_STEP,\n",
        "    reward_delivery=REWARD_DELIVERY,\n",
        "    reward_illegal=REWARD_ILLEGAL\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q8nroXexip6",
        "outputId": "7d9d69ea-9900-4e93-9bda-02af522c1565"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "load model\n",
            "\n",
            "test result (seed 0-999):\n",
            "======================================================================\n",
            "   avg reward: -176.16\n",
            "   avg steps: 178.60\n",
            "   success rate: 11.6%\n",
            "   evaluation score: 0.1088 (10.88%)\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'avg_reward': np.float64(-176.162),\n",
              " 'avg_steps': np.float64(178.598),\n",
              " 'success_rate': 0.116,\n",
              " 'evaluation_score': np.float64(0.10880799999999996)}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desc_maps = [np.asarray(m, dtype=\"c\") for m in MAPS]"
      ],
      "metadata": {
        "id": "K-_qlgF61U5j"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc_maps[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46BkHpQZAomH",
        "outputId": "a9e0a773-6949-4213-e845-f8289402dfde"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'+', b'-', b'-', b'-', b'-', b'-', b'-', b'-', b'-', b'-', b'+'],\n",
              "      dtype='|S1')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCP4VApxApYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}